"""
GAT + åˆ†ç¦»å¼ç¼–ç å™¨ æ¨¡å‹ (ä¼˜åŒ–ç‰ˆ v3.0)

æ ¸å¿ƒæ”¹è¿›ï¼š
1. Cross-Attention Fusion V2: ç‰¹å¾çº§äº¤å‰æ³¨æ„åŠ›ï¼ˆåŠ¨æ€æŸ¥è¯¢å„é™æ€ç‰¹å¾ç»´åº¦ï¼‰
2. Learnable Node Embeddings: å¯å­¦ä¹ èŠ‚ç‚¹åµŒå…¥æ•è·éšå¼ç«™ç‚¹ç‰¹å¾
3. Skip Connection: GATè¾“å…¥åˆ°è¾“å‡ºçš„æ®‹å·®è¿æ¥ï¼ˆé˜²æ­¢è¿‡åº¦å¹³æ»‘ï¼‰
4. å¯è§£é‡Šæ€§å¢å¼º: æ”¯æŒæå–ç‰¹å¾çº§æ³¨æ„åŠ›æƒé‡

æ¶æ„å¯¹æ¯”ï¼š
- v1.0: åˆ†ç¦»å¼ç¼–ç å™¨(é™æ€MLP + åŠ¨æ€LSTM) â†’ ç®€å•èåˆ â†’ GAT â†’ è§£ç å™¨
- v2.0: åˆ†ç¦»å¼ç¼–ç å™¨ â†’ Cross-Attentionèåˆ â†’ GAT(+Skip) â†’ è§£ç å™¨
- v3.0: åˆ†ç¦»å¼ç¼–ç å™¨ â†’ ç‰¹å¾çº§Cross-Attention â†’ GAT(+Skip) â†’ è§£ç å™¨
        + æ³¨æ„åŠ›æƒé‡å¯è§£é‡Šæ€§

ä¼˜åŠ¿ï¼š
1. åŠ¨æ€ç‰¹å¾è‡ªé€‚åº”åœ°"æŸ¥è¯¢"æœ€ç›¸å…³çš„é™æ€åœ°ç†ä¿¡æ¯
2. ç‰¹å¾çº§æ³¨æ„åŠ›æƒé‡å¯è§£é‡Šï¼šå±•ç¤ºæ¨¡å‹å¯¹å„é™æ€ç‰¹å¾çš„å…³æ³¨ç¨‹åº¦
3. èŠ‚ç‚¹åµŒå…¥æ•è·æ•°æ®æœªè®°å½•çš„å¾®æ°”å€™æ•ˆåº”ï¼ˆå¦‚è¡—é“å³¡è°·æ•ˆåº”ï¼‰
4. æ®‹å·®è¿æ¥ä¿ç•™ç«™ç‚¹è‡ªèº«å†å²è¶‹åŠ¿ï¼Œé˜²æ­¢å›¾å·ç§¯è¿‡åº¦å¹³æ»‘

ä½œè€…: GNNæ°”æ¸©é¢„æµ‹é¡¹ç›®
æ—¥æœŸ: 2025-12
ç‰ˆæœ¬: 3.0
"""

import torch
import torch.nn as nn
from torch_geometric.nn import GATv2Conv

# å¯¼å…¥ RevIN å±‚
from .layers import RevIN


def get_norm_layer(norm_type, dim):
    """è§„èŒƒåŒ–å±‚é€‰æ‹©"""
    if norm_type == 'BatchNorm':
        return nn.BatchNorm1d(dim)
    elif norm_type == 'LayerNorm':
        return nn.LayerNorm(dim)
    elif norm_type == 'None' or norm_type is None:
        return None
    else:
        raise ValueError(f"æœªçŸ¥çš„è§„èŒƒåŒ–ç±»å‹: {norm_type}")


def whichAF(AF):
    """æ¿€æ´»å‡½æ•°é€‰æ‹©"""
    if AF == 'PReLU':
        return nn.PReLU()
    elif AF == "LeakyReLU":
        return nn.LeakyReLU()
    elif AF == "ReLU":
        return nn.ReLU()
    elif AF == 'GELU':
        return nn.GELU()
    else:
        return nn.Identity()


class LightweightStaticEncoder(nn.Module):
    """
    è½»é‡çº§é™æ€ç‰¹å¾ç¼–ç å™¨ (ä¼˜åŒ–ç‰ˆ)

    ä¿ç•™ [N, 12, dim] çš„è¾“å‡ºç»“æ„ä»¥æ”¯æŒç‰¹å¾çº§æ³¨æ„åŠ›ï¼Œ
    ä½†ç§»é™¤ç¬¨é‡çš„ MLPï¼Œæ”¹ç”¨ "ç‰¹å¾å€¼ * å¯å­¦ä¹ åŸºå‘é‡" çš„æ–¹å¼ã€‚

    å‚æ•°é‡æä½ï¼Œæéš¾è¿‡æ‹Ÿåˆï¼Œä¸”å®Œå…¨ä¿ç•™å¯è§£é‡Šæ€§ã€‚
    """

    def __init__(self, num_features, output_dim):
        super(LightweightStaticEncoder, self).__init__()
        self.num_features = num_features
        self.output_dim = output_dim

        # å®šä¹‰åŸºå‘é‡ (Basis Vectors)
        # å½¢çŠ¶: [1, 12, output_dim]
        # è¿™é‡Œçš„æ¯ä¸€ä¸ªå‘é‡ä»£è¡¨ä¸€ç§ç‰¹å¾çš„"è¯­ä¹‰èº«ä»½"
        self.feature_embeddings = nn.Parameter(
            torch.randn(1, num_features, output_dim) * 0.02
        )

        # å¯é€‰ï¼šå¦‚æœä½ æ‹…å¿ƒç®€å•çš„ä¹˜æ³•è¡¨è¾¾èƒ½åŠ›ä¸å¤Ÿï¼Œå¯ä»¥åŠ ä¸€ä¸ªå…±äº«çš„å±‚å½’ä¸€åŒ–
        self.norm = nn.LayerNorm(output_dim)

    def forward(self, x):
        """
        Args:
            x: [batch_size, num_features]  (åŸå§‹é™æ€ç‰¹å¾å€¼ï¼Œæ ‡é‡)

        Returns:
            [batch_size, num_features, output_dim] (ç‰¹å¾Tokenåºåˆ—)
        """
        batch_size = x.shape[0]

        # 1. æ‰©å±•è¾“å…¥ç»´åº¦
        # x: [batch, 12] -> [batch, 12, 1]
        x_expanded = x.unsqueeze(-1)

        # 2. å¹¿æ’­ä¹˜æ³• (Scaling)
        # å°†æ ‡é‡ç‰¹å¾å€¼ ä¹˜ä»¥ å¯¹åº”çš„ç‰¹å¾èº«ä»½å‘é‡
        # [batch, 12, 1] * [1, 12, dim] -> [batch, 12, dim]
        # å¹¿æ’­æœºåˆ¶ä¼šè‡ªåŠ¨å¤„ç†ç»´åº¦åŒ¹é…
        tokens = x_expanded * self.feature_embeddings

        # 3. å½’ä¸€åŒ– (è®©è®­ç»ƒæ›´ç¨³å®š)
        tokens = self.norm(tokens)

        return tokens


class StaticFeatureEncoder(nn.Module):
    """
    é™æ€ç‰¹å¾ç‹¬ç«‹ç¼–ç å™¨ (v3.0 æ–°å¢)

    å°†æ¯ä¸ªé™æ€ç‰¹å¾ç»´åº¦ç‹¬ç«‹ç¼–ç ä¸ºtokenï¼Œç”¨äºç‰¹å¾çº§Cross-Attentionã€‚
    æ¯ä¸ªé™æ€ç‰¹å¾ç»è¿‡ç‹¬ç«‹çš„MLPç¼–ç ï¼Œä¿æŒç‰¹å¾é—´çš„åŒºåˆ†åº¦ã€‚

    è¾“å…¥: [N, num_static_features] åŸå§‹é™æ€ç‰¹å¾ï¼ˆ12ç»´ï¼‰
    è¾“å‡º: [N, num_static_features, token_dim] æ¯ä¸ªç‰¹å¾ä¸€ä¸ªtoken
    """

    def __init__(self, num_features, token_dim, dropout=0.1):
        """
        Args:
            num_features: é™æ€ç‰¹å¾æ•°é‡ï¼ˆå¦‚12ï¼‰
            token_dim: æ¯ä¸ªtokençš„ç»´åº¦
            dropout: Dropoutç‡
        """
        super(StaticFeatureEncoder, self).__init__()

        self.num_features = num_features
        self.token_dim = token_dim

        # æ¯ä¸ªé™æ€ç‰¹å¾ç‹¬ç«‹çš„ç¼–ç å™¨
        # è¾“å…¥: 1ç»´æ ‡é‡ â†’ è¾“å‡º: token_dimç»´å‘é‡
        self.feature_encoders = nn.ModuleList([
            nn.Sequential(
                nn.Linear(1, token_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(token_dim, token_dim)
            ) for _ in range(num_features)
        ])

        # å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼ˆåŒºåˆ†ä¸åŒç‰¹å¾çš„ä½ç½®ï¼‰
        self.position_embedding = nn.Parameter(
            torch.randn(1, num_features, token_dim) * 0.02
        )

    def forward(self, x):
        """
        Args:
            x: [num_nodes, num_features] é™æ€ç‰¹å¾

        Returns:
            [num_nodes, num_features, token_dim] ç‰¹å¾tokenåºåˆ—
        """
        batch_size = x.shape[0]
        tokens = []

        # æ¯ä¸ªç‰¹å¾ç‹¬ç«‹ç¼–ç 
        for i in range(self.num_features):
            # æå–ç¬¬iä¸ªç‰¹å¾: [N, 1]
            feat_i = x[:, i:i+1]
            # ç¼–ç ä¸ºtoken: [N, token_dim]
            token_i = self.feature_encoders[i](feat_i)
            tokens.append(token_i)

        # å †å : [N, num_features, token_dim]
        tokens = torch.stack(tokens, dim=1)

        # æ·»åŠ ä½ç½®ç¼–ç 
        tokens = tokens + self.position_embedding

        return tokens


class DynamicEncoder(nn.Module):
    """
    åŠ¨æ€ç‰¹å¾ç¼–ç å™¨

    ä½¿ç”¨LSTMå¯¹åŠ¨æ€ç‰¹å¾ï¼ˆæ°”è±¡è¦ç´ ç­‰ï¼‰è¿›è¡Œæ—¶åºç¼–ç ã€‚
    """

    def __init__(self, input_dim, hidden_dim, num_layers=1,
                 dropout=0.1, bidirectional=False):
        super(DynamicEncoder, self).__init__()

        self.hidden_dim = hidden_dim
        self.bidirectional = bidirectional

        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # LSTMç¼–ç å™¨
        self.lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional,
            batch_first=False
        )

        # åŒå‘LSTMæŠ•å½±
        if bidirectional:
            self.output_proj = nn.Linear(hidden_dim * 2, hidden_dim)

    def forward(self, x):
        """
        Args:
            x: [num_nodes, hist_len, dynamic_dim] åŠ¨æ€ç‰¹å¾

        Returns:
            [num_nodes, hidden_dim] åŠ¨æ€åµŒå…¥ï¼ˆå–æœ€åæ—¶é—´æ­¥ï¼‰
        """
        # [num_nodes, hist_len, dynamic_dim] â†’ [hist_len, num_nodes, dynamic_dim]
        x = x.permute(1, 0, 2)

        # è¾“å…¥æŠ•å½±
        x = self.input_proj(x)  # [hist_len, num_nodes, hidden_dim]

        # LSTMç¼–ç 
        out, _ = self.lstm(x)
        # å–æœ€åæ—¶é—´æ­¥ [num_nodes, hidden_dim] æˆ– [num_nodes, 2*hidden_dim]
        out = out[-1]

        # åŒå‘æŠ•å½±
        if self.bidirectional:
            out = self.output_proj(out)

        return out


class CrossAttentionFusionV2(nn.Module):
    """
    ç‰¹å¾çº§äº¤å‰æ³¨æ„åŠ›èåˆæ¨¡å— (v3.0 æ–°å¢)

    æ ¸å¿ƒæ”¹è¿›ï¼š
    - å°†12ä¸ªé™æ€ç‰¹å¾ä½œä¸ºç‹¬ç«‹çš„K/Våºåˆ—
    - æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: [N, num_heads, 1, 12] â†’ å¯¹æ¯ä¸ªé™æ€ç‰¹å¾çš„å…³æ³¨ç¨‹åº¦
    - æ”¯æŒæå–æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§£é‡Šæ€§åˆ†æ

    æ¶æ„ï¼š
    - Query (Q): åŠ¨æ€ç‰¹å¾ç¼–ç  [N, 1, dim]
    - Key (K) & Value (V): é™æ€ç‰¹å¾tokens [N, 12, dim]
    - è¾“å‡º: èåˆè¡¨ç¤º [N, dim] + å¯é€‰çš„æ³¨æ„åŠ›æƒé‡ [N, num_heads, 12]
    """

    def __init__(self, num_static_features, dynamic_dim, output_dim,
                 num_heads=4, dropout=0.1, use_pre_ln=True):
        """
        Args:
            num_static_features: é™æ€ç‰¹å¾æ•°é‡ï¼ˆå¦‚12ï¼‰
            dynamic_dim: åŠ¨æ€ç‰¹å¾ç»´åº¦ï¼ˆLSTMè¾“å‡ºç»´åº¦ï¼‰
            output_dim: è¾“å‡ºç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutç‡
            use_pre_ln: æ˜¯å¦ä½¿ç”¨Pre-LN
        """
        super(CrossAttentionFusionV2, self).__init__()

        self.num_static_features = num_static_features
        self.output_dim = output_dim
        self.num_heads = num_heads
        self.use_pre_ln = use_pre_ln

        # é™æ€ç‰¹å¾ç‹¬ç«‹ç¼–ç å™¨
        self.static_encoder = StaticFeatureEncoder(
            num_features=num_static_features,
            token_dim=output_dim,
            dropout=dropout
        )

        # self.static_encoder = LightweightStaticEncoder(
        #     num_features=num_static_features,
        #     output_dim=output_dim  # ç¡®ä¿è¿™é‡Œç»´åº¦å’Œ dynamic_emb æŠ•å½±åçš„ç»´åº¦ä¸€è‡´
        # )

        # åŠ¨æ€ç‰¹å¾æŠ•å½±
        self.dynamic_proj = nn.Linear(dynamic_dim, output_dim)

        # äº¤å‰æ³¨æ„åŠ›å±‚
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=output_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        # LayerNormå±‚
        self.ln1_q = nn.LayerNorm(output_dim)
        self.ln1_kv = nn.LayerNorm(output_dim)
        self.ln2 = nn.LayerNorm(output_dim)

        # FeedForward Network (FFN)
        self.ffn = nn.Sequential(
            nn.Linear(output_dim, output_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(output_dim * 2, output_dim),
            nn.Dropout(dropout)
        )

    def forward(self, static_features, dynamic_emb, return_attention=False):
        """
        Args:
            static_features: [N, num_static_features] åŸå§‹é™æ€ç‰¹å¾ï¼ˆ12ç»´ï¼‰
            dynamic_emb: [N, dynamic_dim] åŠ¨æ€ç‰¹å¾ç¼–ç ï¼ˆLSTMè¾“å‡ºï¼‰
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡

        Returns:
            å¦‚æœ return_attention=False:
                [N, output_dim] èåˆåçš„è¡¨ç¤º
            å¦‚æœ return_attention=True:
                ([N, output_dim], [N, num_heads, num_static_features])
        """
        # é™æ€ç‰¹å¾ç¼–ç ä¸ºtokenåºåˆ—
        # [N, 12] â†’ [N, 12, output_dim]
        static_tokens = self.static_encoder(static_features)

        # åŠ¨æ€ç‰¹å¾æŠ•å½±
        # [N, dynamic_dim] â†’ [N, output_dim]
        dynamic_out = self.dynamic_proj(dynamic_emb)

        # æ„å»ºQ, K, V
        # Q: [N, 1, output_dim] (åŠ¨æ€ç‰¹å¾ä½œä¸ºæŸ¥è¯¢)
        # K, V: [N, 12, output_dim] (é™æ€ç‰¹å¾tokensä½œä¸ºé”®å€¼)
        q = dynamic_out.unsqueeze(1)  # [N, 1, output_dim]
        k = static_tokens              # [N, 12, output_dim]
        v = static_tokens              # [N, 12, output_dim]

        # ==================== æ³¨æ„åŠ›å— ====================
        if self.use_pre_ln:
            q_norm = self.ln1_q(q)
            k_norm = self.ln1_kv(k)
            v_norm = self.ln1_kv(v)
            attn_out, attn_weights = self.cross_attention(
                q_norm, k_norm, v_norm,
                need_weights=True,
                average_attn_weights=False  # ä¿ç•™å¤šå¤´ä¿¡æ¯
            )
            # attn_out: [N, 1, output_dim]
            # attn_weights: [N, num_heads, 1, 12]
            attn_out = attn_out.squeeze(1)  # [N, output_dim]
            x = dynamic_out + attn_out
        else:
            attn_out, attn_weights = self.cross_attention(
                q, k, v,
                need_weights=True,
                average_attn_weights=False
            )
            attn_out = attn_out.squeeze(1)
            x = dynamic_out + attn_out
            x = self.ln1_q(x)

        # ==================== FFNå— ====================
        if self.use_pre_ln:
            ffn_out = self.ffn(self.ln2(x))
            x = x + ffn_out
        else:
            ffn_out = self.ffn(x)
            x = x + ffn_out
            x = self.ln2(x)

        if return_attention:
            # attn_weights: [N, num_heads, 1, 12] â†’ [N, num_heads, 12]
            attn_weights = attn_weights.squeeze(2)
            return x, attn_weights
        else:
            return x


class GAT_SeparateEncoder(nn.Module):
    """
    GAT + åˆ†ç¦»å¼ç¼–ç å™¨ æ¨¡å‹ (ä¼˜åŒ–ç‰ˆ v3.0)

    æ¶æ„:
    1. å¯å­¦ä¹ èŠ‚ç‚¹åµŒå…¥ â­
       - æ•è·éšå¼ç«™ç‚¹ç‰¹å¾ï¼ˆå¦‚å¾®æ°”å€™ã€è¡—é“å³¡è°·æ•ˆåº”ç­‰ï¼‰
    2. åˆ†ç¦»å¼ç¼–ç å™¨:
       - åŠ¨æ€ç‰¹å¾ â†’ DynamicEncoder(LSTM) â†’ åŠ¨æ€åµŒå…¥
       - é™æ€ç‰¹å¾(12ç»´) â†’ CrossAttentionFusionV2 â†’ ç‰¹å¾çº§æ³¨æ„åŠ›èåˆ
    3. GATå›¾å·ç§¯å±‚ x N (å¸¦æ®‹å·®è¿æ¥) â­
    4. LSTMè§£ç å™¨ï¼ˆå¤šæ­¥é¢„æµ‹ï¼‰

    v3.0 æ–°ç‰¹æ€§:
    - ç‰¹å¾çº§Cross-Attention: 12ä¸ªé™æ€ç‰¹å¾ä½œä¸ºç‹¬ç«‹K/V
    - æ³¨æ„åŠ›æƒé‡å¯è§£é‡Š: å±•ç¤ºæ¨¡å‹å¯¹å„é™æ€ç‰¹å¾çš„å…³æ³¨ç¨‹åº¦
    - æ”¯æŒæå–æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–åˆ†æ

    è¾“å…¥æ ¼å¼:
    x: [num_nodes, hist_len, in_dim]
    å…¶ä¸­ in_dim = static_dim(12) + dynamic_dim(12) + temporal_dim(4)
    """

    def __init__(self, config, arch_arg):
        super(GAT_SeparateEncoder, self).__init__()

        # ä¿å­˜é…ç½®
        self.config = config

        # ä»configè·å–ç‰¹å¾åˆ†ç¦»å‚æ•°
        self.use_feature_separation = getattr(
            config, 'use_feature_separation', True
        )
        self.static_dim = getattr(config, 'static_encoded_dim', 8)
        self.dynamic_dim = len(getattr(
            config, 'dynamic_feature_indices', list(range(12))
        ))
        self.temporal_dim = (
            config.temporal_features if config.add_temporal_encoding else 0
        )

        # æ¨¡å‹å‚æ•°
        self.hid_dim = arch_arg.hid_dim
        self.nGAT_layer = arch_arg.GAT_layer
        self.heads = arch_arg.heads
        self.out_dim = config.pred_len

        AF = whichAF(arch_arg.AF)

        # ==================== ğŸ”¥ å¯å­¦ä¹ èŠ‚ç‚¹åµŒå…¥ ====================
        # æ•è·æœªè¢«æ•°æ®è®°å½•çš„éšå¼ç«™ç‚¹ç‰¹å¾ï¼ˆå¦‚å¾®æ°”å€™æ•ˆåº”ï¼‰
        self.node_emb_dim = getattr(arch_arg, 'node_emb_dim', 4)  # é»˜è®¤4ç»´
        self.use_node_embedding = getattr(arch_arg, 'use_node_embedding', True)

        if self.use_node_embedding:
            # åˆå§‹åŒ–å¯è®­ç»ƒçš„èŠ‚ç‚¹åµŒå…¥ [num_nodes, node_emb_dim]
            self.node_embedding = nn.Parameter(
                torch.randn(config.node_num, self.node_emb_dim) * 0.01
            )
        else:
            self.node_embedding = None

        # ==================== åˆ†ç¦»å¼ç¼–ç å™¨ ====================
        # åŠ¨æ€ç¼–ç å™¨ï¼ˆLSTMå¤„ç†æ—¶åºç‰¹å¾ï¼‰
        dynamic_input_dim = self.dynamic_dim + self.temporal_dim
        self.dynamic_encoder = DynamicEncoder(
            input_dim=dynamic_input_dim,
            hidden_dim=self.hid_dim // 2,
            num_layers=arch_arg.lstm_num_layers,
            dropout=arch_arg.lstm_dropout,
            bidirectional=arch_arg.lstm_bidirectional
        )

        # ==================== ğŸ”¥ v3.0: ç‰¹å¾çº§äº¤å‰æ³¨æ„åŠ›èåˆ ====================
        # 12ä¸ªé™æ€ç‰¹å¾ä½œä¸ºç‹¬ç«‹çš„K/Vï¼Œæ”¯æŒæå–ç‰¹å¾çº§æ³¨æ„åŠ›æƒé‡
        fusion_num_heads = getattr(arch_arg, 'fusion_num_heads', 4)
        fusion_use_pre_ln = getattr(arch_arg, 'fusion_use_pre_ln', True)

        # é™æ€ç‰¹å¾æ•°é‡ï¼ˆåŒ…å«èŠ‚ç‚¹åµŒå…¥ï¼‰
        num_static_features = self.static_dim
        if self.use_node_embedding:
            num_static_features += self.node_emb_dim
        self.num_static_features = num_static_features

        self.fusion = CrossAttentionFusionV2(
            num_static_features=num_static_features,
            dynamic_dim=self.hid_dim // 2,
            output_dim=self.hid_dim,
            num_heads=fusion_num_heads,
            dropout=arch_arg.inter_drop,
            use_pre_ln=fusion_use_pre_ln
        )

        # ==================== GATå±‚ ====================
        GAT_layers = []

        # è®¡ç®—æ¯ä¸ªGATå—çš„å±‚æ•°
        self.element = 3  # GAT + Linear + AF
        if arch_arg.norm_type and arch_arg.norm_type != 'None':
            self.element += 1
        if arch_arg.dropout:
            self.element += 1

        for n in range(self.nGAT_layer):
            GAT_layers.append(
                GATv2Conv(
                    self.hid_dim, self.hid_dim,
                    heads=self.heads, concat=True,
                    dropout=arch_arg.intra_drop,
                    add_self_loops=True, share_weights=False
                )
            )
            GAT_layers.append(
                nn.Linear(self.heads * self.hid_dim, self.hid_dim))
            GAT_layers.append(AF)

            norm_layer = get_norm_layer(arch_arg.norm_type, self.hid_dim)
            if norm_layer is not None:
                GAT_layers.append(norm_layer)
            if arch_arg.dropout:
                GAT_layers.append(nn.Dropout(arch_arg.inter_drop))

        self.GAT_layers = nn.ModuleList(GAT_layers)

        # ==================== ğŸ”¥ æ–°å¢2: æ®‹å·®è¿æ¥æ§åˆ¶ ====================
        # åœ¨GATè¾“å…¥ï¼ˆèåˆè¾“å‡ºï¼‰å’ŒGATè¾“å‡ºä¹‹é—´æ·»åŠ è·³è·ƒè¿æ¥
        self.use_skip_connection = getattr(
            arch_arg, 'use_skip_connection', True)

        # ==================== è§£ç å™¨ ====================
        self.use_recurrent_decoder = getattr(
            arch_arg, 'use_recurrent_decoder', True
        )
        self.decoder_type = getattr(arch_arg, 'decoder_type', 'LSTM')
        self.decoder_use_context = getattr(
            arch_arg, 'decoder_use_context', False
        )

        if self.use_recurrent_decoder:
            decoder_num_layers = getattr(arch_arg, 'decoder_num_layers', 1)
            decoder_dropout = getattr(arch_arg, 'decoder_dropout', 0.1)
            decoder_dropout = decoder_dropout if decoder_num_layers > 1 else 0
            decoder_mlp_layers = getattr(arch_arg, 'decoder_mlp_layers', 0)

            decoder_input_size = (
                1 + self.hid_dim if self.decoder_use_context else 1
            )

            if decoder_mlp_layers > 0:
                decoder_mlp = []
                mlp_input_size = decoder_input_size
                for _ in range(decoder_mlp_layers):
                    decoder_mlp.append(nn.Linear(mlp_input_size, self.hid_dim))
                    decoder_mlp.append(nn.ReLU())
                    mlp_input_size = self.hid_dim
                self.decoder_mlp = nn.Sequential(*decoder_mlp)
                decoder_input_size = self.hid_dim
            else:
                self.decoder_mlp = None

            if self.decoder_type == 'LSTM':
                self.decoder = nn.LSTM(
                    input_size=decoder_input_size,
                    hidden_size=self.hid_dim,
                    num_layers=decoder_num_layers,
                    dropout=decoder_dropout,
                    batch_first=False
                )
            elif self.decoder_type == 'GRU':
                self.decoder = nn.GRU(
                    input_size=decoder_input_size,
                    hidden_size=self.hid_dim,
                    num_layers=decoder_num_layers,
                    dropout=decoder_dropout,
                    batch_first=False
                )
            else:
                raise ValueError(f"æœªçŸ¥çš„è§£ç å™¨ç±»å‹: {self.decoder_type}")

            self.decoder_output_proj = nn.Linear(self.hid_dim, 1)
            self.decoder_init_proj = nn.Linear(self.hid_dim, 1)
        else:
            # MLPè¾“å‡ºå±‚
            MLP_layers_out = []
            for n in range(arch_arg.MLP_layer):
                MLP_layers_out.append(nn.Linear(self.hid_dim, self.hid_dim))
                MLP_layers_out.append(AF)
            MLP_layers_out.append(nn.Linear(self.hid_dim, self.out_dim))
            self.MLP_layers_out = nn.Sequential(*MLP_layers_out)

        # ==================== RevIN å±‚ï¼ˆæ–°å¢ï¼‰â­ ====================
        # ç”¨äºå¤„ç†éå¹³ç¨³æ—¶é—´åºåˆ—çš„åˆ†å¸ƒåç§»é—®é¢˜
        self.use_revin = getattr(arch_arg, 'use_revin', False)
        if self.use_revin:
            # ä»…å¯¹åŠ¨æ€ç‰¹å¾åº”ç”¨ RevINï¼ˆé™æ€ç‰¹å¾ä¸éšæ—¶é—´å˜åŒ–ï¼‰
            # dynamic_dim: åŠ¨æ€æ°”è±¡è¦ç´ æ•°é‡
            # temporal_dim: æ—¶é—´ç¼–ç ç»´åº¦ï¼ˆsin/cosï¼‰
            revin_num_features = self.dynamic_dim + self.temporal_dim
            self.revin_layer = RevIN(
                num_features=revin_num_features,
                eps=getattr(arch_arg, 'revin_eps', 1e-5),
                affine=getattr(arch_arg, 'revin_affine', True),
                subtract_last=getattr(arch_arg, 'revin_subtract_last', False)
            )
            print(f"âœ“ RevIN å·²å¯ç”¨ (ç‰¹å¾æ•°={revin_num_features}, "
                  f"affine={self.revin_layer.affine}, "
                  f"subtract_last={self.revin_layer.subtract_last})")
        else:
            self.revin_layer = None

    def forward(self, x, edge_index, edge_attr=None, return_cross_attention=False):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: [batch_size * num_nodes, hist_len, in_dim] æˆ– [num_nodes, hist_len, in_dim]
               å…¶ä¸­ in_dim = static_dim + dynamic_dim + temporal_dim
               æ•°æ®æ ¼å¼: [é™æ€ç‰¹å¾(12), åŠ¨æ€ç‰¹å¾(12), æ—¶é—´ç¼–ç (4)]
            edge_index: [2, num_edges]
            edge_attr: è¾¹å±æ€§ï¼ˆå¯é€‰ï¼‰
            return_cross_attention: æ˜¯å¦è¿”å›Cross-Attentionæƒé‡ï¼ˆç”¨äºå¯è§£é‡Šæ€§åˆ†æï¼‰

        Returns:
            å¦‚æœ return_cross_attention=False:
                [batch_size * num_nodes, pred_len] é¢„æµ‹ç»“æœ
            å¦‚æœ return_cross_attention=True:
                (é¢„æµ‹ç»“æœ, Cross-Attentionæƒé‡ [N, num_heads, num_static_features])
        """
        total_nodes, hist_len, in_dim = x.shape

        # ==================== 1. ç‰¹å¾åˆ†ç¦» ====================
        # ä»ç»„åˆè¾“å…¥ä¸­åˆ†ç¦»é™æ€å’ŒåŠ¨æ€éƒ¨åˆ†
        # è¾“å…¥æ ¼å¼: [é™æ€ç‰¹å¾(12), åŠ¨æ€ç‰¹å¾(12), æ—¶é—´ç¼–ç (4)]
        # [total_nodes, static_dim]
        static_features = x[:, 0, :self.static_dim]
        # [total_nodes, hist_len, dynamic_dim+temporal_dim]
        dynamic_features = x[:, :, self.static_dim:]

        # ==================== æ–°å¢: RevIN æ ‡å‡†åŒ– â­ ====================
        if self.use_revin:
            # ä»…å¯¹åŠ¨æ€ç‰¹å¾åº”ç”¨ RevINï¼ˆæ—¶é—´ç¼–ç ä¹ŸåŒ…å«åœ¨å†…ï¼‰
            # dynamic_features: [total_nodes, hist_len, dynamic_dim+temporal_dim]
            dynamic_features = self.revin_layer.normalize(dynamic_features)

        # ==================== 2. èŠ‚ç‚¹åµŒå…¥å¢å¼º â­ ====================
        if self.use_node_embedding:
            # è®¡ç®—å®é™…çš„èŠ‚ç‚¹æ•°å’Œæ‰¹æ¬¡å¤§å°
            batch_size = total_nodes // self.node_embedding.shape[0]

            # å°†èŠ‚ç‚¹åµŒå…¥æ‰©å±•åˆ°æ‰¹æ¬¡ç»´åº¦
            node_emb_expanded = self.node_embedding.unsqueeze(0).expand(
                batch_size, -1, -1
            ).reshape(total_nodes, -1)

            # å°†å¯å­¦ä¹ çš„èŠ‚ç‚¹åµŒå…¥ä¸é™æ€ç‰¹å¾æ‹¼æ¥
            # [total_nodes, static_dim + node_emb_dim]
            static_features = torch.cat(
                [static_features, node_emb_expanded], dim=-1)

        # ==================== 3. åŠ¨æ€ç¼–ç  ====================
        dynamic_emb = self.dynamic_encoder(
            dynamic_features)  # [total_nodes, hid_dim//2]

        # ==================== 4. ç‰¹å¾çº§äº¤å‰æ³¨æ„åŠ›èåˆ (v3.0) â­ ====================
        # åŠ¨æ€ç‰¹å¾ä½œä¸ºQueryï¼ŒæŸ¥è¯¢æœ€ç›¸å…³çš„é™æ€åœ°ç†ä¿¡æ¯
        # è¿”å›èåˆè¡¨ç¤º + å¯é€‰çš„ç‰¹å¾çº§æ³¨æ„åŠ›æƒé‡
        if return_cross_attention:
            fusion_out, cross_attn_weights = self.fusion(
                static_features, dynamic_emb, return_attention=True
            )
            # cross_attn_weights: [N, num_heads, num_static_features]
        else:
            fusion_out = self.fusion(
                static_features, dynamic_emb, return_attention=False)

        # ==================== 5. GATå›¾å·ç§¯ï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰â­ ====================
        x = fusion_out  # ä¿å­˜èåˆè¾“å‡ºï¼Œç”¨äºåç»­æ®‹å·®è¿æ¥

        for i in range(self.nGAT_layer):
            base_idx = i * self.element
            x = self.GAT_layers[base_idx](x, edge_index)
            for j in range(1, self.element):
                x = self.GAT_layers[base_idx + j](x)

        # ğŸ”¥ æ–°å¢: æ®‹å·®è¿æ¥ï¼ˆé˜²æ­¢è¿‡åº¦å¹³æ»‘ï¼Œä¿ç•™ç«™ç‚¹è‡ªèº«å†å²è¶‹åŠ¿ï¼‰
        if self.use_skip_connection:
            # Element-wise Addition: GATè¾“å‡º + èåˆè¾“å‡º
            x = x + fusion_out

        # ==================== 6. è§£ç å™¨ ====================
        if self.use_recurrent_decoder:
            outputs = []
            encoder_context = x

            num_layers = self.decoder.num_layers
            if self.decoder_type == 'LSTM':
                h_0 = x.unsqueeze(0).repeat(num_layers, 1, 1)
                c_0 = h_0.clone()
                hidden = (h_0, c_0)
            else:
                hidden = x.unsqueeze(0).repeat(num_layers, 1, 1)

            prev_pred = self.decoder_init_proj(x)

            for t in range(self.out_dim):
                if self.decoder_use_context:
                    decoder_input = torch.cat(
                        [prev_pred, encoder_context], dim=1
                    )
                else:
                    decoder_input = prev_pred

                if self.decoder_mlp is not None:
                    decoder_input = self.decoder_mlp(decoder_input)

                decoder_input = decoder_input.unsqueeze(0)
                decoder_output, hidden = self.decoder(decoder_input, hidden)

                pred_t = self.decoder_output_proj(decoder_output.squeeze(0))
                outputs.append(pred_t)
                prev_pred = pred_t

            x = torch.cat(outputs, dim=1)
        else:
            x = self.MLP_layers_out(x)

        # ==================== æ–°å¢: RevIN åæ ‡å‡†åŒ– â­ ====================
        if self.use_revin:
            # è®¡ç®—ç›®æ ‡ç‰¹å¾åœ¨åŠ¨æ€ç‰¹å¾ä¸­çš„ç´¢å¼•
            # config.target_feature_idx æ˜¯å…¨å±€ç´¢å¼•ï¼ˆ0-27ï¼‰
            # éœ€è¦æ˜ å°„åˆ°åŠ¨æ€ç‰¹å¾ä¸­çš„ç´¢å¼•ï¼ˆ0-9ï¼‰
            target_global_idx = self.config.target_feature_idx
            dynamic_indices = self.config.dynamic_feature_indices

            if target_global_idx in dynamic_indices:
                target_idx_in_dynamic = dynamic_indices.index(target_global_idx)
            else:
                # å¦‚æœç›®æ ‡ä¸åœ¨åŠ¨æ€ç‰¹å¾ä¸­ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªåŠ¨æ€ç‰¹å¾çš„ç»Ÿè®¡é‡
                print(f"è­¦å‘Š: ç›®æ ‡ç‰¹å¾ç´¢å¼• {target_global_idx} ä¸åœ¨åŠ¨æ€ç‰¹å¾åˆ—è¡¨ä¸­ï¼Œ"
                      f"ä½¿ç”¨ç¬¬ä¸€ä¸ªåŠ¨æ€ç‰¹å¾çš„ç»Ÿè®¡é‡")
                target_idx_in_dynamic = 0

            # æ‰©å±•è¾“å‡ºç»´åº¦: [total_nodes, pred_len] â†’ [total_nodes, pred_len, 1]
            output_expanded = x.unsqueeze(-1)

            # æå–ç›®æ ‡ç‰¹å¾çš„ç»Ÿè®¡é‡
            # mean/stdev å½¢çŠ¶: [total_nodes, 1, dynamic_dim+temporal_dim]
            # é€‰æ‹©ç¬¬ target_idx_in_dynamic ä¸ªç‰¹å¾: [total_nodes, 1, 1]
            mean_target = self.revin_layer.mean[:, :, target_idx_in_dynamic:target_idx_in_dynamic+1]
            stdev_target = self.revin_layer.stdev[:, :, target_idx_in_dynamic:target_idx_in_dynamic+1]

            # åä»¿å°„å˜æ¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.revin_layer.affine:
                gamma_target = self.revin_layer.gamma[target_idx_in_dynamic]
                beta_target = self.revin_layer.beta[target_idx_in_dynamic]
                output_expanded = (output_expanded - beta_target) / gamma_target

            # åæ ‡å‡†åŒ–: output * stdev + mean
            output_expanded = output_expanded * stdev_target + mean_target

            # å‹ç¼©å›åŸå§‹å½¢çŠ¶: [total_nodes, pred_len, 1] â†’ [total_nodes, pred_len]
            x = output_expanded.squeeze(-1)

        # è¿”å›ç»“æœ
        if return_cross_attention:
            return x, cross_attn_weights
        else:
            return x


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=" * 70)
    print("GAT_SeparateEncoder æ¨¡å‹æµ‹è¯• (v3.0 - ç‰¹å¾çº§æ³¨æ„åŠ›)")
    print("=" * 70)

    # æ¨¡æ‹Ÿé…ç½®
    class MockConfig:
        in_dim = 28  # 12(é™æ€) + 12(åŠ¨æ€) + 4(æ—¶é—´ç¼–ç )
        pred_len = 3
        node_num = 28
        use_feature_separation = True
        static_encoded_dim = 12  # 12ä¸ªé™æ€ç‰¹å¾
        dynamic_feature_indices = [3, 4, 5, 6, 7, 8, 9, 19, 20, 21, 22, 23]
        add_temporal_encoding = True
        temporal_features = 4

    class MockArchArg:
        hid_dim = 32
        MLP_layer = 1
        AF = 'ReLU'
        norm_type = 'LayerNorm'
        dropout = True
        GAT_layer = 2
        heads = 2
        intra_drop = 0.1
        inter_drop = 0.1
        lstm_num_layers = 1
        lstm_dropout = 0.1
        lstm_bidirectional = False
        use_recurrent_decoder = True
        decoder_type = 'LSTM'
        decoder_num_layers = 1
        decoder_dropout = 0.1
        decoder_use_context = False
        decoder_mlp_layers = 0

        # v3.0 å‚æ•°
        use_node_embedding = True   # å¯ç”¨èŠ‚ç‚¹åµŒå…¥
        node_emb_dim = 4            # èŠ‚ç‚¹åµŒå…¥ç»´åº¦
        fusion_num_heads = 4        # äº¤å‰æ³¨æ„åŠ›å¤´æ•°
        fusion_use_pre_ln = True    # ä½¿ç”¨Pre-LN
        use_skip_connection = True  # å¯ç”¨æ®‹å·®è¿æ¥

    config = MockConfig()
    arch_arg = MockArchArg()

    print(f"\n{'='*50}")
    print("æµ‹è¯• v3.0 ç‰¹å¾çº§æ³¨æ„åŠ›æ¨¡å‹")
    print(f"  - èŠ‚ç‚¹åµŒå…¥: {'å¯ç”¨' if arch_arg.use_node_embedding else 'ç¦ç”¨'}")
    print(f"  - é™æ€ç‰¹å¾æ•°: {config.static_encoded_dim}")
    print(f"  - äº¤å‰æ³¨æ„åŠ›: {arch_arg.fusion_num_heads} å¤´")
    print(f"  - æ®‹å·®è¿æ¥: {'å¯ç”¨' if arch_arg.use_skip_connection else 'ç¦ç”¨'}")
    print(f"{'='*50}")

    model = GAT_SeparateEncoder(config, arch_arg)
    model.eval()  # Set to evaluation mode for testing

    # æ¨¡æ‹Ÿè¾“å…¥
    batch_size = 28
    hist_len = 7
    x = torch.randn(batch_size, hist_len, config.in_dim)
    edge_index = torch.randint(0, batch_size, (2, 100))

    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")

    # Test 1: Normal forward pass
    print("\n[Test 1] Normal forward pass")
    out = model(x, edge_index)
    print(f"  Output shape: {out.shape}")
    print(f"  Expected shape: [{batch_size}, {config.pred_len}]")
    assert out.shape == (
        batch_size, config.pred_len), f"Shape mismatch: {out.shape}"
    print("  [OK] Shape validation passed")

    # Test 2: Forward pass with attention weights
    print("\n[Test 2] Forward pass with Cross-Attention weights")
    out, attn_weights = model(x, edge_index, return_cross_attention=True)
    print(f"  Output shape: {out.shape}")
    print(f"  Attention weights shape: {attn_weights.shape}")
    num_static_features = config.static_encoded_dim + arch_arg.node_emb_dim
    expected_attn_shape = (
        batch_size, arch_arg.fusion_num_heads, num_static_features)
    print(f"  Expected attention shape: {expected_attn_shape}")
    assert attn_weights.shape == expected_attn_shape, \
        f"Attention shape mismatch: {attn_weights.shape}"
    print("  [OK] Attention weights shape validation passed")

    # Verify attention weights are normalized (sum to 1)
    attn_sum = attn_weights[0, 0, :].sum().item()
    print(f"  Attention weights sum (should be ~1.0): {attn_sum:.4f}")
    assert abs(
        attn_sum - 1.0) < 0.01, f"Attention weights not normalized: {attn_sum}"
    print("  [OK] Attention weights normalization passed")

    # Parameter statistics
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel()
                           for p in model.parameters() if p.requires_grad)
    print(f"\nParameter Statistics:")
    print(f"  - Total parameters: {total_params:,}")
    print(f"  - Trainable parameters: {trainable_params:,}")

    # Check node embeddings
    if arch_arg.use_node_embedding:
        print(f"  - Node embedding shape: {model.node_embedding.shape}")
        print(f"  - Node embedding params: {model.node_embedding.numel()}")

    # Check static features count
    print(f"  - Static features (with node emb): {model.num_static_features}")

    print("\n" + "=" * 70)
    print("All tests passed!")
    print("=" * 70)
