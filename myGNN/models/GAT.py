"""
GAT (Graph Attention Network) + LSTM æ¨¡å‹

ä¿®å¤å†…å®¹ï¼š
1. ä¿®å¤whichAFå‡½æ•°è°ƒç”¨ï¼ˆåˆ é™¤å¤šä½™çš„hid_dimå‚æ•°ï¼‰
2. ä¿®å¤forwardè¿”å›å€¼ï¼ˆåªè¿”å›xï¼Œä¸è¿”å›attentionï¼‰
3. ä¿®å¤å¾ªç¯ç´¢å¼•é—®é¢˜ï¼ˆç¬¬60-62è¡Œï¼‰

ä½œè€…: GNNæ°”æ¸©é¢„æµ‹é¡¹ç›®
æ—¥æœŸ: 2025
"""

import torch
import torch.nn as nn
from torch_geometric.nn import GATv2Conv


def get_norm_layer(norm_type, dim):
    """
    è§„èŒƒåŒ–å±‚é€‰æ‹©

    Args:
        norm_type: è§„èŒƒåŒ–ç±»å‹ ('BatchNorm', 'LayerNorm', 'None')
        dim: ç‰¹å¾ç»´åº¦

    Returns:
        è§„èŒƒåŒ–å±‚æˆ– None
    """
    if norm_type == 'BatchNorm':
        return nn.BatchNorm1d(dim)
    elif norm_type == 'LayerNorm':
        return nn.LayerNorm(dim)
    elif norm_type == 'None' or norm_type is None:
        return None
    else:
        raise ValueError(f"æœªçŸ¥çš„è§„èŒƒåŒ–ç±»å‹: {norm_type}")


class GAT_LSTM(torch.nn.Module):
    """
    GAT + LSTM æ¨¡å‹

    ç»“æ„:
    1. MLPè¾“å…¥å±‚ (hist_len, in_dim) â†’ (hist_len, hid_dim)
    2. LSTMæ—¶åºå»ºæ¨¡ (hist_len, hid_dim) â†’ (hid_dim,)
    3. GATå›¾å·ç§¯å±‚ x N (hid_dim) â†’ (hid_dim)
    4. MLPè¾“å‡ºå±‚ (hid_dim) â†’ (pred_len,)
    """

    def __init__(self, config, arch_arg):
        super(GAT_LSTM, self).__init__()
        self.in_dim = config.in_dim
        self.nMLP_layer = arch_arg.MLP_layer
        self.nGAT_layer = arch_arg.GAT_layer
        self.hid_dim = arch_arg.hid_dim
        self.heads = arch_arg.heads
        AF = whichAF(arch_arg.AF)  # ä¿®å¤ï¼šåˆ é™¤hid_dimå‚æ•°
        self.out_dim = config.pred_len

        MLP_layers_in = [nn.Linear(self.in_dim, self.hid_dim)]
        GAT_layers = []
        MLP_layers_out = []

        # è®¡ç®—æ¯ä¸ªGATå—çš„å±‚æ•°
        self.element = 3  # GAT + Linear + AF
        if arch_arg.norm_type != 'None':
            self.element += 1
        if arch_arg.dropout:
            self.element += 1

        # MLPè¾“å…¥å±‚
        for n in range(self.nMLP_layer):
            MLP_layers_in.append(nn.Linear(self.hid_dim, self.hid_dim))
            MLP_layers_in.append(AF)

        # GATå±‚
        for n in range(self.nGAT_layer):
            in_dim = self.hid_dim
            out_dim = self.hid_dim
            GAT_layers.append(
                GATv2Conv(
                    in_dim, out_dim,
                    heads=self.heads, concat=True,
                    dropout=arch_arg.intra_drop,
                    add_self_loops=False, share_weights=False
                )
            )
            GAT_layers.append(nn.Linear(self.heads * out_dim, out_dim))
            GAT_layers.append(AF)
            # æ·»åŠ è§„èŒƒåŒ–å±‚
            norm_layer = get_norm_layer(arch_arg.norm_type, out_dim)
            if norm_layer is not None:
                GAT_layers.append(norm_layer)
            if arch_arg.dropout:
                GAT_layers.append(nn.Dropout(arch_arg.inter_drop))

        self.MLP_layers_in = nn.Sequential(*MLP_layers_in)

        # LSTMå±‚ï¼ˆä½¿ç”¨é…ç½®å‚æ•°ï¼‰
        self.lstm_bidirectional = arch_arg.lstm_bidirectional
        self.lstm = nn.LSTM(
            input_size=self.hid_dim,
            hidden_size=self.hid_dim,
            num_layers=arch_arg.lstm_num_layers*2,
            dropout=arch_arg.lstm_dropout if arch_arg.lstm_num_layers > 1 else 0,
            bidirectional=arch_arg.lstm_bidirectional,
            batch_first=False
        )
        # åŒå‘LSTMéœ€è¦æŠ•å½±å±‚å°†2*hid_dimæ˜ å°„å›hid_dim
        if arch_arg.lstm_bidirectional:
            self.lstm_fc = nn.Linear(self.hid_dim * 2, self.hid_dim)

        self.GAT_layers = nn.ModuleList(GAT_layers)

        # ğŸ”‘ å¾ªç¯è§£ç å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºå¤šæ­¥é¢„æµ‹ï¼‰
        self.use_recurrent_decoder = getattr(
            arch_arg, 'use_recurrent_decoder', False
        )
        self.decoder_type = getattr(arch_arg, 'decoder_type', 'LSTM')
        self.decoder_use_context = getattr(
            arch_arg, 'decoder_use_context', False
        )

        if self.use_recurrent_decoder:
            decoder_num_layers = getattr(arch_arg, 'decoder_num_layers', 1)
            decoder_dropout = getattr(arch_arg, 'decoder_dropout', 0.1)
            decoder_dropout = decoder_dropout if decoder_num_layers > 1 else 0
            decoder_mlp_layers = getattr(arch_arg, 'decoder_mlp_layers', 0)

            # è§£ç å™¨è¾“å…¥ç»´åº¦
            decoder_input_size = (
                1 + self.hid_dim if self.decoder_use_context else 1
            )

            # è§£ç å™¨å‰ç½®MLPå±‚ï¼ˆå¯é€‰ï¼‰
            if decoder_mlp_layers > 0:
                decoder_mlp = []
                mlp_input_size = decoder_input_size
                for _ in range(decoder_mlp_layers):
                    decoder_mlp.append(nn.Linear(mlp_input_size, self.hid_dim))
                    decoder_mlp.append(nn.ReLU())
                    mlp_input_size = self.hid_dim
                self.decoder_mlp = nn.Sequential(*decoder_mlp)
                decoder_input_size = self.hid_dim
            else:
                self.decoder_mlp = None

            # è§£ç å™¨RNN
            if self.decoder_type == 'LSTM':
                self.decoder = nn.LSTM(
                    input_size=decoder_input_size,
                    hidden_size=self.hid_dim,
                    num_layers=decoder_num_layers,
                    dropout=decoder_dropout,
                    batch_first=False
                )
            elif self.decoder_type == 'GRU':
                self.decoder = nn.GRU(
                    input_size=decoder_input_size,
                    hidden_size=self.hid_dim,
                    num_layers=decoder_num_layers,
                    dropout=decoder_dropout,
                    batch_first=False
                )
            else:
                raise ValueError(
                    f"æœªçŸ¥çš„è§£ç å™¨ç±»å‹: {self.decoder_type}ï¼Œæ”¯æŒ: 'LSTM', 'GRU'"
                )

            self.decoder_output_proj = nn.Linear(self.hid_dim, 1)
            self.decoder_init_proj = nn.Linear(self.hid_dim, 1)
        else:
            # MLPè¾“å‡ºå±‚ï¼ˆåŸæœ‰æ–¹å¼ï¼‰
            for n in range(self.nMLP_layer):
                MLP_layers_out.append(nn.Linear(self.hid_dim, self.hid_dim))
                MLP_layers_out.append(AF)
            MLP_layers_out.append(nn.Linear(self.hid_dim, self.out_dim))
            self.MLP_layers_out = nn.Sequential(*MLP_layers_out)

    def forward(self, x, edge_index, edge_attr=None):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: [num_nodes, hist_len, in_dim]
            edge_index: [2, num_edges]
            edge_attr: è¾¹å±æ€§ï¼ˆGATv2Convä¸ä½¿ç”¨ï¼Œä¸ºäº†æ¥å£ç»Ÿä¸€ï¼‰

        Returns:
            x: [num_nodes, pred_len]
        """
        # 1. è½¬æ¢ç»´åº¦ç”¨äºLSTM: [num_nodes, hist_len, in_dim] â†’ [hist_len, num_nodes, in_dim]
        x = x.permute(1, 0, 2)

        # 2. MLPè¾“å…¥å±‚
        x = self.MLP_layers_in(x)  # [hist_len, num_nodes, hid_dim]

        # 3. LSTMæ—¶åºå»ºæ¨¡
        out, _ = self.lstm(x)  # [hist_len, num_nodes, hid_dim] æˆ– [hist_len, num_nodes, 2*hid_dim]
        x = out.contiguous()
        x = x[-1]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ [num_nodes, hid_dim] æˆ– [num_nodes, 2*hid_dim]

        # åŒå‘LSTMæŠ•å½±
        if self.lstm_bidirectional:
            x = self.lstm_fc(x)  # [num_nodes, hid_dim]

        # 4. GATå›¾å·ç§¯å±‚
        for i in range(self.nGAT_layer):
            # ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—GATå±‚çš„ç´¢å¼•
            base_idx = i * self.element

            # GATå·ç§¯ï¼ˆä¸éœ€è¦è¿”å›attentionï¼‰
            x = self.GAT_layers[base_idx](x, edge_index)  # [num_nodes, heads*hid_dim]

            # åç»­å±‚ï¼ˆLinear, AF, BN?, Dropout?ï¼‰
            for j in range(1, self.element):
                x = self.GAT_layers[base_idx + j](x)

        # 5. è¾“å‡ºç”Ÿæˆ
        if self.use_recurrent_decoder:
            # ğŸ”‘ å¾ªç¯è§£ç å™¨ï¼šé€æ­¥ç”Ÿæˆé¢„æµ‹åºåˆ—
            outputs = []
            encoder_context = x  # [N, hid_dim]

            num_layers = self.decoder.num_layers
            if self.decoder_type == 'LSTM':
                h_0 = x.unsqueeze(0).repeat(num_layers, 1, 1)
                c_0 = torch.zeros_like(h_0)
                hidden = (h_0, c_0)
            else:
                hidden = x.unsqueeze(0).repeat(num_layers, 1, 1)

            prev_pred = self.decoder_init_proj(x)  # [N, 1]

            for t in range(self.out_dim):
                if self.decoder_use_context:
                    decoder_input = torch.cat(
                        [prev_pred, encoder_context], dim=1
                    )
                else:
                    decoder_input = prev_pred

                if self.decoder_mlp is not None:
                    decoder_input = self.decoder_mlp(decoder_input)

                decoder_input = decoder_input.unsqueeze(0)
                decoder_output, hidden = self.decoder(decoder_input, hidden)

                pred_t = self.decoder_output_proj(decoder_output.squeeze(0))
                outputs.append(pred_t)
                prev_pred = pred_t

            x = torch.cat(outputs, dim=1)  # [N, pred_len]
        else:
            # MLPè¾“å‡ºå±‚ï¼ˆåŸæœ‰æ–¹å¼ï¼‰
            x = self.MLP_layers_out(x)  # [num_nodes, pred_len]

        return x


def whichAF(AF):
    """
    æ¿€æ´»å‡½æ•°é€‰æ‹©

    Args:
        AF: æ¿€æ´»å‡½æ•°åç§°

    Returns:
        æ¿€æ´»å‡½æ•°å±‚
    """
    if AF == 'PReLU':
        return nn.PReLU()  # ä¿®å¤ï¼šæ­£ç¡®è¿”å›PReLU
    elif AF == "LeakyReLU":
        return nn.LeakyReLU()
    elif AF == "PReLUMulti":
        return nn.PReLU()
    elif AF == "ReLU":
        return nn.ReLU()
    else:
        return lambda x: x
