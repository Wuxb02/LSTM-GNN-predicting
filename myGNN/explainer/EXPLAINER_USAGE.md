# GNNå¯è§£é‡Šæ€§åˆ†æä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—æä¾›GNNå¯è§£é‡Šæ€§åˆ†ææ¨¡å—çš„è¯¦ç»†ä½¿ç”¨æ–¹æ³•,åŒ…æ‹¬åŸºç¡€åŠŸèƒ½å’Œæœ€æ–°çš„GATæ³¨æ„åŠ›æ·±åº¦åˆ†æåŠŸèƒ½ã€‚

---

## ğŸ“Œ ç‰ˆæœ¬æ›´æ–°

### v2.1.0 (2025-12-16) â­æœ€æ–°

æœ¬æ¬¡æ›´æ–°æ–°å¢GATæ³¨æ„åŠ›æ·±åº¦åˆ†æåŠŸèƒ½,éªŒè¯æ¨¡å‹å­¦ä¹ çš„ç©ºé—´ä¾èµ–æ­£ç¡®æ€§:

**æ–°å¢åŠŸèƒ½:**
- âœ¨ **å…¨å±€æ³¨æ„åŠ›çŸ©é˜µå¯è§†åŒ–**: 28Ã—28çƒ­åŠ›å›¾,å±•ç¤ºæ‰€æœ‰ç«™ç‚¹å¯¹çš„æ³¨æ„åŠ›å¼ºåº¦
- âœ¨ **è·ç¦»-æ³¨æ„åŠ›å…³ç³»åˆ†æ**: ä½¿ç”¨Haversineå…¬å¼è®¡ç®—åœ°ç†è·ç¦»,éªŒè¯ç‰©ç†è§„å¾‹
- âœ¨ **æ¸©åº¦ç›¸å…³æ€§-æ³¨æ„åŠ›å…³ç³»åˆ†æ**: åŸºäºè®­ç»ƒé›†æ¸©åº¦æ•°æ®,éªŒè¯æ°”è±¡æ¨¡å¼
- âœ¨ **æ–°å¢å·¥å…·å‡½æ•°**: `haversine_distance()`, `compute_edge_distances()`, `compute_temperature_correlation()`, `extract_edge_correlations()`, `edge_attention_to_matrix()`
- âœ¨ **3ç§æ–°å¯è§†åŒ–å›¾è¡¨**: æ³¨æ„åŠ›çŸ©é˜µçƒ­åŠ›å›¾ã€è·ç¦»-æ³¨æ„åŠ›æ•£ç‚¹å›¾ã€ç›¸å…³æ€§-æ³¨æ„åŠ›æ•£ç‚¹å›¾
- ğŸ“Š **å¯è§†åŒ–æ•°é‡**: ä»8ç§å¢åŠ åˆ°11ç§

**æŠ€æœ¯æ”¹è¿›:**
- ä½¿ç”¨Haversineå…¬å¼ç²¾ç¡®è®¡ç®—çƒé¢è·ç¦»
- åŸºäºè®­ç»ƒé›†(2010-2015)è®¡ç®—æ¸©åº¦ç›¸å…³æ€§,é¿å…æ•°æ®æ³„éœ²
- çº¿æ€§å›å½’+ç»Ÿè®¡æ£€éªŒ(Pearson r, p-value, RÂ²)
- è¾¹çº§æ³¨æ„åŠ›â†’èŠ‚ç‚¹çº§çŸ©é˜µè½¬æ¢(æ”¯æŒmean/max/sumèšåˆ)

### v2.0.0 (2025-12-01)

**æ–°å¢åŠŸèƒ½:**
- âœ¨ å…¨è¾¹å¯è§†åŒ–(overlay/separateæ¨¡å¼)
- âœ¨ GATæ³¨æ„åŠ›æƒé‡æå–ä¸èšåˆ
- âœ¨ GNNExplainer vs GATæ³¨æ„åŠ›å¯¹æ¯”å›¾
- ğŸ”§ é…ç½®å‚æ•°: `extract_attention`, `all_edges_mode`

### v1.0.0 (åˆå§‹ç‰ˆæœ¬)

- æ—¶åºåˆ†æ(Integrated Gradients)
- ç©ºé—´åˆ†æ(GNNExplainer)
- å­£èŠ‚ç­›é€‰åŠŸèƒ½
- 5ç§åŸºç¡€å¯è§†åŒ–å›¾è¡¨

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install captum>=0.6.0 cartopy>=0.21.0 scipy>=1.7.0
```

æˆ–ä½¿ç”¨requirements.txt:
```bash
pip install -r myGNN/requirements.txt
```

**ä¾èµ–è¯´æ˜:**
- `captum`: Integrated Gradientsæ—¶åºåˆ†æ
- `cartopy`: Mapbox WMTSåœ°ç†åº•å›¾
- `scipy`: ç»Ÿè®¡æ£€éªŒ(Pearsonç›¸å…³ã€çº¿æ€§å›å½’) â­æ–°å¢v2.1.0

### 2. åŸºç¡€ä½¿ç”¨

```python
from myGNN.explainer import HybridExplainer, ExplainerConfig
from myGNN.config import create_config
from myGNN.dataset import create_dataloaders
from myGNN.graph.distance_graph import create_graph_from_config
import numpy as np
import torch

# 1. åŠ è½½é…ç½®å’Œæ•°æ®
config, arch_config, loss_config = create_config()
MetData = np.load(config.MetData_fp)
station_info = np.load(config.station_info_fp)

# 2. æ„å»ºå›¾ç»“æ„
graph = create_graph_from_config(config, station_info)

# 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader, val_loader, test_loader = create_dataloaders(
    config, graph, MetData,
    batch_size=config.batch_size,
    shuffle_train=True
)

# 4. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = torch.load('checkpoints/GAT_LSTM_best/best_model.pth')
model.eval()

# 5. é…ç½®è§£é‡Šå™¨(ä½¿ç”¨æ‰€æœ‰æ–°åŠŸèƒ½)
exp_config = ExplainerConfig(
    num_samples=100,           # åˆ†æ100ä¸ªæ ·æœ¬
    epochs=200,                # GNNExplainerè®­ç»ƒè½®æ•°
    season='summer',           # å­£èŠ‚ç­›é€‰(å¯é€‰)
    extract_attention=True,    # æå–GATæ³¨æ„åŠ›æƒé‡
    all_edges_mode='both'      # å…¨è¾¹å¯è§†åŒ–æ¨¡å¼
)

# 6. è¿è¡Œå®Œæ•´åˆ†æ
explainer = HybridExplainer(model, config, exp_config)
explanation = explainer.explain_full(
    test_loader,
    save_path='checkpoints/GAT_LSTM_best/explanations/summer/'
)

# 7. è®¿é—®ç»“æœ
print("=== æ—¶åºåˆ†æç»“æœ ===")
print(f"æœ€é‡è¦çš„æ—¶é—´æ­¥: {torch.argmax(explanation['temporal']['time_importance']).item()}")
print(f"æœ€é‡è¦çš„ç‰¹å¾: {torch.argmax(explanation['temporal']['feature_importance']).item()}")

print("\n=== ç©ºé—´åˆ†æç»“æœ ===")
print("Top-5é‡è¦è¾¹:")
for src, dst, imp in explanation['spatial']['important_edges'][:5]:
    print(f"  ç«™ç‚¹{src} â†’ ç«™ç‚¹{dst}: {imp:.4f}")

# 8. GATæ³¨æ„åŠ›åˆ†æ(å¦‚æœæå–äº†)
if 'attention' in explanation['spatial']:
    attention_mean = explanation['spatial']['attention']['mean']
    print(f"\n=== GATæ³¨æ„åŠ›ç»Ÿè®¡ ===")
    print(f"æ³¨æ„åŠ›å‡å€¼: {attention_mean.mean():.4f}")
    print(f"æ³¨æ„åŠ›æ ‡å‡†å·®: {attention_mean.std():.4f}")
    print(f"æœ€å¤§æ³¨æ„åŠ›: {attention_mean.max():.4f}")
    print(f"æœ€å°æ³¨æ„åŠ›: {attention_mean.min():.4f}")
```

### 3. å‘½ä»¤è¡Œä½¿ç”¨

```bash
# åŸºæœ¬åˆ†æ(åˆ†æ100ä¸ªæ ·æœ¬,ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–)
python myGNN/explain_model.py \
    --model_path checkpoints/GAT_LSTM_best/best_model.pth \
    --num_samples 100 \
    --visualize

# å¤å­£åˆ†æ + GATæ³¨æ„åŠ›æå–
python myGNN/explain_model.py \
    --model_path checkpoints/GAT_LSTM_best/best_model.pth \
    --num_samples 100 \
    --season summer \
    --extract_attention \
    --visualize

# è‡ªå®šä¹‰ä¿å­˜ç›®å½•
python myGNN/explain_model.py \
    --model_path checkpoints/GAT_LSTM_best/best_model.pth \
    --num_samples 100 \
    --save_dir custom_explanations/ \
    --visualize
```

---

## ğŸ“Š ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶

### æ•°æ®æ–‡ä»¶

**explanation_data.npz** - åŒ…å«æ‰€æœ‰åˆ†æç»“æœ:

```python
data = np.load('explanations/explanation_data.npz')

# æ—¶åºåˆ†æç»“æœ
data['time_importance']          # [hist_len] æ—¶é—´æ­¥é‡è¦æ€§
data['feature_importance']       # [in_dim] ç‰¹å¾é‡è¦æ€§
data['temporal_heatmap']         # [hist_len, in_dim] æ—¶ç©ºçƒ­å›¾

# ç©ºé—´åˆ†æç»“æœ
data['edge_importance_mean']     # [num_edges] è¾¹é‡è¦æ€§å‡å€¼
data['edge_importance_std']      # [num_edges] è¾¹é‡è¦æ€§æ ‡å‡†å·®
data['edge_index']               # [2, num_edges] è¾¹ç´¢å¼•

# GATæ³¨æ„åŠ›æƒé‡(v2.0.0+)
data['attention_mean']           # [num_edges] æ³¨æ„åŠ›å‡å€¼
data['attention_std']            # [num_edges] æ³¨æ„åŠ›æ ‡å‡†å·®
```

**important_edges.txt** - Top-Ké‡è¦è¾¹åˆ—è¡¨:

```
ç«™ç‚¹59264 â†’ ç«™ç‚¹59287: 0.8523
ç«™ç‚¹59287 â†’ ç«™ç‚¹59316: 0.8201
ç«™ç‚¹59316 â†’ ç«™ç‚¹59324: 0.7856
...
```

### å¯è§†åŒ–å›¾è¡¨ (11ç§)

**åŸºç¡€å¯è§†åŒ– (5ç§) - v1.0.0**:
1. `temporal_heatmap.png` - æ—¶åºç‰¹å¾çƒ­å›¾ `[hist_len Ã— in_dim]`
2. `spatial_edges.png` - Top-Ké‡è¦è¾¹åœ°ç†å›¾ (Mapboxåº•å›¾)
3. `edge_distribution.png` - è¾¹é‡è¦æ€§åˆ†å¸ƒç›´æ–¹å›¾
4. `time_importance.png` - æ—¶é—´æ­¥é‡è¦æ€§æŸ±çŠ¶å›¾
5. `feature_importance.png` - ç‰¹å¾é‡è¦æ€§æ’åå›¾

**å…¨è¾¹å¯è§†åŒ– (3ç§) - v2.0.0**:
6. `spatial_all_edges_overlay.png` - å…¨è¾¹å åŠ æ¨¡å¼å›¾ (ç°è‰²å…¨è¾¹ + çº¢è‰²Top-K)
7. `spatial_all_edges_separate.png` - å…¨è¾¹åˆ†ç¦»æ¨¡å¼å›¾ (å·¦å³å¯¹æ¯”)
8. `comparison_explainer_vs_attention.png` - GNNExplainer vs GATæ³¨æ„åŠ›å¯¹æ¯”å›¾

**GATæ³¨æ„åŠ›æ·±åº¦åˆ†æ (3ç§) - v2.1.0** â­â­â­:
9. `attention_matrix_heatmap.png` - å…¨å±€æ³¨æ„åŠ›çŸ©é˜µçƒ­åŠ›å›¾ (28Ã—28)
10. `distance_vs_attention.png` - è·ç¦»-æ³¨æ„åŠ›æ•£ç‚¹å›¾ (è¶‹åŠ¿çº¿ + ç»Ÿè®¡æ£€éªŒ)
11. `correlation_vs_attention.png` - æ¸©åº¦ç›¸å…³æ€§-æ³¨æ„åŠ›æ•£ç‚¹å›¾ (RÂ² + på€¼)

---

## ğŸ”¬ é«˜çº§ç”¨æ³•

### 1. GATæ³¨æ„åŠ›æƒé‡æ·±åº¦åˆ†æ â­â­â­ (v2.1.0æ–°å¢)

å¯¹GATæ¨¡å‹å­¦ä¹ çš„æ³¨æ„åŠ›æƒé‡è¿›è¡Œå…¨é¢éªŒè¯:

```python
from myGNN.explainer import SpatialExplainer, ExplainerConfig
from myGNN.explainer.utils import (
    edge_attention_to_matrix,
    compute_edge_distances,
    compute_temperature_correlation,
    extract_edge_correlations,
    haversine_distance
)
from scipy.stats import pearsonr, linregress
import numpy as np
import matplotlib.pyplot as plt

# === æ­¥éª¤1: æå–GATæ³¨æ„åŠ›æƒé‡ ===
exp_config = ExplainerConfig(num_samples=100, extract_attention=True)
spatial_explainer = SpatialExplainer(model, config, exp_config)

attention_result = spatial_explainer.extract_attention_weights_batch(
    test_loader, num_samples=100
)

attention_mean = attention_result['attention_mean']  # [num_edges]
attention_std = attention_result['attention_std']    # [num_edges]
edge_index = attention_result['edge_index']         # [2, num_edges]

print(f"æˆåŠŸæå– {len(attention_mean)} æ¡è¾¹çš„æ³¨æ„åŠ›æƒé‡")
print(f"æ³¨æ„åŠ›å‡å€¼: {attention_mean.mean():.4f} Â± {attention_mean.std():.4f}")

# === æ­¥éª¤2: åˆ†ææ³¨æ„åŠ›-è·ç¦»å…³ç³» ===
# 2.1 è®¡ç®—æ‰€æœ‰è¾¹çš„åœ°ç†è·ç¦»(ä½¿ç”¨Haversineå…¬å¼)
edge_distances = compute_edge_distances(edge_index, station_coords)
print(f"\nè·ç¦»ç»Ÿè®¡: {edge_distances.min():.1f}km - {edge_distances.max():.1f}km")

# 2.2 ç»Ÿè®¡åˆ†æ
r_dist, p_dist = pearsonr(edge_distances, attention_mean.numpy())
slope, intercept, r_value, p_value, std_err = linregress(
    edge_distances, attention_mean.numpy()
)

print(f"\n=== è·ç¦»-æ³¨æ„åŠ›å…³ç³» ===")
print(f"Pearsonç›¸å…³ç³»æ•°: r = {r_dist:.3f}")
print(f"på€¼(æ˜¾è‘—æ€§): p = {p_dist:.2e}")
print(f"çº¿æ€§å›å½’: attention = {slope:.6f} Ã— distance + {intercept:.4f}")
print(f"RÂ²(æ‹Ÿåˆä¼˜åº¦): {r_value**2:.3f}")

if r_dist < 0:
    print("âœ“ è´Ÿç›¸å…³: è·ç¦»è¶Šè¿œ,æ³¨æ„åŠ›è¶Šå°(ç¬¦åˆç‰©ç†è§„å¾‹)")
else:
    print("âš  æ­£ç›¸å…³: éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥æ¨¡å‹")

# 2.3 å¯è§†åŒ–è·ç¦»-æ³¨æ„åŠ›å…³ç³»
from myGNN.explainer.visualize_explainer import plot_distance_vs_attention
plot_distance_vs_attention(
    edge_distances, attention_mean,
    save_path='analysis_distance_vs_attention.png',
    dpi=300
)

# === æ­¥éª¤3: åˆ†ææ³¨æ„åŠ›-æ¸©åº¦ç›¸å…³æ€§å…³ç³» ===
# 3.1 è®¡ç®—è®­ç»ƒé›†æ¸©åº¦ç›¸å…³æ€§çŸ©é˜µ(é¿å…æ•°æ®æ³„éœ²)
weather_data = np.load('data/real_weather_data_2010_2017.npy')
corr_matrix = compute_temperature_correlation(
    weather_data,
    train_indices=(0, 2191),  # ä»…ä½¿ç”¨è®­ç»ƒé›†(2010-2015)
    target_feature_idx=4      # tmaxæœ€é«˜æ°”æ¸©
)
print(f"\næ¸©åº¦ç›¸å…³æ€§çŸ©é˜µå½¢çŠ¶: {corr_matrix.shape}")  # (28, 28)

# 3.2 æå–è¾¹çº§ç›¸å…³ç³»æ•°
edge_corrs = extract_edge_correlations(edge_index, corr_matrix)
print(f"è¾¹çº§ç›¸å…³æ€§ç»Ÿè®¡: {edge_corrs.min():.3f} - {edge_corrs.max():.3f}")

# 3.3 ç»Ÿè®¡åˆ†æ
r_corr, p_corr = pearsonr(edge_corrs, attention_mean.numpy())
print(f"\n=== æ¸©åº¦ç›¸å…³æ€§-æ³¨æ„åŠ›å…³ç³» ===")
print(f"Pearsonç›¸å…³ç³»æ•°: r = {r_corr:.3f}")
print(f"på€¼(æ˜¾è‘—æ€§): p = {p_corr:.2e}")

if r_corr > 0 and p_corr < 0.05:
    print("âœ“ æ˜¾è‘—æ­£ç›¸å…³: æ¸©åº¦æ¨¡å¼ç›¸ä¼¼çš„ç«™ç‚¹æ³¨æ„åŠ›é«˜(ç¬¦åˆæ°”è±¡è§„å¾‹)")
else:
    print("âš  ç›¸å…³æ€§ä¸æ˜¾è‘—æˆ–ä¸ºè´Ÿ,éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")

# 3.4 å¯è§†åŒ–æ¸©åº¦ç›¸å…³æ€§-æ³¨æ„åŠ›å…³ç³»
from myGNN.explainer.visualize_explainer import plot_correlation_vs_attention
plot_correlation_vs_attention(
    edge_corrs, attention_mean,
    save_path='analysis_correlation_vs_attention.png',
    dpi=300
)

# === æ­¥éª¤4: æ£€æŸ¥æœ€é«˜/æœ€ä½æ³¨æ„åŠ›çš„è¾¹ ===
top_indices = np.argsort(attention_mean.numpy())[-10:][::-1]
bottom_indices = np.argsort(attention_mean.numpy())[:10]

print("\n=== æœ€é«˜æ³¨æ„åŠ›çš„10æ¡è¾¹ ===")
print(f"{'æºç«™ç‚¹':<8} {'ç›®æ ‡ç«™ç‚¹':<8} {'æ³¨æ„åŠ›':<10} {'è·ç¦»(km)':<10} {'æ¸©åº¦ç›¸å…³æ€§':<10}")
print("-" * 60)
for idx in top_indices:
    src, dst = edge_index[:, idx]
    dist = edge_distances[idx]
    corr = edge_corrs[idx]
    attn = attention_mean[idx]
    print(f"{src:<8} {dst:<8} {attn:.4f}     {dist:>6.1f}     {corr:>6.3f}")

print("\n=== æœ€ä½æ³¨æ„åŠ›çš„10æ¡è¾¹ ===")
print(f"{'æºç«™ç‚¹':<8} {'ç›®æ ‡ç«™ç‚¹':<8} {'æ³¨æ„åŠ›':<10} {'è·ç¦»(km)':<10} {'æ¸©åº¦ç›¸å…³æ€§':<10}")
print("-" * 60)
for idx in bottom_indices:
    src, dst = edge_index[:, idx]
    dist = edge_distances[idx]
    corr = edge_corrs[idx]
    attn = attention_mean[idx]
    print(f"{src:<8} {dst:<8} {attn:.4f}     {dist:>6.1f}     {corr:>6.3f}")

# === æ­¥éª¤5: å…¨å±€æ³¨æ„åŠ›çŸ©é˜µå¯è§†åŒ– ===
attention_matrix = edge_attention_to_matrix(
    edge_index, attention_mean,
    num_nodes=28, aggregation='mean'
)
print(f"\nå…¨å±€æ³¨æ„åŠ›çŸ©é˜µå½¢çŠ¶: {attention_matrix.shape}")  # (28, 28)

from myGNN.explainer.visualize_explainer import plot_attention_matrix_heatmap
plot_attention_matrix_heatmap(
    attention_matrix,
    save_path='analysis_attention_matrix_heatmap.png',
    dpi=300
)

# === æ­¥éª¤6: æ¡ˆä¾‹ç ”ç©¶ - ç‰¹å®šç«™ç‚¹å¯¹åˆ†æ ===
def analyze_station_pair(src_id, dst_id, edge_index, attention_mean,
                         edge_distances, edge_corrs, station_coords):
    """åˆ†æç‰¹å®šç«™ç‚¹å¯¹çš„æ³¨æ„åŠ›æƒé‡"""
    # æŸ¥æ‰¾è¾¹ç´¢å¼•
    edge_mask = (edge_index[0] == src_id) & (edge_index[1] == dst_id)
    if not edge_mask.any():
        print(f"æœªæ‰¾åˆ°ç«™ç‚¹{src_id}â†’{dst_id}çš„è¾¹")
        return

    idx = torch.where(edge_mask)[0][0].item()

    # è·å–ä¿¡æ¯
    attn = attention_mean[idx].item()
    dist = edge_distances[idx]
    corr = edge_corrs[idx]

    # è®¡ç®—ç›¸å¯¹æ’å
    rank = (attention_mean >= attn).sum().item()
    total = len(attention_mean)

    print(f"\n=== ç«™ç‚¹å¯¹åˆ†æ: {src_id} â†’ {dst_id} ===")
    print(f"æ³¨æ„åŠ›æƒé‡: {attn:.4f}")
    print(f"é‡è¦æ€§æ’å: {rank}/{total} (å‰{rank/total*100:.1f}%)")
    print(f"åœ°ç†è·ç¦»: {dist:.1f} km")
    print(f"æ¸©åº¦ç›¸å…³æ€§: {corr:.3f}")

    # è®¡ç®—å®é™…è·ç¦»
    src_coord = station_coords[src_id]
    dst_coord = station_coords[dst_id]
    actual_dist = haversine_distance(
        src_coord[1], src_coord[0],  # lat, lon
        dst_coord[1], dst_coord[0]
    )
    print(f"å®é™…åœ°ç†è·ç¦»: {actual_dist:.1f} km (Haversineå…¬å¼)")

# ç¤ºä¾‹: åˆ†æç«™ç‚¹0â†’5çš„è¿æ¥
analyze_station_pair(0, 5, edge_index, attention_mean,
                    edge_distances, edge_corrs, station_coords)
```

### 2. ä»…æå–æ³¨æ„åŠ›æƒé‡(ä¸è¿è¡Œå®Œæ•´åˆ†æ)

```python
from myGNN.explainer import SpatialExplainer, ExplainerConfig

# é…ç½®ä»…æå–æ³¨æ„åŠ›
exp_config = ExplainerConfig(num_samples=100, extract_attention=True)
spatial_explainer = SpatialExplainer(model, config, exp_config)

# æ‰¹é‡æå–
attention_result = spatial_explainer.extract_attention_weights_batch(
    data_loader=test_loader,
    num_samples=100
)

if attention_result is not None:
    # ä¿å­˜ç»“æœ
    np.savez('attention_weights.npz',
             attention_mean=attention_result['attention_mean'].numpy(),
             attention_std=attention_result['attention_std'].numpy(),
             edge_index=attention_result['edge_index'].numpy())
    print("âœ“ æ³¨æ„åŠ›æƒé‡å·²ä¿å­˜åˆ° attention_weights.npz")
else:
    print("âš  æ¨¡å‹ä¸æ”¯æŒæ³¨æ„åŠ›æå–(å¯èƒ½ä¸æ˜¯GATæ¨¡å‹)")
```

### 3. è‡ªå®šä¹‰å¯è§†åŒ–

ä½¿ç”¨å¯è§†åŒ–å‡½æ•°ç”Ÿæˆè‡ªå®šä¹‰å›¾è¡¨:

```python
from myGNN.explainer.visualize_explainer import (
    plot_spatial_edges_with_all,
    plot_gat_attention_comparison,
    plot_attention_matrix_heatmap,
    plot_distance_vs_attention,
    plot_correlation_vs_attention
)
import numpy as np

# åŠ è½½æ•°æ®
data = np.load('explanations/explanation_data.npz')
edge_index = data['edge_index']
edge_importance = data['edge_importance_mean']
attention_mean = data['attention_mean']

# 1. è‡ªå®šä¹‰å…¨è¾¹å¯è§†åŒ–(ä»…æ˜¾ç¤ºTop-30)
plot_spatial_edges_with_all(
    edge_importance=edge_importance,
    edge_index=edge_index,
    station_coords=station_coords,
    save_path='custom_top30_all_edges.png',
    top_k=30,
    show_mode='overlay',
    dpi=300
)

# 2. è‡ªå®šä¹‰GNNExplainer vs GATå¯¹æ¯”å›¾
plot_gat_attention_comparison(
    explainer_importance=edge_importance,
    attention_weights=attention_mean,
    edge_index=edge_index,
    station_coords=station_coords,
    save_path='custom_comparison.png',
    top_k=15,
    dpi=300
)

# 3. è‡ªå®šä¹‰æ³¨æ„åŠ›çŸ©é˜µçƒ­åŠ›å›¾(ä½¿ç”¨maxèšåˆ)
from myGNN.explainer.utils import edge_attention_to_matrix
attention_matrix_max = edge_attention_to_matrix(
    edge_index, attention_mean,
    num_nodes=28, aggregation='max'
)
plot_attention_matrix_heatmap(
    attention_matrix_max,
    save_path='custom_attention_matrix_max.png',
    title='GAT Attention Matrix (Max Aggregation)',
    dpi=300
)

# 4. è‡ªå®šä¹‰è·ç¦»-æ³¨æ„åŠ›åˆ†æ(é«˜åˆ†è¾¨ç‡)
edge_distances = compute_edge_distances(edge_index, station_coords)
plot_distance_vs_attention(
    edge_distances, attention_mean,
    save_path='custom_high_res_distance.png',
    dpi=600,  # é«˜åˆ†è¾¨ç‡
    figsize=(12, 8)
)

# 5. è‡ªå®šä¹‰æ¸©åº¦ç›¸å…³æ€§-æ³¨æ„åŠ›åˆ†æ
weather_data = np.load('data/real_weather_data_2010_2017.npy')
corr_matrix = compute_temperature_correlation(
    weather_data,
    train_indices=(0, 2191),
    target_feature_idx=4  # tmax
)
edge_corrs = extract_edge_correlations(edge_index, corr_matrix)
plot_correlation_vs_attention(
    edge_corrs, attention_mean,
    save_path='custom_correlation.png',
    dpi=300
)
```

### 4. å­£èŠ‚å·®å¼‚åˆ†æ

å¯¹æ¯”ä¸åŒå­£èŠ‚çš„æ¨¡å‹è¡Œä¸º:

```python
from myGNN.explainer import HybridExplainer, ExplainerConfig
import numpy as np
import matplotlib.pyplot as plt

seasons = ['spring', 'summer', 'autumn', 'winter']
results = {}

# åˆ†åˆ«åˆ†æå››ä¸ªå­£èŠ‚
for season in seasons:
    exp_config = ExplainerConfig(
        num_samples=100,
        season=season,
        extract_attention=True
    )
    explainer = HybridExplainer(model, config, exp_config)
    results[season] = explainer.explain_full(
        test_loader,
        save_path=f'explanations/{season}/'
    )
    print(f"âœ“ å®Œæˆ {season} åˆ†æ")

# å¯¹æ¯”ç‰¹å¾é‡è¦æ€§
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
feature_names = ['x', 'y', 'height', 'tmin', 'tmax', 'tave', 'pre', 'prs', 'rh', 'win']

for idx, season in enumerate(seasons):
    ax = axes[idx // 2, idx % 2]
    feat_imp = results[season]['temporal']['feature_importance'][:10]  # å‰10ä¸ªç‰¹å¾
    ax.barh(range(10), feat_imp.numpy())
    ax.set_yticks(range(10))
    ax.set_yticklabels(feature_names)
    ax.set_xlabel('Importance')
    ax.set_title(f'{season.capitalize()} - Feature Importance')
    ax.invert_yaxis()

plt.tight_layout()
plt.savefig('seasonal_comparison_features.png', dpi=300, bbox_inches='tight')
print("âœ“ å­£èŠ‚å¯¹æ¯”å›¾å·²ä¿å­˜")

# å¯¹æ¯”è¾¹é‡è¦æ€§å·®å¼‚
print("\n=== å­£èŠ‚è¾¹é‡è¦æ€§å·®å¼‚åˆ†æ ===")
for i, season1 in enumerate(seasons):
    for season2 in seasons[i+1:]:
        data1 = np.load(f'explanations/{season1}/explanation_data.npz')
        data2 = np.load(f'explanations/{season2}/explanation_data.npz')

        diff = data1['edge_importance_mean'] - data2['edge_importance_mean']
        max_diff_idx = np.argmax(np.abs(diff))

        print(f"{season1} vs {season2}:")
        print(f"  æœ€å¤§å·®å¼‚è¾¹ç´¢å¼•: {max_diff_idx}")
        print(f"  å·®å¼‚å€¼: {diff[max_diff_idx]:.4f}")
```

### 5. å¤šæ¨¡å‹å¯¹æ¯”åˆ†æ

å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è§£é‡Š:

```python
from myGNN.explainer import HybridExplainer, ExplainerConfig
import torch

models = {
    'GAT_LSTM': 'checkpoints/GAT_LSTM_best/best_model.pth',
    'GSAGE_LSTM': 'checkpoints/GSAGE_LSTM_best/best_model.pth',
    'GAT_SeparateEncoder': 'checkpoints/GAT_SeparateEncoder_best/best_model.pth'
}

exp_config = ExplainerConfig(num_samples=100, extract_attention=True)
explanations = {}

for model_name, model_path in models.items():
    print(f"\nåˆ†ææ¨¡å‹: {model_name}")
    model = torch.load(model_path)
    model.eval()

    explainer = HybridExplainer(model, config, exp_config)
    explanations[model_name] = explainer.explain_full(
        test_loader,
        save_path=f'results/{model_name}/explanations/'
    )

# å¯¹æ¯”åˆ†æ
print("\n=== æ¨¡å‹å¯¹æ¯”åˆ†æ ===")
print(f"\n{'æ¨¡å‹':<25} {'Top-1ç‰¹å¾':<12} {'Top-1è¾¹æƒé‡':<12} {'æ³¨æ„åŠ›å‡å€¼':<12}")
print("-" * 70)

for model_name, explanation in explanations.items():
    # æœ€é‡è¦ç‰¹å¾
    top_feat_idx = torch.argmax(explanation['temporal']['feature_importance']).item()

    # æœ€é‡è¦è¾¹çš„æƒé‡
    top_edge_imp = explanation['spatial']['edge_importance_mean'].max().item()

    # æ³¨æ„åŠ›å‡å€¼(å¦‚æœæœ‰)
    if 'attention' in explanation['spatial']:
        attn_mean = explanation['spatial']['attention']['mean'].mean().item()
        attn_str = f"{attn_mean:.4f}"
    else:
        attn_str = "N/A"

    print(f"{model_name:<25} {top_feat_idx:<12} {top_edge_imp:<12.4f} {attn_str:<12}")
```

---

## ğŸ”§ é…ç½®å‚æ•°è¯¦è§£

### ExplainerConfigå‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|--------|------|------|
| `num_samples` | int | 100 | v1.0 | åˆ†ææ ·æœ¬æ•°é‡ |
| `epochs` | int | 200 | v1.0 | GNNExplainerè®­ç»ƒè½®æ•° |
| `season` | str\|None | None | v1.0 | å­£èŠ‚ç­›é€‰ ('spring', 'summer', 'autumn', 'winter', None) |
| `ig_steps` | int | 50 | v1.0 | Integrated Gradientsç§¯åˆ†æ­¥æ•° |
| `lr` | float | 0.01 | v1.0 | GNNExplainerå­¦ä¹ ç‡ |
| `top_k_edges` | int | 20 | v1.0 | ä¿å­˜çš„Top-Ké‡è¦è¾¹æ•°é‡ |
| `extract_attention` | bool | True | v2.0 | æ˜¯å¦æå–GATæ³¨æ„åŠ›æƒé‡ |
| `all_edges_mode` | str\|None | 'both' | v2.0 | å…¨è¾¹å¯è§†åŒ–æ¨¡å¼ |
| `use_basemap` | bool | True | v2.0 | æ˜¯å¦ä½¿ç”¨Mapboxåœ°å›¾åº•å›¾ |
| `viz_dpi` | int | 300 | v2.0 | å›¾è¡¨åˆ†è¾¨ç‡ |

### all_edges_modeé€‰é¡¹

- `'overlay'`: åªç”Ÿæˆå åŠ æ¨¡å¼å›¾(ç°è‰²å…¨è¾¹ + çº¢è‰²Top-K)
- `'separate'`: åªç”Ÿæˆåˆ†ç¦»æ¨¡å¼å›¾(å·¦å³å¯¹æ¯”å­å›¾)
- `'both'`: ç”Ÿæˆä¸¤ç§æ¨¡å¼å›¾(æ¨è)
- `None`: ä¸ç”Ÿæˆå…¨è¾¹å¯è§†åŒ–å›¾

---

## ğŸ“ æŠ€æœ¯ç»†èŠ‚

### GATæ³¨æ„åŠ›æƒé‡èšåˆç­–ç•¥

**ä¸‰çº§èšåˆæµç¨‹:**

1. **å¤šå¤´èšåˆ** (Head-level Aggregation):
   ```python
   # GATæ¯å±‚æœ‰Hä¸ªattention head
   attn_weights: [num_edges, num_heads]
   attn_avg = attn_weights.mean(dim=1)  # â†’ [num_edges]
   ```

2. **å¤šå±‚èšåˆ** (Layer-level Aggregation):
   ```python
   # GATæœ‰Lå±‚,æ¯å±‚äº§ç”Ÿä¸€ç»„æ³¨æ„åŠ›æƒé‡
   layer_attns = [layer_1_attn, layer_2_attn, ...]  # æ¯ä¸ª [num_edges]
   layer_stacked = torch.stack(layer_attns)  # [num_layers, num_edges]
   sample_avg = layer_stacked.mean(dim=0)    # [num_edges]
   ```

3. **å¤šæ ·æœ¬èšåˆ** (Sample-level Aggregation):
   ```python
   # å¯¹Nä¸ªæ ·æœ¬çš„æ³¨æ„åŠ›æ±‚ç»Ÿè®¡é‡
   all_samples = torch.stack(sample_attns)  # [num_samples, num_edges]
   attention_mean = all_samples.mean(dim=0)  # [num_edges]
   attention_std = all_samples.std(dim=0)    # [num_edges]
   ```

**è®¾è®¡ç†ç”±:**
- å¤šå¤´èšåˆ: GATå¤šå¤´æœºåˆ¶æ•è·ä¸åŒæ¨¡å¼,å¹³å‡å¯è·å¾—æ•´ä½“ä¾èµ–
- å¤šå±‚èšåˆ: æ·±å±‚æ³¨æ„åŠ›æ›´å…³æ³¨é«˜çº§ç‰¹å¾,æµ…å±‚å…³æ³¨ä½çº§ç‰¹å¾,å¹³å‡å…¼é¡¾
- å¤šæ ·æœ¬èšåˆ: æé«˜ç»Ÿè®¡é²æ£’æ€§,å‡å°‘å•æ ·æœ¬éšæœºæ€§

### è·ç¦»è®¡ç®— - Haversineå…¬å¼ (v2.1.0)

ä½¿ç”¨Haversineå…¬å¼è®¡ç®—çƒé¢è·ç¦»,è€ƒè™‘åœ°çƒæ›²ç‡:

```python
def haversine_distance(lat1, lon1, lat2, lon2, radius=6371.0):
    """
    è®¡ç®—çƒé¢è·ç¦»(å…¬é‡Œ)

    Args:
        lat1, lon1: ç‚¹1çš„çº¬åº¦å’Œç»åº¦(åº¦)
        lat2, lon2: ç‚¹2çš„çº¬åº¦å’Œç»åº¦(åº¦)
        radius: åœ°çƒåŠå¾„(km, é»˜è®¤6371)

    Returns:
        distance: è·ç¦»(km)
    """
    # è½¬æ¢ä¸ºå¼§åº¦
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])

    # Haversineå…¬å¼
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))

    return radius * c
```

**ä¼˜åŠ¿:**
- ç²¾åº¦é«˜: è€ƒè™‘åœ°çƒæ›²ç‡,é€‚åˆä¸­çŸ­è·ç¦»(æ•°ç™¾å…¬é‡Œ)
- è®¡ç®—ç¨³å®š: é¿å…æ•°å€¼æº¢å‡º
- æ ‡å‡†åŒ–: ç¬¦åˆåœ°ç†ä¿¡æ¯ç³»ç»Ÿæ ‡å‡†

### æ¸©åº¦ç›¸å…³æ€§è®¡ç®— (v2.1.0)

åŸºäºè®­ç»ƒé›†æ¸©åº¦æ•°æ®è®¡ç®—ç«™ç‚¹é—´ç›¸å…³æ€§:

```python
def compute_temperature_correlation(weather_data, train_indices, target_feature_idx):
    """
    è®¡ç®—è®­ç»ƒé›†æ¸©åº¦ç›¸å…³æ€§çŸ©é˜µ

    Args:
        weather_data: [time_steps, num_stations, num_features]
        train_indices: (start, end) è®­ç»ƒé›†ç´¢å¼•èŒƒå›´
        target_feature_idx: ç›®æ ‡ç‰¹å¾ç´¢å¼•(å¦‚4=tmax)

    Returns:
        corr_matrix: [num_stations, num_stations] ç›¸å…³ç³»æ•°çŸ©é˜µ
    """
    train_start, train_end = train_indices

    # æå–è®­ç»ƒé›†ç›®æ ‡ç‰¹å¾
    train_data = weather_data[train_start:train_end, :, target_feature_idx]
    # shape: [train_len, num_stations]

    # è®¡ç®—Pearsonç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = np.corrcoef(train_data.T)  # [num_stations, num_stations]

    return corr_matrix
```

**è®¾è®¡è€ƒè™‘:**
- **é¿å…æ•°æ®æ³„éœ²**: ä»…ä½¿ç”¨è®­ç»ƒé›†(2010-2015, ç´¢å¼•0-2190)
- **ç›®æ ‡ç‰¹å¾**: é€šå¸¸ä½¿ç”¨tmax(ç´¢å¼•4),ä¹Ÿå¯é€‰æ‹©tmin/tave
- **æ—¶é—´èŒƒå›´**: 6å¹´æ•°æ®(2191å¤©),ç»Ÿè®¡æ˜¾è‘—æ€§é«˜

### è¾¹ç´¢å¼•ä¸€è‡´æ€§

- GATv2Convå¯èƒ½å†…éƒ¨é‡æ’edge_indexç”¨äºæ¶ˆæ¯ä¼ é€’ä¼˜åŒ–
- æœ¬å®ç°ä½¿ç”¨åŸå§‹edge_indexè¿›è¡Œå‰å‘ä¼ æ’­,ç¡®ä¿è¿”å›çš„æ³¨æ„åŠ›æƒé‡ä¸è¾“å…¥edge_indexå¯¹åº”
- éªŒè¯æ–¹æ³•: æ£€æŸ¥è¿”å›çš„edge_indexä¸è¾“å…¥edge_indexæ˜¯å¦ä¸€è‡´

---

## ğŸ” å¸¸è§é—®é¢˜ (FAQ)

### Q1: å¦‚ä½•å…³é—­æ³¨æ„åŠ›æƒé‡æå–?

```python
exp_config = ExplainerConfig(extract_attention=False)
```

### Q2: å¦‚ä½•åªç”Ÿæˆå åŠ æ¨¡å¼çš„å…¨è¾¹å›¾?

```python
exp_config = ExplainerConfig(all_edges_mode='overlay')
# æˆ–åœ¨å¯è§†åŒ–æ—¶æŒ‡å®š
generate_all_visualizations(..., all_edges_mode='overlay')
```

### Q3: ä¸ºä»€ä¹ˆGNNExplainerå’ŒGATæ³¨æ„åŠ›æƒé‡ä¸åŒ?

**ä¸¤è€…å…³æ³¨ç‚¹ä¸åŒ:**

| ç»´åº¦ | GNNExplainer | GATæ³¨æ„åŠ› |
|------|-------------|----------|
| **åˆ†æå¯¹è±¡** | æ•´ä¸ªæ¨¡å‹(LSTM+GAT+MLP) | ä»…GATå±‚ |
| **æ–¹æ³•** | äº‹åæ‰°åŠ¨åˆ†æ | æ¨¡å‹åŸç”Ÿæƒé‡ |
| **è¾“å‡º** | è¾¹å¯¹æœ€ç»ˆé¢„æµ‹çš„æ•´ä½“é‡è¦æ€§ | GATå±‚å†…é‚»åŸŸèšåˆæƒé‡ |
| **ä¼˜åŠ¿** | å…¨é¢,è€ƒè™‘æ‰€æœ‰ç»„ä»¶ | ç›´è§‚,åæ˜ ç©ºé—´ä¾èµ– |
| **ç”¨é€”** | ç†è§£æ•´ä½“å†³ç­–æœºåˆ¶ | éªŒè¯ç©ºé—´å»ºæ¨¡æ­£ç¡®æ€§ |

**äº’è¡¥å…³ç³»:**
- GNNExplaineræ›´å…¨é¢,é€‚åˆè§£é‡Š"ä¸ºä»€ä¹ˆæ¨¡å‹è¿™æ ·é¢„æµ‹"
- GATæ³¨æ„åŠ›æ›´ç›´è§‚,é€‚åˆéªŒè¯"æ¨¡å‹å­¦åˆ°äº†ä»€ä¹ˆç©ºé—´è§„å¾‹"
- ä¸¤è€…å¯¹æ¯”å¯ä»¥å‘ç°æ¨¡å‹çš„ä¼˜ç¼ºç‚¹

### Q4: å¦‚ä½•ç†è§£è·ç¦»-æ³¨æ„åŠ›çš„è´Ÿç›¸å…³?

**è´Ÿç›¸å…³(r<0)æ˜¯ç¬¦åˆé¢„æœŸçš„:**

```
è·ç¦» â†‘ â†’ æ³¨æ„åŠ› â†“  (è´Ÿç›¸å…³)
```

**ç‰©ç†è§£é‡Š:**
- æ°”è±¡ç«™ä¹‹é—´çš„ç©ºé—´å½±å“éšè·ç¦»è¡°å‡
- è·ç¦»è¿‘çš„ç«™ç‚¹æ¸©åº¦æ¨¡å¼æ›´ç›¸ä¼¼
- GATæ¨¡å‹æ­£ç¡®å­¦ä¹ äº†è¿™ç§ç©ºé—´ä¾èµ–

**ç»Ÿè®¡æ£€éªŒ:**
- |r| > 0.3: ä¸­ç­‰ç›¸å…³
- |r| > 0.5: å¼ºç›¸å…³
- p < 0.05: ç»Ÿè®¡æ˜¾è‘—

**ç¤ºä¾‹:**
```
r = -0.45, p = 1.2e-15  â†’ å¼ºè´Ÿç›¸å…³,é«˜åº¦æ˜¾è‘—
```

### Q5: å¦‚ä½•ç†è§£æ¸©åº¦ç›¸å…³æ€§-æ³¨æ„åŠ›çš„å…³ç³»?

**æ­£ç›¸å…³(r>0)è¡¨ç¤ºæ¨¡å‹å­¦åˆ°äº†æ°”è±¡æ¨¡å¼:**

```
æ¸©åº¦ç›¸å…³æ€§ â†‘ â†’ æ³¨æ„åŠ› â†‘  (æ­£ç›¸å…³)
```

**æ°”è±¡è§£é‡Š:**
- æ¸©åº¦æ¨¡å¼ç›¸ä¼¼çš„ç«™ç‚¹é€šå¸¸å—ç›¸åŒå¤©æ°”ç³»ç»Ÿå½±å“
- GATåº”è¯¥å¯¹è¿™äº›ç«™ç‚¹åˆ†é…æ›´é«˜æ³¨æ„åŠ›
- éªŒè¯æ¨¡å‹æ˜¯å¦å­¦åˆ°äº†çœŸå®çš„æ°”è±¡è§„å¾‹

**æ³¨æ„:**
- ç›¸å…³æ€§åŸºäºè®­ç»ƒé›†è®¡ç®—,é¿å…æ•°æ®æ³„éœ²
- ä»…ä½¿ç”¨2010-2015å¹´æ•°æ®(ç´¢å¼•0-2190)
- ä¸åŒ…å«éªŒè¯é›†(2016)å’Œæµ‹è¯•é›†(2017)

### Q6: å¦‚ä½•å¤„ç†å¤§å›¾(è¾¹æ•°è¿‡å¤š)?

**ä¼˜åŒ–ç­–ç•¥:**

1. **å…³é—­å…¨è¾¹å¯è§†åŒ–:**
   ```python
   exp_config = ExplainerConfig(all_edges_mode=None)
   ```

2. **è°ƒæ•´Top-Kå‚æ•°:**
   ```python
   exp_config = ExplainerConfig(top_k_edges=10)  # åªä¿å­˜Top-10
   ```

3. **ä½¿ç”¨å åŠ æ¨¡å¼:**
   ```python
   exp_config = ExplainerConfig(all_edges_mode='overlay')  # é¿å…ç”Ÿæˆå¤šå¼ å¤§å›¾
   ```

4. **é™ä½åˆ†è¾¨ç‡:**
   ```python
   exp_config = ExplainerConfig(viz_dpi=150)  # é™ä½DPI
   ```

### Q7: æ—§çš„explanation_data.npzæ–‡ä»¶å…¼å®¹å—?

**å®Œå…¨å…¼å®¹!** æ–°ä»£ç å‘åå…¼å®¹:

- v1.0.0æ–‡ä»¶: ç¼ºå°‘`attention_mean`å­—æ®µ,è‡ªåŠ¨è·³è¿‡æ³¨æ„åŠ›ç›¸å…³å¯è§†åŒ–
- v2.0.0+æ–‡ä»¶: åŒ…å«`attention_mean`å­—æ®µ,ç”Ÿæˆæ‰€æœ‰11ç§å¯è§†åŒ–
- æ‰€æœ‰åŸæœ‰åŠŸèƒ½ä¿æŒä¸å˜

### Q8: GSAGEæ¨¡å‹æ”¯æŒæ³¨æ„åŠ›æå–å—?

**ä¸æ”¯æŒã€‚** SAGEConvä½¿ç”¨å›ºå®šèšåˆ(mean/max/add),æ²¡æœ‰å¯å­¦ä¹ çš„æ³¨æ„åŠ›æƒé‡ã€‚

**å¤„ç†æ–¹å¼:**
- è®¾ç½®`extract_attention=False`æˆ–å¿½ç•¥æ­¤å‚æ•°
- `extract_attention_weights_batch()`ä¼šè¿”å›Noneå¹¶æç¤º
- å¯è§†åŒ–ä¼šè‡ªåŠ¨è·³è¿‡æ³¨æ„åŠ›ç›¸å…³å›¾è¡¨(å›¾9-11)
- å…¶ä»–åŠŸèƒ½æ­£å¸¸ä½¿ç”¨(å›¾1-8)

### Q9: å¦‚ä½•è§£è¯»æ³¨æ„åŠ›çŸ©é˜µçƒ­åŠ›å›¾?

**ç†è§£çƒ­åŠ›å›¾:**

```
attention_matrix[i, j] = ç«™ç‚¹iå¯¹ç«™ç‚¹jçš„å¹³å‡æ³¨æ„åŠ›
```

**è§‚å¯Ÿè¦ç‚¹:**
1. **å¯¹è§’çº¿**: é€šå¸¸è¾ƒæš—(è‡ªè¿æ¥æ³¨æ„åŠ›ä½æˆ–ä¸å­˜åœ¨)
2. **äº®ç‚¹**: é«˜æ³¨æ„åŠ›è¿æ¥,è¡¨ç¤ºå¼ºç©ºé—´ä¾èµ–
3. **æ¨¡å¼**:
   - å—çŠ¶: åŒºåŸŸå†…ç«™ç‚¹ç›¸äº’å…³æ³¨
   - æ¡å¸¦çŠ¶: æŸäº›ç«™ç‚¹ä½œä¸º"æ¢çº½"è¢«å¹¿æ³›å…³æ³¨
   - ç¨€ç–: é€‰æ‹©æ€§å…³æ³¨,ç¬¦åˆKè¿‘é‚»å›¾ç»“æ„

**éªŒè¯:**
- å¯¹æ¯”è·ç¦»çŸ©é˜µ,æ£€æŸ¥è¿‘è·ç¦»ç«™ç‚¹æ˜¯å¦æ³¨æ„åŠ›é«˜
- å¯¹æ¯”æ¸©åº¦ç›¸å…³æ€§çŸ©é˜µ,æ£€æŸ¥ç›¸ä¼¼ç«™ç‚¹æ˜¯å¦æ³¨æ„åŠ›é«˜

### Q10: å¦‚ä½•é€‰æ‹©æœ€ä½³çš„num_samples?

**æƒè¡¡è€ƒè™‘:**

| num_samples | åˆ†ææ—¶é—´ | ç»Ÿè®¡ç¨³å®šæ€§ | æ¨èåœºæ™¯ |
|------------|---------|-----------|---------|
| 50 | å¿« | ä½ | å¿«é€Ÿæ¢ç´¢ |
| 100 | ä¸­ç­‰ | ä¸­ç­‰ | æ ‡å‡†åˆ†æ(æ¨è) |
| 200+ | æ…¢ | é«˜ | è®ºæ–‡å‘è¡¨ |

**å»ºè®®:**
- **å¼€å‘é˜¶æ®µ**: 50ä¸ªæ ·æœ¬,å¿«é€Ÿè¿­ä»£
- **æ­£å¼åˆ†æ**: 100ä¸ªæ ·æœ¬,å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦
- **å­¦æœ¯å‘è¡¨**: 200+ä¸ªæ ·æœ¬,æœ€å¤§åŒ–ç»Ÿè®¡å¯ä¿¡åº¦

---

## ğŸ“‹ ç¤ºä¾‹è„šæœ¬

å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹è¯·å‚è€ƒ:
- [README.md](README.md) - æ¨¡å—æ¦‚è§ˆ
- [../../CLAUDE.md](../../CLAUDE.md) - é¡¹ç›®æ¶æ„è¯¦ç»†è¯´æ˜
- [../../myGNN/README.md](../README.md) - myGNNæ¡†æ¶æ–‡æ¡£

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

**æ–¹æ³•è®º:**
- **GNNExplainer**: Ying et al. "GNNExplainer: Generating Explanations for Graph Neural Networks." NeurIPS 2019.
- **Integrated Gradients**: Sundararajan et al. "Axiomatic Attribution for Deep Networks." ICML 2017.

**åœ°ç†è®¡ç®—:**
- **Haversineå…¬å¼**: R.W. Sinnott. "Virtues of the Haversine." Sky and Telescope 68(2):159, 1984.

---

## ğŸ™ åé¦ˆä¸è´¡çŒ®

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®,è¯·è”ç³»é¡¹ç›®ç»´æŠ¤è€…æˆ–æäº¤Issueã€‚

---

<div align="center">

**ç‰ˆæœ¬**: v2.1.0
**æœ€åæ›´æ–°**: 2025-12-16
**ç»´æŠ¤è€…**: GNNæ°”æ¸©é¢„æµ‹é¡¹ç›®ç»„

</div>
