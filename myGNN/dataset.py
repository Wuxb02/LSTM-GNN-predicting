"""
myGNNæ•°æ®åŠ è½½æ¨¡å—

åŠŸèƒ½ï¼š
1. æ»‘åŠ¨çª—å£åˆ‡åˆ†
2. 4ç»´æ—¶é—´å‘¨æœŸç¼–ç ï¼ˆå¹´å‘¨æœŸsin/cos + æœˆå‘¨æœŸsin/cosï¼‰
3. ç‰¹å¾é€‰æ‹©
4. æ•°æ®æ ‡å‡†åŒ–
5. PyG Dataæ ¼å¼è½¬æ¢
6. é™æ€/åŠ¨æ€ç‰¹å¾åˆ†ç¦»ç¼–ç ï¼ˆæ–°å¢ï¼‰

ä½œè€…: GNNæ°”æ¸©é¢„æµ‹é¡¹ç›®
æ—¥æœŸ: 2025
"""

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torch_geometric.data import Data, Batch


# å¹´ä»½è¾¹ç•Œå®šä¹‰ï¼ˆ2010-2017å¹´æ•°æ®ï¼‰
# æ•°æ®ç´¢å¼•ä»0å¼€å§‹ï¼Œæ¯å¹´çš„[start, end)
YEAR_BOUNDARIES = {
    2010: (0, 365),       # 365å¤©
    2011: (365, 730),     # 365å¤©
    2012: (730, 1096),    # 366å¤©ï¼ˆé—°å¹´ï¼‰
    2013: (1096, 1461),   # 365å¤©
    2014: (1461, 1826),   # 365å¤©
    2015: (1826, 2191),   # 365å¤©
    2016: (2191, 2557),   # 366å¤©ï¼ˆé—°å¹´ï¼‰
    2017: (2557, 2922),   # 365å¤©
}


def _get_year_boundaries():
    """
    è·å–å¹´ä»½è¾¹ç•Œå­—å…¸

    Returns:
        dict: {year: (start_idx, end_idx)} æ¯å¹´çš„ç´¢å¼•è¾¹ç•Œ
        ä¾‹å¦‚: {2010: (0, 365), 2011: (365, 730), ...}
    """
    return YEAR_BOUNDARIES.copy()


def _get_year_from_idx(time_idx):
    """
    æ ¹æ®æ—¶é—´ç´¢å¼•è·å–å¯¹åº”çš„å¹´ä»½

    Args:
        time_idx: æ—¶é—´ç´¢å¼•

    Returns:
        int: å¹´ä»½
    """
    for year, (start, end) in YEAR_BOUNDARIES.items():
        if start <= time_idx < end:
            return year
    # è¾¹ç•Œæƒ…å†µå¤„ç†
    if time_idx < 0:
        return 2010
    return 2017


def _get_years_in_range(start_idx, end_idx):
    """
    æ ¹æ®ç´¢å¼•èŒƒå›´åˆ¤æ–­è·¨è¶Šå“ªäº›å¹´ä»½

    Args:
        start_idx: çª—å£èµ·å§‹ç´¢å¼•
        end_idx: çª—å£ç»“æŸç´¢å¼•

    Returns:
        list: å¹´ä»½åˆ—è¡¨ï¼Œå¦‚ [2015] æˆ– [2015, 2016]
    """
    years = set()
    for idx in [start_idx, end_idx - 1]:  # æ£€æŸ¥èµ·å§‹å’Œç»“æŸç‚¹
        years.add(_get_year_from_idx(idx))
    return sorted(list(years))


class WeatherGraphDataset(Dataset):
    """
    æ°”è±¡å›¾æ•°æ®é›†

    æ”¯æŒï¼š
    - æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼ˆå¯é…ç½®hist_len, pred_lenï¼‰
    - æ—¶é—´å‘¨æœŸç¼–ç ï¼ˆdoy, month â†’ 4ç»´sin/cosï¼‰
    - ç‰¹å¾é€‰æ‹©ï¼ˆfeature_indicesï¼‰
    - ä»»æ„é¢„æµ‹ç›®æ ‡ï¼ˆtarget_feature_idxï¼‰
    - é™æ€/åŠ¨æ€ç‰¹å¾åˆ†ç¦»ç¼–ç ï¼ˆuse_feature_separationï¼‰
    """

    def __init__(self, MetData, graph, start_idx, end_idx, config,
                 static_encoded=None, static_encoded_by_year=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            MetData: æ°”è±¡æ•°æ® [time_steps, num_stations, features]
            graph: å›¾ç»“æ„å¯¹è±¡
            start_idx: æ•°æ®é›†èµ·å§‹ç´¢å¼•
            end_idx: æ•°æ®é›†ç»“æŸç´¢å¼•
            config: é…ç½®å¯¹è±¡
            static_encoded: é¢„ç¼–ç çš„é™æ€ç‰¹å¾ [num_nodes, encoded_dim]ï¼ˆå¯é€‰ï¼Œå‘åå…¼å®¹ï¼‰
            static_encoded_by_year: æŒ‰å¹´ä»½çš„é™æ€ç¼–ç å­—å…¸ï¼ˆå¯é€‰ï¼‰
                {year: [num_nodes, encoded_dim], ...}
        """
        self.MetData = MetData
        self.graph = graph
        self.start_idx = start_idx
        self.end_idx = end_idx
        self.config = config

        # ç‰¹å¾åˆ†ç¦»ç›¸å…³
        self.use_feature_separation = getattr(
            config, 'use_feature_separation', False
        )
        self.static_encoded = static_encoded
        self.static_encoded_by_year = static_encoded_by_year

        # è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ•°é‡
        # éœ€è¦hist_lenä¸ªå†å²æ•°æ®å’Œpred_lenä¸ªæœªæ¥æ•°æ®
        self.valid_start = start_idx + config.hist_len
        self.valid_end = end_idx - config.pred_len
        self.num_samples = max(0, self.valid_end - self.valid_start)

    def __len__(self):
        """è¿”å›æ•°æ®é›†æ ·æœ¬æ•°é‡"""
        return self.num_samples

    def _encode_temporal_features(self, time_idx):
        """
        å°†æ—¶é—´ç‰¹å¾è½¬æ¢ä¸º4ç»´sin/cosç¼–ç 

        Args:
            time_idx: å½“å‰æ—¶é—´ç´¢å¼•

        Returns:
            temporal_features: [hist_len, 4] æ•°ç»„
                - åˆ—0: doy_sin (å¹´å‘¨æœŸæ­£å¼¦)
                - åˆ—1: doy_cos (å¹´å‘¨æœŸä½™å¼¦)
                - åˆ—2: month_sin (æœˆå‘¨æœŸæ­£å¼¦)
                - åˆ—3: month_cos (æœˆå‘¨æœŸä½™å¼¦)
        """
        temporal_list = []

        for t in range(self.config.hist_len):
            current_idx = time_idx - self.config.hist_len + t

            # è·å–åŸå§‹æ—¶é—´ç‰¹å¾ï¼ˆæ‰€æœ‰æ°”è±¡ç«™çš„æ—¶é—´ç‰¹å¾ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ªï¼‰
            doy = self.MetData[current_idx, 0, 26]      # å¹´å†…æ—¥åºæ•° (1-366)
            month = self.MetData[current_idx, 0, 27]    # æœˆä»½ (1-12)

            # å¹´å‘¨æœŸç¼–ç 
            days_in_year = 366 if doy > 365 else 365
            year_phase = 2 * np.pi * (doy - 1) / days_in_year
            doy_sin = np.sin(year_phase)
            doy_cos = np.cos(year_phase)

            # æœˆå‘¨æœŸç¼–ç  (12ä¸ªæœˆä¸€ä¸ªå‘¨æœŸ)
            # monthä»1å¼€å§‹ï¼Œéœ€è¦å‡1ä½¿å…¶ä»0å¼€å§‹
            month_phase = 2 * np.pi * (month - 1) / 12
            month_sin = np.sin(month_phase)
            month_cos = np.cos(month_phase)

            # ç»„åˆä¸º4ç»´æ—¶é—´ç‰¹å¾
            temporal_feat = np.array([
                doy_sin, doy_cos,      # å¹´å‘¨æœŸ
                month_sin, month_cos   # æœˆå‘¨æœŸ
            ])

            temporal_list.append(temporal_feat)

        return np.array(temporal_list)  # [hist_len, 4]

    def __getitem__(self, idx):
        """
        è·å–ä¸€ä¸ªæ ·æœ¬

        Args:
            idx: æ ·æœ¬ç´¢å¼•

        Returns:
            data: PyG Dataå¯¹è±¡
            time_idx: æ—¶é—´ç´¢å¼•ï¼ˆç”¨äºè¿½è¸ªï¼‰
        """
        time_idx = self.valid_start + idx

        # æ ¹æ®æ˜¯å¦å¯ç”¨ç‰¹å¾åˆ†ç¦»é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if self.use_feature_separation:
            return self._getitem_separated(time_idx)
        else:
            return self._getitem_original(time_idx)

    def _getitem_original(self, time_idx):
        """
        åŸæ¨¡å¼è·å–æ ·æœ¬ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰

        Args:
            time_idx: æ—¶é—´ç´¢å¼•

        Returns:
            data: PyG Dataå¯¹è±¡
            time_idx: æ—¶é—´ç´¢å¼•å¼ é‡
        """
        # 1. æå–å†å²çª—å£ [hist_len, num_stations, features]
        hist_window = self.MetData[time_idx - self.config.hist_len:time_idx]

        # 2. ç‰¹å¾é€‰æ‹©
        if self.config.feature_indices is not None:
            # ä½¿ç”¨æŒ‡å®šç‰¹å¾
            features = hist_window[:, :, self.config.feature_indices]
        else:
            # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾ï¼Œä½†ç§»é™¤doyå’Œmonthï¼ˆç´¢å¼•26-27ï¼‰
            # ä¿ç•™ç´¢å¼•0-25
            features = hist_window[:, :, :26]

        # features shape: [hist_len, num_stations, base_features]

        # 3. æ·»åŠ æ—¶é—´å‘¨æœŸç¼–ç 
        if self.config.add_temporal_encoding:
            temporal_features = self._encode_temporal_features(time_idx)

            # ä¸ºæ¯ä¸ªæ°”è±¡ç«™å¤åˆ¶æ—¶é—´ç‰¹å¾
            num_stations = features.shape[1]
            temporal_expanded = np.tile(
                temporal_features[:, np.newaxis, :],
                (1, num_stations, 1)
            )

            # æ‹¼æ¥åŸå§‹ç‰¹å¾å’Œæ—¶é—´ç‰¹å¾
            features = np.concatenate([features, temporal_expanded], axis=2)

        # 4. è½¬æ¢ä¸º [num_stations, hist_len, features]
        x = features.transpose(1, 0, 2).copy()

        # 5. æå–æ ‡ç­¾
        y = self.MetData[
            time_idx:time_idx + self.config.pred_len,
            :,
            self.config.target_feature_idx
        ]
        y = y.T.copy()

        # 6. æ„å»ºPyG Dataå¯¹è±¡
        x_tensor = torch.FloatTensor(x)
        y_tensor = torch.FloatTensor(y)

        if self.config.use_edge_attr and hasattr(self.graph, 'edge_attr'):
            data = Data(
                x=x_tensor,
                y=y_tensor,
                edge_index=self.graph.edge_index,
                edge_attr=self.graph.edge_attr
            )
        else:
            data = Data(
                x=x_tensor,
                y=y_tensor,
                edge_index=self.graph.edge_index
            )

        return data, torch.LongTensor([time_idx])

    def _get_static_embedding_for_window(self, time_idx):
        """
        æ ¹æ®å†å²çª—å£çš„æ—¶é—´èŒƒå›´è·å–å¯¹åº”çš„é™æ€ç‰¹å¾åµŒå…¥

        å¦‚æœçª—å£åœ¨å•ä¸€å¹´ä»½å†…ï¼Œè¿”å›è¯¥å¹´çš„é™æ€ç¼–ç ï¼›
        å¦‚æœçª—å£è·¨è¶Šä¸¤å¹´ï¼Œè¿”å›ä¸¤å¹´é™æ€ç¼–ç çš„å¹³å‡å€¼ã€‚

        Args:
            time_idx: å½“å‰æ—¶é—´ç´¢å¼•ï¼ˆçª—å£ç»“æŸä½ç½®ï¼‰

        Returns:
            static_embedding: [num_nodes, encoded_dim] é™æ€ç‰¹å¾åµŒå…¥
        """
        # å¦‚æœä½¿ç”¨æŒ‰å¹´ä»½çš„é™æ€ç¼–ç 
        if self.static_encoded_by_year is not None:
            window_start = time_idx - self.config.hist_len
            window_end = time_idx

            # è·å–çª—å£è·¨è¶Šçš„å¹´ä»½
            years_in_window = _get_years_in_range(window_start, window_end)

            if len(years_in_window) == 1:
                # å•å¹´ï¼šç›´æ¥ä½¿ç”¨è¯¥å¹´çš„é™æ€ç¼–ç 
                year = years_in_window[0]
                return self.static_encoded_by_year[year]
            else:
                # è·¨å¹´ï¼šå–å¤šå¹´å¹³å‡
                embeddings = [
                    self.static_encoded_by_year[y] for y in years_in_window
                ]
                return np.mean(embeddings, axis=0)

        # å‘åå…¼å®¹ï¼šä½¿ç”¨å•ä¸€é™æ€ç¼–ç 
        elif self.static_encoded is not None:
            return self.static_encoded

        # éƒ½æ²¡æœ‰é¢„ç¼–ç ï¼Œä½¿ç”¨åŸå§‹é™æ€ç‰¹å¾
        else:
            hist_window = self.MetData[
                time_idx - self.config.hist_len:time_idx
            ]
            static_indices = self.config.static_feature_indices
            return hist_window[0, :, static_indices]

    def _getitem_separated(self, time_idx):
        """
        ç‰¹å¾åˆ†ç¦»æ¨¡å¼è·å–æ ·æœ¬

        æ•°æ®æµï¼š
        1. æå–åŠ¨æ€ç‰¹å¾çª—å£ [hist_len, nodes, dynamic_dim]
        2. æ·»åŠ æ—¶é—´ç¼–ç  [hist_len, nodes, dynamic_dim + 4]
        3. è·å–é™æ€åµŒå…¥ [nodes, static_encoded_dim]ï¼ˆæ”¯æŒæŒ‰å¹´ä»½è·å–å’Œè·¨å¹´å¹³å‡ï¼‰
        4. å¹¿æ’­é™æ€åµŒå…¥ [hist_len, nodes, static_encoded_dim]
        5. æ‹¼æ¥ [hist_len, nodes, total_dim]
        6. è½¬ç½® [nodes, hist_len, total_dim]

        Args:
            time_idx: æ—¶é—´ç´¢å¼•

        Returns:
            data: PyG Dataå¯¹è±¡
            time_idx: æ—¶é—´ç´¢å¼•å¼ é‡
        """
        # 1. æå–åŠ¨æ€ç‰¹å¾å†å²çª—å£
        hist_window = self.MetData[time_idx - self.config.hist_len:time_idx]
        dynamic_indices = self.config.dynamic_feature_indices
        dynamic_features = hist_window[:, :, dynamic_indices]
        # shape: [hist_len, num_nodes, dynamic_dim]

        # 2. æ·»åŠ æ—¶é—´å‘¨æœŸç¼–ç 
        if self.config.add_temporal_encoding:
            temporal_features = self._encode_temporal_features(time_idx)
            # shape: [hist_len, 4]

            num_nodes = dynamic_features.shape[1]
            temporal_expanded = np.tile(
                temporal_features[:, np.newaxis, :],
                (1, num_nodes, 1)
            )
            # shape: [hist_len, num_nodes, 4]

            dynamic_features = np.concatenate(
                [dynamic_features, temporal_expanded], axis=2
            )
            # shape: [hist_len, num_nodes, dynamic_dim + 4]

        # 3. è·å–é™æ€ç‰¹å¾åµŒå…¥ï¼ˆæ”¯æŒæŒ‰å¹´ä»½å’Œè·¨å¹´å¹³å‡ï¼‰
        static_embedding = self._get_static_embedding_for_window(time_idx)
        # shape: [num_nodes, encoded_dim]

        # 4. å¹¿æ’­é™æ€åµŒå…¥åˆ°æ¯ä¸ªæ—¶é—´æ­¥
        hist_len = dynamic_features.shape[0]
        static_broadcast = np.tile(
            static_embedding[np.newaxis, :, :],  # [1, num_nodes, static_dim]
            (hist_len, 1, 1)                      # [hist_len, num_nodes, static_dim]
        )

        # 5. æ‹¼æ¥é™æ€å’ŒåŠ¨æ€ç‰¹å¾
        # é¡ºåºï¼š[é™æ€ç¼–ç , åŠ¨æ€ç‰¹å¾, æ—¶é—´ç¼–ç ]
        features = np.concatenate([static_broadcast, dynamic_features], axis=2)
        # shape: [hist_len, num_nodes, static_dim + dynamic_dim + temporal_dim]

        # 6. è½¬ç½®ä¸ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼
        x = features.transpose(1, 0, 2).copy()
        # shape: [num_nodes, hist_len, total_dim]

        # 7. æå–æ ‡ç­¾
        y = self.MetData[
            time_idx:time_idx + self.config.pred_len,
            :,
            self.config.target_feature_idx
        ]
        y = y.T.copy()

        # 8. æ„å»ºPyG Dataå¯¹è±¡
        x_tensor = torch.FloatTensor(x)
        y_tensor = torch.FloatTensor(y)

        if self.config.use_edge_attr and hasattr(self.graph, 'edge_attr'):
            data = Data(
                x=x_tensor,
                y=y_tensor,
                edge_index=self.graph.edge_index,
                edge_attr=self.graph.edge_attr
            )
        else:
            data = Data(
                x=x_tensor,
                y=y_tensor,
                edge_index=self.graph.edge_index
            )

        return data, torch.LongTensor([time_idx])


def create_dataloaders(config, graph):
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨

    Args:
        config: é…ç½®å¯¹è±¡
        graph: å›¾ç»“æ„å¯¹è±¡

    Returns:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    # 1. åŠ è½½æ•°æ®
    MetData = np.load(config.MetData_fp)
    print(f"âœ“ åŠ è½½æ•°æ®: {config.MetData_fp}")
    print(f"  åŸå§‹å½¢çŠ¶: {MetData.shape}")

    # 2. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç‰¹å¾åˆ†ç¦»æ¨¡å¼
    use_separation = getattr(config, 'use_feature_separation', False)

    if use_separation:
        return _create_dataloaders_separated(config, graph, MetData)
    else:
        return _create_dataloaders_original(config, graph, MetData)


def _create_dataloaders_original(config, graph, MetData):
    """
    åŸæ¨¡å¼åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    """
    # 2. è®¡ç®—æ ‡å‡†åŒ–ç»Ÿè®¡é‡ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰
    train_data = MetData[config.train_start:config.train_end]

    # 2.1 è®¡ç®—æ‰€æœ‰è¾“å…¥ç‰¹å¾çš„ç»Ÿè®¡é‡ï¼ˆ0-25ç´¢å¼•ï¼Œç§»é™¤æ—¶é—´ç‰¹å¾26-27ï¼‰
    feature_data = train_data[:, :, :26]

    feature_mean = feature_data.mean(axis=(0, 1))
    feature_std = feature_data.std(axis=(0, 1))

    # å®‰å…¨æ£€æŸ¥
    if np.isnan(feature_mean).any() or np.isnan(feature_std).any():
        raise ValueError(
            f"æ ‡å‡†åŒ–å‚æ•°åŒ…å«NaN - mean: {feature_mean}, std: {feature_std}"
        )

    if (feature_std < 1e-6).any():
        problematic_features = np.where(feature_std < 1e-6)[0]
        raise ValueError(
            f"éƒ¨åˆ†ç‰¹å¾æ ‡å‡†å·®è¿‡å°: ç‰¹å¾ç´¢å¼• {problematic_features}\n"
            f"æ ‡å‡†å·®å€¼: {feature_std[problematic_features]}"
        )

    # 2.2 æå–ç›®æ ‡ç‰¹å¾çš„ç»Ÿè®¡é‡
    ta_mean = float(feature_mean[config.target_feature_idx])
    ta_std = float(feature_std[config.target_feature_idx])

    # ğŸ†• è®¡ç®—90åˆ†ä½æ•°ï¼ˆç”¨äºåŠ¨æ€é«˜æ¸©é˜ˆå€¼ï¼‰
    target_feature_data = train_data[:, :, config.target_feature_idx]  # shape: [train_len, 28]
    ta_p90 = float(np.percentile(target_feature_data, 90))  # 90åˆ†ä½æ•°

    print(f"\næ ‡å‡†åŒ–å‚æ•°è®¡ç®—å®Œæˆ:")
    print(f"  ç‰¹å¾å‡å€¼èŒƒå›´: [{feature_mean.min():.4f}, {feature_mean.max():.4f}]")
    print(f"  ç‰¹å¾æ ‡å‡†å·®èŒƒå›´: [{feature_std.min():.4f}, {feature_std.max():.4f}]")
    print(f"  ç›®æ ‡ç‰¹å¾(ç´¢å¼•{config.target_feature_idx}) - "
          f"mean: {ta_mean:.4f}, std: {ta_std:.4f}")
    print(f"  ç›®æ ‡ç‰¹å¾90åˆ†ä½æ•°: {ta_p90:.4f}Â°C (å¯ç”¨äºåŠ¨æ€é«˜æ¸©é˜ˆå€¼)")

    # 3. æ ‡å‡†åŒ–
    MetData[:, :, :26] = (MetData[:, :, :26] - feature_mean) / (feature_std + 1e-8)

    print(f"âœ“ å·²æ ‡å‡†åŒ–æ‰€æœ‰26ä¸ªè¾“å…¥ç‰¹å¾")

    stats = {
        'ta_mean': ta_mean,
        'ta_std': ta_std,
        'ta_p90': ta_p90,          # ğŸ†• æ·»åŠ 90åˆ†ä½æ•°
        'feature_mean': feature_mean,
        'feature_std': feature_std
    }

    # 4. åˆ›å»ºæ•°æ®é›†
    train_dataset = WeatherGraphDataset(
        MetData, graph, config.train_start, config.train_end, config
    )
    val_dataset = WeatherGraphDataset(
        MetData, graph, config.val_start, config.val_end, config
    )
    test_dataset = WeatherGraphDataset(
        MetData, graph, config.test_start, config.test_end, config
    )

    print(f"\næ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")

    # 5. åˆ›å»ºDataLoader
    train_loader, val_loader, test_loader = _create_loaders(
        train_dataset, val_dataset, test_dataset, config
    )

    return train_loader, val_loader, test_loader, stats


def _create_dataloaders_separated(config, graph, MetData):
    """
    ç‰¹å¾åˆ†ç¦»æ¨¡å¼åˆ›å»ºæ•°æ®åŠ è½½å™¨

    æµç¨‹ï¼š
    1. åˆ†ç¦»æå–é™æ€å’ŒåŠ¨æ€ç‰¹å¾
    2. åˆ†åˆ«è®¡ç®—ç»Ÿè®¡é‡å¹¶æ ‡å‡†åŒ–
    3. æŒ‰å¹´ä»½æå–é™æ€ç‰¹å¾å¹¶ä½¿ç”¨MLPç¼–ç 
    4. åˆ›å»ºæ•°æ®é›†ï¼ˆæ”¯æŒè·¨å¹´å¹³å‡ï¼‰
    """
    from myGNN.feature_encoder import StaticFeatureEncoder

    print(f"\nâœ“ å¯ç”¨ç‰¹å¾åˆ†ç¦»æ¨¡å¼ï¼ˆæŒ‰å¹´ä»½æå–é™æ€ç‰¹å¾ï¼‰")
    print(f"  é™æ€ç‰¹å¾ç´¢å¼•: {config.static_feature_indices} "
          f"({len(config.static_feature_indices)}ä¸ª)")
    print(f"  åŠ¨æ€ç‰¹å¾ç´¢å¼•: {config.dynamic_feature_indices} "
          f"({len(config.dynamic_feature_indices)}ä¸ª)")

    static_indices = config.static_feature_indices
    dynamic_indices = config.dynamic_feature_indices

    # 2. åˆ†åˆ«è®¡ç®—é™æ€å’ŒåŠ¨æ€ç‰¹å¾çš„æ ‡å‡†åŒ–å‚æ•°ï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†ï¼‰
    train_data = MetData[config.train_start:config.train_end]

    # 2.1 é™æ€ç‰¹å¾ç»Ÿè®¡é‡
    static_data = train_data[:, :, static_indices]
    static_mean = static_data.mean(axis=(0, 1))
    static_std = static_data.std(axis=(0, 1))

    # å®‰å…¨æ£€æŸ¥é™æ€ç‰¹å¾
    if (static_std < 1e-6).any():
        problematic = np.where(static_std < 1e-6)[0]
        print(f"  âš  éƒ¨åˆ†é™æ€ç‰¹å¾æ ‡å‡†å·®è¿‡å°: ç´¢å¼• {problematic}")
        static_std = np.maximum(static_std, 1e-6)

    # 2.2 åŠ¨æ€ç‰¹å¾ç»Ÿè®¡é‡
    dynamic_data = train_data[:, :, dynamic_indices]
    dynamic_mean = dynamic_data.mean(axis=(0, 1))
    dynamic_std = dynamic_data.std(axis=(0, 1))

    # å®‰å…¨æ£€æŸ¥åŠ¨æ€ç‰¹å¾
    if (dynamic_std < 1e-6).any():
        problematic = np.where(dynamic_std < 1e-6)[0]
        print(f"  âš  éƒ¨åˆ†åŠ¨æ€ç‰¹å¾æ ‡å‡†å·®è¿‡å°: ç´¢å¼• {problematic}")
        dynamic_std = np.maximum(dynamic_std, 1e-6)

    print(f"\né™æ€ç‰¹å¾æ ‡å‡†åŒ–:")
    print(f"  å‡å€¼èŒƒå›´: [{static_mean.min():.4f}, {static_mean.max():.4f}]")
    print(f"  æ ‡å‡†å·®èŒƒå›´: [{static_std.min():.4f}, {static_std.max():.4f}]")
    print(f"\nåŠ¨æ€ç‰¹å¾æ ‡å‡†åŒ–:")
    print(f"  å‡å€¼èŒƒå›´: [{dynamic_mean.min():.4f}, {dynamic_mean.max():.4f}]")
    print(f"  æ ‡å‡†å·®èŒƒå›´: [{dynamic_std.min():.4f}, {dynamic_std.max():.4f}]")

    # ğŸ†• åœ¨æ ‡å‡†åŒ–ä¹‹å‰è®¡ç®—ç›®æ ‡ç‰¹å¾çš„90åˆ†ä½æ•°ï¼ˆç”¨äºåŠ¨æ€é«˜æ¸©é˜ˆå€¼ï¼‰
    target_idx = config.target_feature_idx
    target_feature_data = train_data[:, :, target_idx]  # ä½¿ç”¨åŸå§‹æœªæ ‡å‡†åŒ–çš„æ•°æ®
    ta_p90 = float(np.percentile(target_feature_data, 90))
    print(f"\nç›®æ ‡ç‰¹å¾(ç´¢å¼•{target_idx})90åˆ†ä½æ•°: {ta_p90:.4f}Â°C (å¯ç”¨äºåŠ¨æ€é«˜æ¸©é˜ˆå€¼)")

    # 3. æ ‡å‡†åŒ–æ•´ä¸ªæ•°æ®é›†
    MetData[:, :, static_indices] = (
        (MetData[:, :, static_indices] - static_mean) / (static_std + 1e-8)
    )
    MetData[:, :, dynamic_indices] = (
        (MetData[:, :, dynamic_indices] - dynamic_mean) / (dynamic_std + 1e-8)
    )

    print(f"âœ“ å·²åˆ†åˆ«æ ‡å‡†åŒ–é™æ€å’ŒåŠ¨æ€ç‰¹å¾")

    # 4. æŒ‰å¹´ä»½æå–å¹¶ç¼–ç é™æ€ç‰¹å¾
    year_boundaries = _get_year_boundaries()
    print(f"\næŒ‰å¹´ä»½æå–é™æ€ç‰¹å¾:")

    # åˆ›å»ºé™æ€ç¼–ç å™¨ï¼ˆæ’ç­‰æ˜ å°„ï¼‰
    static_encoder = StaticFeatureEncoder(
        input_dim=len(static_indices),
        output_dim=config.static_encoded_dim
    )

    # æŒ‰å¹´ä»½æå–å’Œç¼–ç é™æ€ç‰¹å¾
    static_encoded_by_year = {}
    for year, (start, end) in year_boundaries.items():
        # æå–è¯¥å¹´çš„é™æ€ç‰¹å¾å¹³å‡å€¼
        year_static = MetData[start:end, :, static_indices].mean(axis=0)
        # shape: [num_nodes, static_dim]

        # ç¼–ç 
        with torch.no_grad():
            year_encoded = static_encoder(
                torch.FloatTensor(year_static)
            ).numpy()
        # shape: [num_nodes, encoded_dim]

        static_encoded_by_year[year] = year_encoded
        print(f"  {year}å¹´: ç´¢å¼•[{start}, {end}), "
              f"ç¼–ç å½¢çŠ¶: {year_encoded.shape}")

    print(f"\né™æ€ç¼–ç å™¨å‚æ•°é‡: "
          f"{sum(p.numel() for p in static_encoder.parameters()):,}")

    # 5. è®¡ç®—ç›®æ ‡ç‰¹å¾ç»Ÿè®¡é‡ï¼ˆç”¨äºæŸå¤±åæ ‡å‡†åŒ–ï¼‰
    # æ³¨æ„ï¼šta_p90 å·²åœ¨æ ‡å‡†åŒ–ä¹‹å‰è®¡ç®—ï¼ˆè§ä¸Šæ–¹ç¬¬561-565è¡Œï¼‰
    if target_idx in dynamic_indices:
        target_in_dynamic = dynamic_indices.index(target_idx)
        ta_mean = float(dynamic_mean[target_in_dynamic])
        ta_std = float(dynamic_std[target_in_dynamic])
    else:
        # å¦‚æœç›®æ ‡åœ¨é™æ€ç‰¹å¾ä¸­ï¼ˆä¸å¸¸è§ï¼‰
        target_in_static = static_indices.index(target_idx)
        ta_mean = float(static_mean[target_in_static])
        ta_std = float(static_std[target_in_static])

    print(f"\nç›®æ ‡ç‰¹å¾(ç´¢å¼•{target_idx}) - mean: {ta_mean:.4f}, std: {ta_std:.4f}")

    # ç»Ÿè®¡è·¨å¹´æ ·æœ¬æ•°é‡
    cross_year_count = 0
    for year, (start, end) in year_boundaries.items():
        # æ£€æŸ¥è¯¥å¹´åˆæœ‰å¤šå°‘æ ·æœ¬ä¼šè·¨è¶Šåˆ°ä¸Šä¸€å¹´
        cross_start = start + config.hist_len
        if year > 2010:
            # hist_lenä¸ªæ ·æœ¬çš„çª—å£ä¼šè·¨è¶Šå¹´ä»½è¾¹ç•Œ
            cross_count = min(config.hist_len, end - start)
            cross_year_count += cross_count

    print(f"\nè·¨å¹´æ ·æœ¬ç»Ÿè®¡:")
    print(f"  å†å²çª—å£é•¿åº¦: {config.hist_len} å¤©")
    print(f"  é¢„è®¡è·¨å¹´æ ·æœ¬æ•°: ~{cross_year_count} (æ¯å¹´åˆçº¦{config.hist_len}ä¸ª)")

    stats = {
        'ta_mean': ta_mean,
        'ta_std': ta_std,
        'ta_p90': ta_p90,          # ğŸ†• æ·»åŠ 90åˆ†ä½æ•°
        'static_mean': static_mean,
        'static_std': static_std,
        'dynamic_mean': dynamic_mean,
        'dynamic_std': dynamic_std,
        'static_encoder': static_encoder,
        'static_encoded_by_year': static_encoded_by_year,
        # å‘åå…¼å®¹ï¼šä¿ç•™ä¸€ä¸ªå…¨å±€é™æ€ç¼–ç ï¼ˆå–æ‰€æœ‰å¹´ä»½å¹³å‡ï¼‰
        'static_encoded': np.mean(
            list(static_encoded_by_year.values()), axis=0
        )
    }

    # 6. åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨æŒ‰å¹´ä»½çš„é™æ€ç¼–ç ï¼‰
    train_dataset = WeatherGraphDataset(
        MetData, graph, config.train_start, config.train_end, config,
        static_encoded_by_year=static_encoded_by_year
    )
    val_dataset = WeatherGraphDataset(
        MetData, graph, config.val_start, config.val_end, config,
        static_encoded_by_year=static_encoded_by_year
    )
    test_dataset = WeatherGraphDataset(
        MetData, graph, config.test_start, config.test_end, config,
        static_encoded_by_year=static_encoded_by_year
    )

    print(f"\næ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")

    # ç»´åº¦ä¿¡æ¯
    print(f"\nç‰¹å¾ç»´åº¦ä¿¡æ¯:")
    print(f"  é™æ€ç¼–ç ç»´åº¦: {config.static_encoded_dim}")
    print(f"  åŠ¨æ€ç‰¹å¾ç»´åº¦: {len(dynamic_indices)}")
    print(f"  æ—¶é—´ç¼–ç ç»´åº¦: "
          f"{config.temporal_features if config.add_temporal_encoding else 0}")
    print(f"  æ€»è¾“å…¥ç»´åº¦: {config.in_dim}")

    # 7. åˆ›å»ºDataLoader
    train_loader, val_loader, test_loader = _create_loaders(
        train_dataset, val_dataset, test_dataset, config
    )

    return train_loader, val_loader, test_loader, stats


def _create_loaders(train_dataset, val_dataset, test_dataset, config):
    """
    åˆ›å»ºDataLoaderï¼ˆå…±ç”¨å‡½æ•°ï¼‰
    """
    def collate_fn(batch):
        """å°†batchä¸­çš„æ ·æœ¬ç»„åˆä¸ºPyG Batchå¯¹è±¡"""
        data_list = [item[0] for item in batch]
        time_indices = [item[1] for item in batch]
        batched_data = Batch.from_data_list(data_list)
        time_indices = torch.cat(time_indices, dim=0)
        return batched_data, time_indices

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0,
        collate_fn=collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn
    )

    return train_loader, val_loader, test_loader


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½
    import sys
    sys.path.append('..')

    from config import create_config
    from graph import load_graph_from_station_info

    print("=" * 70)
    print("æ•°æ®åŠ è½½æ¨¡å—æµ‹è¯•")
    print("=" * 70)

    # æµ‹è¯•1: ç‰¹å¾åˆ†ç¦»æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    print("\nã€æµ‹è¯•1: ç‰¹å¾åˆ†ç¦»æ¨¡å¼ã€‘")
    print("-" * 50)

    config, arch_config = create_config()
    config.use_feature_separation = True

    # æ„å»ºå›¾
    graph = load_graph_from_station_info(
        config.station_info_fp,
        top_neighbors=config.top_neighbors,
        use_edge_attr=config.use_edge_attr
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader, stats = create_dataloaders(
        config, graph
    )

    # æµ‹è¯•ä¸€ä¸ªbatch
    for batch_data, time_indices in train_loader:
        print(f"\nç‰¹å¾åˆ†ç¦»æ¨¡å¼ - æµ‹è¯•batch:")
        print(f"  batch_data.x shape: {batch_data.x.shape}")
        print(f"  batch_data.y shape: {batch_data.y.shape}")
        print(f"  æœŸæœ›xå½¢çŠ¶: [batch*nodes, hist_len, {config.in_dim}]")

        # éªŒè¯ç»´åº¦
        expected_dim = config.in_dim
        actual_dim = batch_data.x.shape[-1]
        assert actual_dim == expected_dim, \
            f"ç»´åº¦ä¸åŒ¹é…: å®é™…{actual_dim} vs æœŸæœ›{expected_dim}"
        print(f"  âœ“ ç»´åº¦éªŒè¯é€šè¿‡: {actual_dim}")
        break

    # æµ‹è¯•2: åŸæ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    print("\n" + "=" * 70)
    print("ã€æµ‹è¯•2: åŸæ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰ã€‘")
    print("-" * 50)

    config2, _ = create_config()
    config2.use_feature_separation = False

    train_loader2, val_loader2, test_loader2, stats2 = create_dataloaders(
        config2, graph
    )

    for batch_data2, time_indices2 in train_loader2:
        print(f"\nåŸæ¨¡å¼ - æµ‹è¯•batch:")
        print(f"  batch_data.x shape: {batch_data2.x.shape}")
        print(f"  batch_data.y shape: {batch_data2.y.shape}")
        print(f"  æœŸæœ›xå½¢çŠ¶: [batch*nodes, hist_len, {config2.in_dim}]")

        expected_dim2 = config2.in_dim
        actual_dim2 = batch_data2.x.shape[-1]
        assert actual_dim2 == expected_dim2, \
            f"ç»´åº¦ä¸åŒ¹é…: å®é™…{actual_dim2} vs æœŸæœ›{expected_dim2}"
        print(f"  âœ“ ç»´åº¦éªŒè¯é€šè¿‡: {actual_dim2}")
        break

    # å¯¹æ¯”ä¸¤ç§æ¨¡å¼
    print("\n" + "=" * 70)
    print("ã€æ¨¡å¼å¯¹æ¯”ã€‘")
    print("-" * 50)
    print(f"ç‰¹å¾åˆ†ç¦»æ¨¡å¼ è¾“å…¥ç»´åº¦: {config.in_dim}")
    print(f"  - é™æ€ç¼–ç : {config.static_encoded_dim}")
    print(f"  - åŠ¨æ€ç‰¹å¾: {len(config.dynamic_feature_indices)}")
    print(f"  - æ—¶é—´ç¼–ç : {config.temporal_features}")
    print(f"\nåŸæ¨¡å¼ è¾“å…¥ç»´åº¦: {config2.in_dim}")
    print(f"  - å…¨éƒ¨ç‰¹å¾: 26")
    print(f"  - æ—¶é—´ç¼–ç : {config2.temporal_features}")

    print("\n" + "=" * 70)
    print("æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 70)

