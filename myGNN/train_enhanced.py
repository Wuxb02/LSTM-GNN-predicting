"""
å¢å¼ºè®­ç»ƒæµç¨‹æ¨¡å—

æä¾›å¯é…ç½®çš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒé€»è¾‘ï¼Œæ”¯æŒï¼š
1. å¤šç§æŸå¤±å‡½æ•°çš„åŠ¨æ€é€‰æ‹©
2. ä¸åŸè®­ç»ƒæµç¨‹å…¼å®¹
3. æ”¯æŒä¼ é€’æ—¶é—´ä¿¡æ¯ï¼ˆdoyï¼‰åˆ°æŸå¤±å‡½æ•°

ä½¿ç”¨æ–¹æ³•:
    from myGNN.train_enhanced import get_loss_function, train_epoch

    # åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = get_loss_function(config)

    # è®­ç»ƒä¸€ä¸ªepoch
    loss = train_epoch(model, dataloader, optimizer, criterion, config, device)
"""
import numpy as np
import torch
import torch.nn as nn
from myGNN.losses import WeightedTrendMSELoss


def get_loss_function(config):
    """
    æ ¹æ®é…ç½®åˆ›å»ºæŸå¤±å‡½æ•°

    å‚æ•°:
        config: é…ç½®å¯¹è±¡ï¼Œéœ€åŒ…å«loss_configå±æ€§

    è¿”å›:
        nn.Module: æŸå¤±å‡½æ•°å®ä¾‹

    æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹:
        - 'MSE': æ ‡å‡†å‡æ–¹è¯¯å·®ï¼ˆé»˜è®¤ï¼‰
        - 'WeightedTrend': åŠ æƒè¶‹åŠ¿æŸå¤±ï¼ˆğŸ”¥è®ºæ–‡æ–¹æ³•ï¼Œæ¨èï¼‰

    ç¤ºä¾‹:
        >>> from myGNN.config import Config
        >>> config = Config()
        >>> config.loss_config.loss_type = 'WeightedTrend'
        >>> criterion = get_loss_function(config)
    """
    # å¦‚æœæ²¡æœ‰loss_configï¼Œä½¿ç”¨é»˜è®¤MSE
    if not hasattr(config, 'loss_config'):
        print("è­¦å‘Š: configä¸­æœªæ‰¾åˆ°loss_configï¼Œä½¿ç”¨é»˜è®¤MSEæŸå¤±")
        return nn.MSELoss()

    loss_cfg = config.loss_config

    # æ ‡å‡†MSEæŸå¤±
    if loss_cfg.loss_type == 'MSE':
        print("ä½¿ç”¨æ ‡å‡†MSEæŸå¤±å‡½æ•°")
        return nn.MSELoss()

    # ğŸ”¥ åŠ æƒè¶‹åŠ¿æŸå¤±ï¼ˆè®ºæ–‡æ–¹æ³• - æ¨èï¼‰
    elif loss_cfg.loss_type == 'WeightedTrend':
        print(f"ä½¿ç”¨è‡ªé€‚åº”åŠ æƒè¶‹åŠ¿MSEæŸå¤±å‡½æ•° (æ¸©åº¦åŠ æƒ + è¶‹åŠ¿çº¦æŸ)")
        print(f"  - å›ºå®šé˜ˆå€¼: {loss_cfg.alert_temp}Â°C")
        print(f"  - æ¼æŠ¥æƒé‡c_under: {loss_cfg.c_under} (ä½ä¼°é«˜æ¸©çš„æƒ©ç½š)")
        print(f"  - è¯¯æŠ¥æƒé‡c_over: {loss_cfg.c_over} (é«˜ä¼°çš„æƒ©ç½š)")
        print(f"  - æ­£ç¡®é¢„æŠ¥é«˜æ¸©æƒé‡: {loss_cfg.c_default_high}")
        print(f"  - è¶‹åŠ¿æƒé‡Î±: {loss_cfg.trend_weight}")

        return WeightedTrendMSELoss(
            alert_temp=loss_cfg.alert_temp,
            c_under=loss_cfg.c_under,
            c_over=loss_cfg.c_over,
            delta=loss_cfg.delta,
            trend_weight=loss_cfg.trend_weight,
            ta_mean=config.ta_mean,
            ta_std=config.ta_std
        )

    else:
        raise ValueError(
            f"æœªçŸ¥çš„æŸå¤±å‡½æ•°ç±»å‹: {loss_cfg.loss_type}\n"
            f"æ”¯æŒçš„ç±»å‹: MSE, WeightedTrend"
        )


def compute_loss(criterion, pred, label, doy=None, config=None):
    """
    è®¡ç®—æŸå¤±ï¼ˆæ ¹æ®æŸå¤±å‡½æ•°ç±»å‹ä¼ é€’ä¸åŒå‚æ•°ï¼‰

    å‚æ•°:
        criterion (nn.Module): æŸå¤±å‡½æ•°
        pred (torch.Tensor): é¢„æµ‹å€¼ï¼ˆæ ‡å‡†åŒ–ï¼‰ï¼Œshape [B, P]
        label (torch.Tensor): çœŸå®å€¼ï¼ˆæ ‡å‡†åŒ–ï¼‰ï¼Œshape [B, P]
        doy (torch.Tensor, optional): å¹´å†…æ—¥åºæ•°ï¼Œshape [B]ï¼ˆæš‚æ—¶ä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼‰
        config: é…ç½®å¯¹è±¡ï¼ˆç”¨äºè·å–ta_meanå’Œta_stdï¼‰

    è¿”å›:
        torch.Tensor: æ ‡é‡æŸå¤±å€¼

    å¼‚å¸¸:
        ValueError: å½“æŸå¤±å‡½æ•°éœ€è¦çš„å‚æ•°æœªæä¾›æ—¶

    ç¤ºä¾‹:
        >>> loss = compute_loss(criterion, pred, label, doy, config)
    """
    # æ ‡å‡†MSE - ä¸éœ€è¦é¢å¤–å‚æ•°
    if isinstance(criterion, nn.MSELoss):
        return criterion(pred, label)

    # åŠ æƒè¶‹åŠ¿æŸå¤± - éœ€è¦ta_meanå’Œta_std
    elif isinstance(criterion, WeightedTrendMSELoss):
        if config is None:
            raise ValueError("WeightedTrendMSELosséœ€è¦configå‚æ•°")
        return criterion(pred, label)

    else:
        # æœªçŸ¥çš„æŸå¤±å‡½æ•°ç±»å‹ï¼Œå°è¯•ç›´æ¥è°ƒç”¨
        return criterion(pred, label)


def train_epoch(model, dataloader, optimizer, scheduler, criterion, config, device):
    """
    è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒå¢å¼ºæŸå¤±å‡½æ•°ï¼‰

    å‚æ•°:
        model (nn.Module): æ¨¡å‹
        dataloader (DataLoader): æ•°æ®åŠ è½½å™¨
        optimizer (torch.optim.Optimizer): ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
        criterion (nn.Module): æŸå¤±å‡½æ•°
        config: é…ç½®å¯¹è±¡
        device (torch.device): è®¾å¤‡

    è¿”å›:
        float: å¹³å‡RMSEæŸå¤±å€¼ï¼ˆÂ°Cï¼‰- å·²åæ ‡å‡†åŒ–

    æ³¨æ„:
        - å¦‚æœdataloaderè¿”å›(graph_data, doy)å…ƒç»„ï¼Œä¼šè‡ªåŠ¨æå–doy
        - å¦‚æœåªè¿”å›graph_dataï¼Œdoyå°†ä¸ºNoneï¼ˆä»…é€‚ç”¨äºä¸éœ€è¦doyçš„æŸå¤±å‡½æ•°ï¼‰
        - schedulerä¼šåœ¨epochç»“æŸæ—¶è‡ªåŠ¨è°ƒç”¨ï¼ˆé’ˆå¯¹éReduceLROnPlateauè°ƒåº¦å™¨ï¼‰
        - åŒ…å«æ¢¯åº¦è£å‰ªï¼ˆmax_norm=1.0ï¼‰å’ŒNaN/Infæ£€æµ‹

    ç¤ºä¾‹:
        >>> avg_loss = train_epoch(
        ...     model, train_loader, optimizer, scheduler, criterion, config, device
        ... )
        >>> print(f"Epoch RMSE: {avg_loss:.4f} Â°C")
    """
    model.train()
    total_loss = 0
    num_batches = 0

    for batch_data in dataloader:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¢å¼ºæ•°æ®é›†ï¼ˆè¿”å›doyï¼‰
        if isinstance(batch_data, tuple) and len(batch_data) == 2:
            graph_data, doy_batch = batch_data
            doy_batch = doy_batch.to(device)
        else:
            graph_data = batch_data
            doy_batch = None

        # æå–å›¾æ•°æ®ï¼ˆDataLoaderè¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å›¾æ•°æ®ï¼‰
        if isinstance(graph_data, list):
            graph_data = graph_data[0]

        # å‰å‘ä¼ æ’­ - æ ¹æ®æ˜¯å¦ä½¿ç”¨è¾¹å±æ€§è°ƒç”¨æ¨¡å‹
        if not config.use_edge_attr:
            # ä¸ä½¿ç”¨è¾¹å±æ€§
            feature = graph_data.x.to(device)
            label = graph_data.y.to(device)
            edge_index = graph_data.edge_index.to(device)
            pred = model(feature, edge_index)
        else:
            # ä½¿ç”¨è¾¹å±æ€§
            feature = graph_data.x.to(device)
            label = graph_data.y.to(device)
            edge_index = graph_data.edge_index.to(device)
            edge_attr = graph_data.edge_attr.to(device)
            pred = model(feature, edge_index, edge_attr)

        # è®¡ç®—æŸå¤±
        loss = compute_loss(criterion, pred, label, doy_batch, config)

        # æ£€æµ‹ NaN æˆ– Inf
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"\nè­¦å‘Š: æ£€æµ‹åˆ°å¼‚å¸¸æŸå¤±å€¼!")
            print(f"  Loss: {loss.item()}")
            print(f"  Pred - min: {pred.min().item():.4f}, max: {pred.max().item():.4f}, mean: {pred.mean().item():.4f}")
            print(f"  Label - min: {label.min().item():.4f}, max: {label.max().item():.4f}, mean: {label.mean().item():.4f}")
            if doy_batch is not None:
                print(f"  DOY - min: {doy_batch.min().item():.1f}, max: {doy_batch.max().item():.1f}")
            raise ValueError("è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç° NaN æˆ– Inf æŸå¤±å€¼ï¼Œè®­ç»ƒç»ˆæ­¢")

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

    # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼‰
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0

    # è½¬æ¢ä¸ºRMSEï¼ˆÂ°Cï¼‰- ä¸æ ‡å‡†è®­ç»ƒæµç¨‹ä¿æŒä¸€è‡´
    avg_loss_rmse = np.sqrt(avg_loss * (config.ta_std ** 2))

    # æ›´æ–°å­¦ä¹ ç‡ï¼ˆéReduceLROnPlateauè°ƒåº¦å™¨ï¼‰
    # ReduceLROnPlateauéœ€è¦éªŒè¯é›†æŸå¤±ï¼Œåœ¨ä¸»è®­ç»ƒå¾ªç¯ä¸­å•ç‹¬å¤„ç†
    if scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
        scheduler.step()

    return avg_loss_rmse


def validate_epoch(model, dataloader, criterion, config, device):
    """
    éªŒè¯ä¸€ä¸ªepochï¼ˆæ”¯æŒå¢å¼ºæŸå¤±å‡½æ•°ï¼‰

    å‚æ•°:
        model (nn.Module): æ¨¡å‹
        dataloader (DataLoader): æ•°æ®åŠ è½½å™¨
        criterion (nn.Module): æŸå¤±å‡½æ•°
        config: é…ç½®å¯¹è±¡
        device (torch.device): è®¾å¤‡

    è¿”å›:
        tuple: (avg_loss_rmse, predict_epoch, label_epoch, time_epoch)
            - avg_loss_rmse (float): å¹³å‡RMSEæŸå¤±å€¼ï¼ˆÂ°Cï¼‰
            - predict_epoch (np.ndarray): é¢„æµ‹ç»“æœ [num_samples, num_stations, pred_len]
            - label_epoch (np.ndarray): çœŸå®æ ‡ç­¾ [num_samples, num_stations, pred_len]
            - time_epoch (np.ndarray): æ—¶é—´ç´¢å¼• [num_samples]

    ç¤ºä¾‹:
        >>> val_loss, pred, label, time = validate_epoch(model, val_loader, criterion, config, device)
        >>> print(f"Validation RMSE: {val_loss:.4f} Â°C")
    """
    model.eval()
    total_loss = 0
    num_batches = 0

    # æ”¶é›†é¢„æµ‹å€¼ã€æ ‡ç­¾å’Œæ—¶é—´ä¿¡æ¯
    predict_list = []
    label_list = []
    time_list = []

    with torch.no_grad():
        for batch_data in dataloader:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¢å¼ºæ•°æ®é›†
            if isinstance(batch_data, tuple) and len(batch_data) == 2:
                graph_data, doy_batch = batch_data
                doy_batch = doy_batch.to(device)
            else:
                graph_data = batch_data
                doy_batch = None

            # æå–å›¾æ•°æ®ï¼ˆDataLoaderè¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å›¾æ•°æ®ï¼‰
            if isinstance(graph_data, list):
                graph_data = graph_data[0]

            # å‰å‘ä¼ æ’­ - æ ¹æ®æ˜¯å¦ä½¿ç”¨è¾¹å±æ€§è°ƒç”¨æ¨¡å‹
            if not config.use_edge_attr:
                # ä¸ä½¿ç”¨è¾¹å±æ€§
                feature = graph_data.x.to(device)
                label = graph_data.y.to(device)
                edge_index = graph_data.edge_index.to(device)
                pred = model(feature, edge_index)
            else:
                # ä½¿ç”¨è¾¹å±æ€§
                feature = graph_data.x.to(device)
                label = graph_data.y.to(device)
                edge_index = graph_data.edge_index.to(device)
                edge_attr = graph_data.edge_attr.to(device)
                pred = model(feature, edge_index, edge_attr)

            # è®¡ç®—æŸå¤±
            loss = compute_loss(criterion, pred, label, doy_batch, config)

            total_loss += loss.item()
            num_batches += 1

            # åæ ‡å‡†åŒ–é¢„æµ‹å€¼å’Œæ ‡ç­¾
            pred_denorm = pred.cpu().numpy() * config.ta_std + config.ta_mean
            label_denorm = label.cpu().numpy() * config.ta_std + config.ta_mean

            # æ”¶é›†æ•°æ®
            predict_list.append(pred_denorm)
            label_list.append(label_denorm)

            # æå–æ—¶é—´ç´¢å¼•
            if hasattr(graph_data, 'time_idx'):
                time_list.append(graph_data.time_idx.cpu().numpy())

    # è®¡ç®—å¹³å‡æŸå¤±ï¼ˆå½’ä¸€åŒ–ç©ºé—´ï¼‰
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0

    # è½¬æ¢ä¸ºRMSEï¼ˆÂ°Cï¼‰
    avg_loss_rmse = np.sqrt(avg_loss * (config.ta_std ** 2))

    # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
    predict_epoch = np.concatenate(predict_list, axis=0)
    label_epoch = np.concatenate(label_list, axis=0)
    time_epoch = np.concatenate(time_list, axis=0) if time_list else np.array([])

    # é‡å¡‘ä¸ºæ ‡å‡†æ ¼å¼ [num_samples, num_stations, pred_len]
    # å½“å‰æ ¼å¼: [num_nodes_total, pred_len]
    # éœ€è¦è½¬æ¢ä¸º: [num_samples, num_stations, pred_len]
    num_total_nodes = predict_epoch.shape[0]
    num_stations = config.node_num
    num_samples = num_total_nodes // num_stations

    predict_epoch = predict_epoch.reshape(num_samples, num_stations, config.pred_len)
    label_epoch = label_epoch.reshape(num_samples, num_stations, config.pred_len)

    return avg_loss_rmse, predict_epoch, label_epoch, time_epoch
