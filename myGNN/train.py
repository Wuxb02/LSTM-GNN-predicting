"""
myGNNä¸»è®­ç»ƒè„šæœ¬

è¿™æ˜¯æ•´ä¸ªæ¡†æ¶çš„ç»Ÿä¸€å…¥å£ï¼Œæä¾›å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼š
1. é…ç½®ç®¡ç†ï¼ˆconfig.pyï¼‰
2. æ•°æ®åŠ è½½ï¼ˆdataset.pyï¼‰
3. å›¾æ„å»ºï¼ˆdistance_graph.pyï¼‰
4. æ¨¡å‹è®­ç»ƒï¼ˆnetwork_GNN.pyï¼‰
5. ç»“æœä¿å­˜

ä½¿ç”¨æ–¹æ³•ï¼š
    python train.py

é…ç½®ä¿®æ”¹ï¼š
    ä¿®æ”¹config.pyä¸­çš„å‚æ•°ï¼Œæ— éœ€å‘½ä»¤è¡Œå‚æ•°

ä½œè€…: GNNæ°”æ¸©é¢„æµ‹é¡¹ç›®
æ—¥æœŸ: 2025
"""

import os
import sys
import time
import numpy as np
import torch
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # æ— GUIç¯å¢ƒä¸‹ä½¿ç”¨

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config import create_config, print_config
from dataset import create_dataloaders
from graph.distance_graph import create_graph_from_config
from network_GNN import (get_model, get_optimizer, get_scheduler, train, val,
                         test, get_metric, get_metrics_per_step, get_exp_info,
                         get_extreme_metrics, get_extreme_metrics_per_step)


def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True


def plot_loss_curves(train_losses, val_losses, best_epoch, save_dir):
    """
    ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿

    Args:
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        best_epoch: æœ€ä½³epoch
        save_dir: ä¿å­˜ç›®å½•
    """
    plt.figure(figsize=(12, 5))

    # å­å›¾1: å®Œæ•´è®­ç»ƒæ›²çº¿
    plt.subplot(1, 2, 1)
    epochs = range(1, len(train_losses) + 1)
    plt.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=1.5, alpha=0.8)
    plt.plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=1.5, alpha=0.8)
    plt.axvline(x=best_epoch, color='g', linestyle='--', linewidth=1.5,
                label=f'Best Epoch ({best_epoch})')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)

    # å­å›¾2: å¯¹æ•°å°ºåº¦æŸ¥çœ‹ç»†èŠ‚
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=1.5, alpha=0.8)
    plt.plot(epochs, val_losses, 'r-', label='Val Loss', linewidth=1.5, alpha=0.8)
    plt.axvline(x=best_epoch, color='g', linestyle='--', linewidth=1.5,
                label=f'Best Epoch ({best_epoch})')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss (log scale)', fontsize=12)
    plt.title('Training and Validation Loss (Log Scale)', fontsize=14, fontweight='bold')
    plt.yscale('log')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3, which='both')

    plt.tight_layout()
    plt.savefig(save_dir / 'loss_curves.png', dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ“ æŸå¤±æ›²çº¿å›¾å·²ä¿å­˜: {save_dir / 'loss_curves.png'}")


def save_loss_history(train_losses, val_losses, best_epoch, save_dir):
    """
    ä¿å­˜è¯¦ç»†çš„losså†å²è®°å½•åˆ°æ–‡æœ¬æ–‡ä»¶

    Args:
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        best_epoch: æœ€ä½³epoch
        save_dir: ä¿å­˜ç›®å½•
    """
    with open(save_dir / 'loss_history.txt', 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("è®­ç»ƒæŸå¤±å†å²è®°å½•\n")
        f.write("=" * 80 + "\n\n")

        f.write(f"æ€»è®­ç»ƒè½®æ•°: {len(train_losses)}\n")
        f.write(f"æœ€ä½³Epoch: {best_epoch}\n")
        f.write(f"æœ€ä½³éªŒè¯æŸå¤±: {val_losses[best_epoch-1]:.6f}\n\n")

        # ç»Ÿè®¡ä¿¡æ¯
        f.write("ã€è®­ç»ƒæŸå¤±ç»Ÿè®¡ã€‘\n")
        f.write(f"  æœ€å°å€¼: {min(train_losses):.6f} (Epoch {train_losses.index(min(train_losses))+1})\n")
        f.write(f"  æœ€å¤§å€¼: {max(train_losses):.6f} (Epoch {train_losses.index(max(train_losses))+1})\n")
        f.write(f"  æœ€ç»ˆå€¼: {train_losses[-1]:.6f}\n")
        f.write(f"  å¹³å‡å€¼: {np.mean(train_losses):.6f}\n\n")

        f.write("ã€éªŒè¯æŸå¤±ç»Ÿè®¡ã€‘\n")
        f.write(f"  æœ€å°å€¼: {min(val_losses):.6f} (Epoch {val_losses.index(min(val_losses))+1})\n")
        f.write(f"  æœ€å¤§å€¼: {max(val_losses):.6f} (Epoch {val_losses.index(max(val_losses))+1})\n")
        f.write(f"  æœ€ç»ˆå€¼: {val_losses[-1]:.6f}\n")
        f.write(f"  å¹³å‡å€¼: {np.mean(val_losses):.6f}\n\n")

        # è¯¦ç»†è®°å½•
        f.write("=" * 80 + "\n")
        f.write("é€è½®æŸå¤±è¯¦æƒ…\n")
        f.write("=" * 80 + "\n")
        f.write(f"{'Epoch':>6} | {'Train Loss':>12} | {'Val Loss':>12} | {'Improvement':>12}\n")
        f.write("-" * 80 + "\n")

        for i in range(len(train_losses)):
            epoch = i + 1
            train_loss = train_losses[i]
            val_loss = val_losses[i]

            # è®¡ç®—æ”¹å–„æƒ…å†µ
            if i == 0:
                improvement = "-"
            else:
                improvement = val_losses[i-1] - val_loss
                improvement = f"{improvement:+.6f}"

            # æ ‡è®°æœ€ä½³epoch
            marker = " *BEST*" if epoch == best_epoch else ""

            f.write(f"{epoch:6d} | {train_loss:12.6f} | {val_loss:12.6f} | {improvement:>12} {marker}\n")

        f.write("=" * 80 + "\n")

    print(f"âœ“ æŸå¤±å†å²è®°å½•å·²ä¿å­˜: {save_dir / 'loss_history.txt'}")


def create_save_dir(config):
    """
    åˆ›å»ºä¿å­˜ç›®å½•

    Args:
        config: é…ç½®å¯¹è±¡

    Returns:
        save_dir: ä¿å­˜ç›®å½•è·¯å¾„
    """
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼šsave_path/æ¨¡å‹å_æ—¶é—´æˆ³
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    save_dir = Path(config.save_path) / f"{config.exp_model}_{timestamp}"
    save_dir.mkdir(parents=True, exist_ok=True)
    return save_dir


def save_results(save_dir, config, arch_config, best_epoch, train_results, val_results, test_results):
    """
    ä¿å­˜è®­ç»ƒç»“æœ

    Args:
        save_dir: ä¿å­˜ç›®å½•
        config: é…ç½®å¯¹è±¡
        arch_config: æ¨¡å‹æ¶æ„é…ç½®å¯¹è±¡
        best_epoch: æœ€ä½³epoch
        train_results: è®­ç»ƒé›†ç»“æœå­—å…¸
        val_results: éªŒè¯é›†ç»“æœå­—å…¸
        test_results: æµ‹è¯•é›†ç»“æœå­—å…¸
    """
    # 1. ä¿å­˜å®Œæ•´é…ç½®ï¼ˆåŒ…å«æ‰€æœ‰Configå’ŒArchConfigå‚æ•°ï¼‰
    config_str = f"""
{'=' * 80}
myGNNè®­ç»ƒé…ç½® - å®Œæ•´è®°å½•
{'=' * 80}

ã€æ•°æ®é…ç½®ã€‘
  æ•°æ®è·¯å¾„: {config.MetData_fp}
  ç«™ç‚¹ä¿¡æ¯è·¯å¾„: {config.station_info_fp}
  æ•°æ®é›†æ ‡è¯†: {config.dataset_num}
  èŠ‚ç‚¹æ•°é‡: {config.node_num}
  åŸå¸‚æ•°é‡: {config.city_num}

ã€æ•°æ®é›†åˆ’åˆ†ã€‘
  è®­ç»ƒé›†: ç´¢å¼• {config.train_start}-{config.train_end-1} ({config.train_end - config.train_start} å¤©)
  éªŒè¯é›†: ç´¢å¼• {config.val_start}-{config.val_end-1} ({config.val_end - config.val_start} å¤©)
  æµ‹è¯•é›†: ç´¢å¼• {config.test_start}-{config.test_end-1} ({config.test_end - config.test_start} å¤©)

ã€æ—¶é—´çª—å£é…ç½®ã€‘
  å†å²çª—å£é•¿åº¦ (hist_len): {config.hist_len} å¤©
  é¢„æµ‹é•¿åº¦ (pred_len): {config.pred_len} å¤©

ã€ç‰¹å¾é…ç½®ã€‘
  åŸå§‹ç‰¹å¾ç»´åº¦ (base_feature_dim): {config.base_feature_dim}
  é¢„æµ‹ç›®æ ‡ç´¢å¼• (target_feature_idx): {config.target_feature_idx}
  é€‰æ‹©ç‰¹å¾ (feature_indices): {config.feature_indices if config.feature_indices else 'æ‰€æœ‰åŸºç¡€ç‰¹å¾(0-23)'}
  æ—¶é—´ç¼–ç  (add_temporal_encoding): {'å¯ç”¨' if config.add_temporal_encoding else 'ç¦ç”¨'}
  æ—¶é—´ç‰¹å¾ç»´åº¦ (temporal_features): {config.temporal_features if config.add_temporal_encoding else 0}
  æœ€ç»ˆè¾“å…¥ç»´åº¦ (in_dim): {config.in_dim}
  æ•°æ®æ ‡å‡†åŒ–å‚æ•°:
    - ta_mean: {config.ta_mean:.6f}
    - ta_std: {config.ta_std:.6f}

ã€æ¨¡å‹é…ç½®ã€‘
  æ¨¡å‹ç±»å‹ (exp_model): {config.exp_model}

ã€å›¾ç»“æ„é…ç½®ã€‘
  å›¾ç±»å‹ (graph_type): {config.graph_type}
  {'Kè¿‘é‚»æ•°é‡ (top_neighbors): ' + str(config.top_neighbors) if config.graph_type in ['inv_dis', 'knn'] else ''}
  {'ä½¿ç”¨è¾¹å±æ€§ (use_edge_attr): ' + str(config.use_edge_attr) if config.graph_type in ['inv_dis', 'knn'] else ''}
  {'ç©ºé—´ç›¸ä¼¼æ€§é‚»å±…æ•° (spatial_sim_top_k): ' + str(config.spatial_sim_top_k) if config.graph_type == 'spatial_similarity' else ''}
  {'é‚»åŸŸæƒé‡ç³»æ•° (spatial_sim_alpha): ' + str(config.spatial_sim_alpha) if config.graph_type == 'spatial_similarity' else ''}
  {'ä½¿ç”¨é‚»åŸŸç›¸ä¼¼æ€§ (spatial_sim_use_neighborhood): ' + str(config.spatial_sim_use_neighborhood) if config.graph_type == 'spatial_similarity' else ''}
  {'åˆå§‹ç©ºé—´é‚»å±…æ•° (spatial_sim_initial_neighbors): ' + str(config.spatial_sim_initial_neighbors) if config.graph_type == 'spatial_similarity' else ''}

ã€è®­ç»ƒé…ç½®ã€‘
  æ‰¹æ¬¡å¤§å° (batch_size): {config.batch_size}
  æœ€å¤§è®­ç»ƒè½®æ•° (epochs): {config.epochs}
  å­¦ä¹ ç‡ (lr): {config.lr}
  æƒé‡è¡°å‡ (weight_decay): {config.weight_decay}
  æ—©åœè€å¿ƒå€¼ (early_stop): {config.early_stop}
  éšæœºç§å­ (seed): {config.seed}

ã€ä¼˜åŒ–å™¨é…ç½®ã€‘
  ä¼˜åŒ–å™¨ç±»å‹ (optimizer): {config.optimizer}
  {'åŠ¨é‡ (momentum): ' + str(config.momentum) if config.optimizer == 'SGD' else ''}
  {'Betas (betas): ' + str(config.betas) if config.optimizer in ['Adam', 'AdamW'] else ''}

ã€å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®ã€‘
  è°ƒåº¦å™¨ç±»å‹ (scheduler): {config.scheduler if config.scheduler else 'ä¸ä½¿ç”¨'}
  {'StepLR - Step Size (step_size): ' + str(config.step_size) if config.scheduler == 'StepLR' else ''}
  {'StepLR - Gamma (gamma): ' + str(config.gamma) if config.scheduler == 'StepLR' else ''}
  {'CosineAnnealingLR - T_max (T_max): ' + str(config.T_max) if config.scheduler == 'CosineAnnealingLR' else ''}
  {'CosineAnnealingLR - Eta_min (eta_min): ' + str(config.eta_min) if config.scheduler == 'CosineAnnealingLR' else ''}
  {'ReduceLROnPlateau - Patience (patience): ' + str(config.patience) if config.scheduler == 'ReduceLROnPlateau' else ''}
  {'ReduceLROnPlateau - Factor (factor): ' + str(config.factor) if config.scheduler == 'ReduceLROnPlateau' else ''}
  {'MultiStepLR - Milestones (milestones): ' + str(config.milestones) if config.scheduler == 'MultiStepLR' else ''}
  {'MultiStepLR - Gamma (gamma): ' + str(config.gamma) if config.scheduler == 'MultiStepLR' else ''}

ã€æŸå¤±å‡½æ•°é…ç½®ã€‘
  æŸå¤±ç±»å‹ (loss_type): {config.loss_config.loss_type}
  {'åŠ æƒè¶‹åŠ¿æŸå¤±å‚æ•°:' if config.loss_config.loss_type == 'WeightedTrend' else ''}
  {'  - è­¦æˆ’é˜ˆå€¼ (alert_temp): ' + str(config.loss_config.alert_temp) + 'Â°C' if config.loss_config.loss_type == 'WeightedTrend' else ''}
  {'  - æ¼æŠ¥æƒé‡ (c_under): ' + str(config.loss_config.c_under) if config.loss_config.loss_type == 'WeightedTrend' else ''}
  {'  - è¯¯æŠ¥æƒé‡ (c_over): ' + str(config.loss_config.c_over) if config.loss_config.loss_type == 'WeightedTrend' else ''}
  {'  - é«˜æ¸©æƒé‡ (c_default_high): ' + str(config.loss_config.c_default_high) if config.loss_config.loss_type == 'WeightedTrend' else ''}
  {'å¤šé˜ˆå€¼åŠ æƒå‚æ•°:' if config.loss_config.loss_type == 'MultiThreshold' else ''}
  {'  - æ¸©åº¦é˜ˆå€¼ (multi_thresholds): ' + str(config.loss_config.multi_thresholds) if config.loss_config.loss_type == 'MultiThreshold' else ''}
  {'  - æƒé‡åˆ—è¡¨ (multi_weights): ' + str(config.loss_config.multi_weights) if config.loss_config.loss_type == 'MultiThreshold' else ''}
  {'å­£èŠ‚åŠ æƒå‚æ•°:' if config.loss_config.loss_type == 'SeasonalWeighted' else ''}
  {'  - å¤å­£æƒé‡ (summer_weight): ' + str(config.loss_config.summer_weight) if config.loss_config.loss_type == 'SeasonalWeighted' else ''}
  {'  - å†¬å­£æƒé‡ (winter_weight): ' + str(config.loss_config.winter_weight) if config.loss_config.loss_type == 'SeasonalWeighted' else ''}
  {'  - æ˜¥ç§‹æƒé‡ (spring_fall_weight): ' + str(config.loss_config.spring_fall_weight) if config.loss_config.loss_type == 'SeasonalWeighted' else ''}


ã€æ¨¡å‹æ¶æ„é…ç½® (ArchConfig)ã€‘
  éšè—å±‚ç»´åº¦ (hid_dim): {arch_config.hid_dim}
  MLPå±‚æ•° (MLP_layer): {arch_config.MLP_layer}
  æ¿€æ´»å‡½æ•° (AF): {arch_config.AF}
  è§„èŒƒåŒ–ç±»å‹ (norm_type): {arch_config.norm_type}
  ä½¿ç”¨Dropout (dropout): {arch_config.dropout}

  {'GATç‰¹å®šå‚æ•°:' if 'GAT' in config.exp_model else ''}
  {'  - GATå±‚æ•° (GAT_layer): ' + str(arch_config.GAT_layer) if 'GAT' in config.exp_model else ''}
  {'  - æ³¨æ„åŠ›å¤´æ•° (heads): ' + str(arch_config.heads) if 'GAT' in config.exp_model else ''}
  {'  - å±‚å†…Dropout (intra_drop): ' + str(arch_config.intra_drop) if 'GAT' in config.exp_model else ''}
  {'  - å±‚é—´Dropout (inter_drop): ' + str(arch_config.inter_drop) if 'GAT' in config.exp_model else ''}

  {'GraphSAGEç‰¹å®šå‚æ•°:' if 'SAGE' in config.exp_model else ''}
  {'  - SAGEå±‚æ•° (SAGE_layer): ' + str(arch_config.SAGE_layer) if 'SAGE' in config.exp_model else ''}
  {'  - èšåˆæ–¹å¼ (aggr): ' + str(arch_config.aggr) if 'SAGE' in config.exp_model else ''}
  {'  - å±‚é—´Dropout (inter_drop): ' + str(arch_config.inter_drop) if 'SAGE' in config.exp_model else ''}

  LSTMå‚æ•°:
    - LSTMå±‚æ•° (lstm_num_layers): {arch_config.lstm_num_layers}
    - LSTM Dropout (lstm_dropout): {arch_config.lstm_dropout}
    - åŒå‘LSTM (lstm_bidirectional): {arch_config.lstm_bidirectional}

  å¾ªç¯è§£ç å™¨å‚æ•°:
    - ä½¿ç”¨å¾ªç¯è§£ç å™¨ (use_recurrent_decoder): {arch_config.use_recurrent_decoder}
    {'- è§£ç å™¨ç±»å‹ (decoder_type): ' + str(arch_config.decoder_type) if arch_config.use_recurrent_decoder else ''}
    {'- è§£ç å™¨å±‚æ•° (decoder_num_layers): ' + str(arch_config.decoder_num_layers) if arch_config.use_recurrent_decoder else ''}
    {'- è§£ç å™¨Dropout (decoder_dropout): ' + str(arch_config.decoder_dropout) if arch_config.use_recurrent_decoder else ''}
    {'- ä½¿ç”¨ä¸Šä¸‹æ–‡æ³¨å…¥ (decoder_use_context): ' + str(arch_config.decoder_use_context) if arch_config.use_recurrent_decoder else ''}
    {'- å‰ç½®MLPå±‚æ•° (decoder_mlp_layers): ' + str(arch_config.decoder_mlp_layers) if arch_config.use_recurrent_decoder else ''}

ã€å¯è§†åŒ–é…ç½®ã€‘
  è‡ªåŠ¨å¯è§†åŒ– (auto_visualize): {config.auto_visualize}
  å¯è§†åŒ–æ­¥é•¿ (viz_pred_steps): {config.viz_pred_steps}
  ç»˜åˆ¶æ‰€æœ‰ç«™ç‚¹ (viz_plot_all_stations): {config.viz_plot_all_stations}
  å›¾è¡¨DPI (viz_dpi): {config.viz_dpi}
  ä½¿ç”¨åœ°ç†åº•å›¾ (viz_use_basemap): {config.viz_use_basemap}

ã€è®¾å¤‡é…ç½®ã€‘
  è®¡ç®—è®¾å¤‡ (device): {config.device}

ã€è·¯å¾„é…ç½®ã€‘
  ä¿å­˜è·¯å¾„ (save_path): {config.save_path}
  æ—¥å¿—è·¯å¾„ (log_path): {config.log_path}

{'=' * 80}
è®­ç»ƒç»“æœ
{'=' * 80}
æœ€ä½³Epoch: {best_epoch}
{'=' * 80}
"""
    with open(save_dir / 'config.txt', 'w', encoding='utf-8') as f:
        f.write(config_str)

    # 2. ä¿å­˜è®­ç»ƒé›†ç»“æœ
    np.save(save_dir / 'train_predict.npy', train_results['predict'])
    np.save(save_dir / 'train_label.npy', train_results['label'])
    np.save(save_dir / 'train_time.npy', train_results['time'])

    # 3. ä¿å­˜éªŒè¯é›†ç»“æœ
    np.save(save_dir / 'val_predict.npy', val_results['predict'])
    np.save(save_dir / 'val_label.npy', val_results['label'])
    np.save(save_dir / 'val_time.npy', val_results['time'])

    # 4. ä¿å­˜æµ‹è¯•é›†ç»“æœ
    np.save(save_dir / 'test_predict.npy', test_results['predict'])
    np.save(save_dir / 'test_label.npy', test_results['label'])
    np.save(save_dir / 'test_time.npy', test_results['time'])

    # 4. è®¡ç®—æç«¯å€¼ç›‘æ§æŒ‡æ ‡
    print("\næ­£åœ¨è®¡ç®—æç«¯å€¼ç›‘æ§æŒ‡æ ‡...")

    # å®šä¹‰æ¸©åº¦é˜ˆå€¼
    high_thresholds = [28, 30, 35]  # é«˜æ¸©é˜ˆå€¼: è½»åº¦, ä¸­åº¦, æç«¯
    low_thresholds = [0, -5, -10]   # ä½æ¸©é˜ˆå€¼: è½»åº¦, ä¸­åº¦, æç«¯

    # è®¡ç®—æ•´ä½“æç«¯å€¼æŒ‡æ ‡
    train_extreme_metrics = get_extreme_metrics(
        train_results['predict'], train_results['label'],
        high_thresholds=high_thresholds,
        low_thresholds=low_thresholds
    )
    val_extreme_metrics = get_extreme_metrics(
        val_results['predict'], val_results['label'],
        high_thresholds=high_thresholds,
        low_thresholds=low_thresholds
    )
    test_extreme_metrics = get_extreme_metrics(
        test_results['predict'], test_results['label'],
        high_thresholds=high_thresholds,
        low_thresholds=low_thresholds
    )

    # è®¡ç®—æŒ‰æ­¥é•¿åˆ†è§£çš„æç«¯å€¼æŒ‡æ ‡
    train_extreme_per_step = get_extreme_metrics_per_step(
        train_results['predict'], train_results['label'],
        high_thresholds=high_thresholds,
        low_thresholds=low_thresholds
    )
    val_extreme_per_step = get_extreme_metrics_per_step(
        val_results['predict'], val_results['label'],
        high_thresholds=high_thresholds,
        low_thresholds=low_thresholds
    )
    test_extreme_per_step = get_extreme_metrics_per_step(
        test_results['predict'], test_results['label'],
        high_thresholds=high_thresholds,
        low_thresholds=low_thresholds
    )

    # 5. ä¿å­˜è¯„ä¼°æŒ‡æ ‡ (åŒ…å«æç«¯å€¼ä¿¡æ¯)
    metrics_str = f"""
è¯„ä¼°æŒ‡æ ‡
{'=' * 80}
è®­ç»ƒé›†:
  RMSE: {train_results['rmse']:.4f} Â°C
  MAE:  {train_results['mae']:.4f} Â°C
  RÂ²:   {train_results['r2']:.4f}
  Bias: {train_results['bias']:+.4f} Â°C

éªŒè¯é›†:
  RMSE: {val_results['rmse']:.4f} Â°C
  MAE:  {val_results['mae']:.4f} Â°C
  RÂ²:   {val_results['r2']:.4f}
  Bias: {val_results['bias']:+.4f} Â°C

æµ‹è¯•é›†:
  RMSE: {test_results['rmse']:.4f} Â°C
  MAE:  {test_results['mae']:.4f} Â°C
  RÂ²:   {test_results['r2']:.4f}
  Bias: {test_results['bias']:+.4f} Â°C

æŒ‡æ ‡è¯´æ˜:
  RMSE (å‡æ–¹æ ¹è¯¯å·®): å€¼è¶Šå°è¶Šå¥½ï¼Œå•ä½ä¸ºÂ°C
  MAE (å¹³å‡ç»å¯¹è¯¯å·®): å€¼è¶Šå°è¶Šå¥½ï¼Œå•ä½ä¸ºÂ°C
  RÂ² (å†³å®šç³»æ•°): èŒƒå›´[0, 1]ï¼Œå€¼è¶Šæ¥è¿‘1è¶Šå¥½
    - RÂ² = 1: å®Œç¾é¢„æµ‹
    - RÂ² = 0: é¢„æµ‹æ•ˆæœç­‰åŒäºä½¿ç”¨å¹³å‡å€¼
    - RÂ² < 0: é¢„æµ‹æ•ˆæœå·®äºä½¿ç”¨å¹³å‡å€¼
  Bias (ç³»ç»Ÿæ€§åå·®): å•ä½ä¸ºÂ°C
    - Bias > 0: æ¨¡å‹å€¾å‘äºé«˜ä¼°æ¸©åº¦
    - Bias = 0: æ¨¡å‹æ— ç³»ç»Ÿæ€§åå·®
    - Bias < 0: æ¨¡å‹å€¾å‘äºä½ä¼°æ¸©åº¦


{'=' * 80}
æç«¯å€¼ç›‘æ§æŒ‡æ ‡
{'=' * 80}

ã€æ ·æœ¬åˆ†å¸ƒç»Ÿè®¡ã€‘
"""

    # æ·»åŠ æ ·æœ¬åˆ†å¸ƒç»Ÿè®¡
    if train_extreme_metrics['normal_temp']:
        metrics_str += f"è®­ç»ƒé›†:\n"
        metrics_str += f"  æ­£å¸¸æ¸©åº¦ ({train_extreme_metrics['normal_temp']['range']}): "
        metrics_str += f"{train_extreme_metrics['normal_temp']['sample_count']}æ ·æœ¬ "
        metrics_str += f"({train_extreme_metrics['normal_temp']['percentage']:.1f}%)\n"

        for ht in train_extreme_metrics['high_temp']:
            metrics_str += f"  é«˜æ¸© (â‰¥{ht['threshold']}Â°C): "
            metrics_str += f"{ht['sample_count']}æ ·æœ¬ ({ht['percentage']:.1f}%)\n"

        for lt in train_extreme_metrics['low_temp']:
            metrics_str += f"  ä½æ¸© (â‰¤{lt['threshold']}Â°C): "
            metrics_str += f"{lt['sample_count']}æ ·æœ¬ ({lt['percentage']:.1f}%)\n"

    if val_extreme_metrics['normal_temp']:
        metrics_str += f"\néªŒè¯é›†:\n"
        metrics_str += f"  æ­£å¸¸æ¸©åº¦ ({val_extreme_metrics['normal_temp']['range']}): "
        metrics_str += f"{val_extreme_metrics['normal_temp']['sample_count']}æ ·æœ¬ "
        metrics_str += f"({val_extreme_metrics['normal_temp']['percentage']:.1f}%)\n"

        for ht in val_extreme_metrics['high_temp']:
            metrics_str += f"  é«˜æ¸© (â‰¥{ht['threshold']}Â°C): "
            metrics_str += f"{ht['sample_count']}æ ·æœ¬ ({ht['percentage']:.1f}%)\n"

        for lt in val_extreme_metrics['low_temp']:
            metrics_str += f"  ä½æ¸© (â‰¤{lt['threshold']}Â°C): "
            metrics_str += f"{lt['sample_count']}æ ·æœ¬ ({lt['percentage']:.1f}%)\n"

        metrics_str += f"\næµ‹è¯•é›†:\n"
        metrics_str += f"  æ­£å¸¸æ¸©åº¦ ({test_extreme_metrics['normal_temp']['range']}): "
        metrics_str += f"{test_extreme_metrics['normal_temp']['sample_count']}æ ·æœ¬ "
        metrics_str += f"({test_extreme_metrics['normal_temp']['percentage']:.1f}%)\n"

        for ht in test_extreme_metrics['high_temp']:
            metrics_str += f"  é«˜æ¸© (â‰¥{ht['threshold']}Â°C): "
            metrics_str += f"{ht['sample_count']}æ ·æœ¬ ({ht['percentage']:.1f}%)\n"

        for lt in test_extreme_metrics['low_temp']:
            metrics_str += f"  ä½æ¸© (â‰¤{lt['threshold']}Â°C): "
            metrics_str += f"{lt['sample_count']}æ ·æœ¬ ({lt['percentage']:.1f}%)\n"

    # æ·»åŠ é«˜æ¸©äº‹ä»¶æ€§èƒ½æŒ‡æ ‡
    metrics_str += f"\nã€é«˜æ¸©äº‹ä»¶æ€§èƒ½ã€‘\n"
    metrics_str += f"è®­ç»ƒé›†:\n"
    for ht in train_extreme_metrics['high_temp']:
        metrics_str += f"  æç«¯é«˜æ¸© (â‰¥{ht['threshold']}Â°C):\n"
        metrics_str += f"    RMSE: {ht['rmse']:.4f} Â°C, MAE: {ht['mae']:.4f} Â°C, Bias: {ht['bias']:+.4f} Â°C\n"
        metrics_str += f"    ä½ä¼°ç‡: {ht['underestimate_rate']:.1f}%, é«˜ä¼°ç‡: {ht['overestimate_rate']:.1f}%\n"
        metrics_str += f"    å‘½ä¸­ç‡: {ht['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {ht['false_alarm_rate']:.1f}%, æ¼æŠ¥ç‡: {ht['miss_rate']:.1f}%\n"

    metrics_str += f"\néªŒè¯é›†:\n"
    for ht in val_extreme_metrics['high_temp']:
        metrics_str += f"  æç«¯é«˜æ¸© (â‰¥{ht['threshold']}Â°C):\n"
        metrics_str += f"    RMSE: {ht['rmse']:.4f} Â°C, MAE: {ht['mae']:.4f} Â°C, Bias: {ht['bias']:+.4f} Â°C\n"
        metrics_str += f"    ä½ä¼°ç‡: {ht['underestimate_rate']:.1f}%, é«˜ä¼°ç‡: {ht['overestimate_rate']:.1f}%\n"
        metrics_str += f"    å‘½ä¸­ç‡: {ht['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {ht['false_alarm_rate']:.1f}%, æ¼æŠ¥ç‡: {ht['miss_rate']:.1f}%\n"

    metrics_str += f"\næµ‹è¯•é›†:\n"
    for ht in test_extreme_metrics['high_temp']:
        metrics_str += f"  æç«¯é«˜æ¸© (â‰¥{ht['threshold']}Â°C):\n"
        metrics_str += f"    RMSE: {ht['rmse']:.4f} Â°C, MAE: {ht['mae']:.4f} Â°C, Bias: {ht['bias']:+.4f} Â°C\n"
        metrics_str += f"    ä½ä¼°ç‡: {ht['underestimate_rate']:.1f}%, é«˜ä¼°ç‡: {ht['overestimate_rate']:.1f}%\n"
        metrics_str += f"    å‘½ä¸­ç‡: {ht['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {ht['false_alarm_rate']:.1f}%, æ¼æŠ¥ç‡: {ht['miss_rate']:.1f}%\n"

    # æ·»åŠ ä½æ¸©äº‹ä»¶æ€§èƒ½æŒ‡æ ‡
    metrics_str += f"\nã€ä½æ¸©äº‹ä»¶æ€§èƒ½ã€‘\n"
    metrics_str += f"è®­ç»ƒé›†:\n"
    for lt in train_extreme_metrics['low_temp']:
        metrics_str += f"  æç«¯ä½æ¸© (â‰¤{lt['threshold']}Â°C):\n"
        metrics_str += f"    RMSE: {lt['rmse']:.4f} Â°C, MAE: {lt['mae']:.4f} Â°C, Bias: {lt['bias']:+.4f} Â°C\n"
        metrics_str += f"    é«˜ä¼°ç‡: {lt['overestimate_rate']:.1f}%, ä½ä¼°ç‡: {lt['underestimate_rate']:.1f}%\n"
        metrics_str += f"    å‘½ä¸­ç‡: {lt['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {lt['false_alarm_rate']:.1f}%, æ¼æŠ¥ç‡: {lt['miss_rate']:.1f}%\n"

    metrics_str += f"\néªŒè¯é›†:\n"
    for lt in val_extreme_metrics['low_temp']:
        metrics_str += f"  æç«¯ä½æ¸© (â‰¤{lt['threshold']}Â°C):\n"
        metrics_str += f"    RMSE: {lt['rmse']:.4f} Â°C, MAE: {lt['mae']:.4f} Â°C, Bias: {lt['bias']:+.4f} Â°C\n"
        metrics_str += f"    é«˜ä¼°ç‡: {lt['overestimate_rate']:.1f}%, ä½ä¼°ç‡: {lt['underestimate_rate']:.1f}%\n"
        metrics_str += f"    å‘½ä¸­ç‡: {lt['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {lt['false_alarm_rate']:.1f}%, æ¼æŠ¥ç‡: {lt['miss_rate']:.1f}%\n"

    metrics_str += f"\næµ‹è¯•é›†:\n"
    for lt in test_extreme_metrics['low_temp']:
        metrics_str += f"  æç«¯ä½æ¸© (â‰¤{lt['threshold']}Â°C):\n"
        metrics_str += f"    RMSE: {lt['rmse']:.4f} Â°C, MAE: {lt['mae']:.4f} Â°C, Bias: {lt['bias']:+.4f} Â°C\n"
        metrics_str += f"    é«˜ä¼°ç‡: {lt['overestimate_rate']:.1f}%, ä½ä¼°ç‡: {lt['underestimate_rate']:.1f}%\n"
        metrics_str += f"    å‘½ä¸­ç‡: {lt['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {lt['false_alarm_rate']:.1f}%, æ¼æŠ¥ç‡: {lt['miss_rate']:.1f}%\n"

    metrics_str += f"{'=' * 80}\n"

    # ä¿å­˜ metrics.txt
    with open(save_dir / 'metrics.txt', 'w', encoding='utf-8') as f:
        f.write(metrics_str)

    # 6. ä¿å­˜æŒ‰é¢„æµ‹æ­¥é•¿åˆ†è§£çš„æŒ‡æ ‡
    train_metrics_per_step = get_metrics_per_step(train_results['predict'], train_results['label'])
    val_metrics_per_step = get_metrics_per_step(val_results['predict'], val_results['label'])
    test_metrics_per_step = get_metrics_per_step(test_results['predict'], test_results['label'])

    # ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆä¾¿äºåˆ†æï¼‰
    import pandas as pd

    # è®­ç»ƒé›†æŒ‰æ­¥é•¿æŒ‡æ ‡
    train_df = pd.DataFrame(train_metrics_per_step)
    train_df.to_csv(save_dir / 'train_metrics_per_step.csv', index=False)

    # éªŒè¯é›†æŒ‰æ­¥é•¿æŒ‡æ ‡
    val_df = pd.DataFrame(val_metrics_per_step)
    val_df.to_csv(save_dir / 'val_metrics_per_step.csv', index=False)

    # æµ‹è¯•é›†æŒ‰æ­¥é•¿æŒ‡æ ‡
    test_df = pd.DataFrame(test_metrics_per_step)
    test_df.to_csv(save_dir / 'test_metrics_per_step.csv', index=False)

    # åŒæ—¶ä¿å­˜ä¸ºå¯è¯»çš„txtæ ¼å¼
    metrics_per_step_str = f"""
æŒ‰é¢„æµ‹æ­¥é•¿åˆ†è§£çš„æŒ‡æ ‡
{'=' * 80}
è®­ç»ƒé›†:
"""
    for m in train_metrics_per_step:
        metrics_per_step_str += (
            f"  ç¬¬{m['step']}æ­¥ (+{m['step']}å¤©): "
            f"RMSE={m['rmse']:.4f}Â°C, "
            f"MAE={m['mae']:.4f}Â°C, "
            f"RÂ²={m['r2']:.4f}, "
            f"Bias={m['bias']:+.4f}Â°C\n"
        )

    metrics_per_step_str += "\néªŒè¯é›†:\n"
    for m in val_metrics_per_step:
        metrics_per_step_str += (
            f"  ç¬¬{m['step']}æ­¥ (+{m['step']}å¤©): "
            f"RMSE={m['rmse']:.4f}Â°C, "
            f"MAE={m['mae']:.4f}Â°C, "
            f"RÂ²={m['r2']:.4f}, "
            f"Bias={m['bias']:+.4f}Â°C\n"
        )

    metrics_per_step_str += "\næµ‹è¯•é›†:\n"
    for m in test_metrics_per_step:
        metrics_per_step_str += (
            f"  ç¬¬{m['step']}æ­¥ (+{m['step']}å¤©): "
            f"RMSE={m['rmse']:.4f}Â°C, "
            f"MAE={m['mae']:.4f}Â°C, "
            f"RÂ²={m['r2']:.4f}, "
            f"Bias={m['bias']:+.4f}Â°C\n"
        )

    metrics_per_step_str += f"{'=' * 80}\n"

    with open(save_dir / 'metrics_per_step.txt', 'w', encoding='utf-8') as f:
        f.write(metrics_per_step_str)

    # 7. ä¿å­˜æç«¯å€¼æŒ‡æ ‡æ–‡ä»¶
    # 7.1 ä¿å­˜æ•´ä½“æç«¯å€¼æŒ‡æ ‡ä¸ºCSV
    extreme_csv_data = []

    # è®­ç»ƒé›† - é«˜æ¸©
    for ht in train_extreme_metrics['high_temp']:
        extreme_csv_data.append({
            'dataset': 'train',
            'temp_type': 'high',
            'threshold': ht['threshold'],
            'sample_count': ht['sample_count'],
            'percentage': ht['percentage'],
            'rmse': ht['rmse'],
            'mae': ht['mae'],
            'bias': ht['bias'],
            'underestimate_rate': ht['underestimate_rate'],
            'overestimate_rate': ht['overestimate_rate'],
            'hit_rate': ht['hit_rate'],
            'false_alarm_rate': ht['false_alarm_rate'],
            'miss_rate': ht['miss_rate']
        })

    # è®­ç»ƒé›† - ä½æ¸©
    for lt in train_extreme_metrics['low_temp']:
        extreme_csv_data.append({
            'dataset': 'train',
            'temp_type': 'low',
            'threshold': lt['threshold'],
            'sample_count': lt['sample_count'],
            'percentage': lt['percentage'],
            'rmse': lt['rmse'],
            'mae': lt['mae'],
            'bias': lt['bias'],
            'underestimate_rate': lt['underestimate_rate'],
            'overestimate_rate': lt['overestimate_rate'],
            'hit_rate': lt['hit_rate'],
            'false_alarm_rate': lt['false_alarm_rate'],
            'miss_rate': lt['miss_rate']
        })

    # éªŒè¯é›† - é«˜æ¸©
    for ht in val_extreme_metrics['high_temp']:
        extreme_csv_data.append({
            'dataset': 'val',
            'temp_type': 'high',
            'threshold': ht['threshold'],
            'sample_count': ht['sample_count'],
            'percentage': ht['percentage'],
            'rmse': ht['rmse'],
            'mae': ht['mae'],
            'bias': ht['bias'],
            'underestimate_rate': ht['underestimate_rate'],
            'overestimate_rate': ht['overestimate_rate'],
            'hit_rate': ht['hit_rate'],
            'false_alarm_rate': ht['false_alarm_rate'],
            'miss_rate': ht['miss_rate']
        })

    # éªŒè¯é›† - ä½æ¸©
    for lt in val_extreme_metrics['low_temp']:
        extreme_csv_data.append({
            'dataset': 'val',
            'temp_type': 'low',
            'threshold': lt['threshold'],
            'sample_count': lt['sample_count'],
            'percentage': lt['percentage'],
            'rmse': lt['rmse'],
            'mae': lt['mae'],
            'bias': lt['bias'],
            'underestimate_rate': lt['underestimate_rate'],
            'overestimate_rate': lt['overestimate_rate'],
            'hit_rate': lt['hit_rate'],
            'false_alarm_rate': lt['false_alarm_rate'],
            'miss_rate': lt['miss_rate']
        })

    # æµ‹è¯•é›† - é«˜æ¸©
    for ht in test_extreme_metrics['high_temp']:
        extreme_csv_data.append({
            'dataset': 'test',
            'temp_type': 'high',
            'threshold': ht['threshold'],
            'sample_count': ht['sample_count'],
            'percentage': ht['percentage'],
            'rmse': ht['rmse'],
            'mae': ht['mae'],
            'bias': ht['bias'],
            'underestimate_rate': ht['underestimate_rate'],
            'overestimate_rate': ht['overestimate_rate'],
            'hit_rate': ht['hit_rate'],
            'false_alarm_rate': ht['false_alarm_rate'],
            'miss_rate': ht['miss_rate']
        })

    # æµ‹è¯•é›† - ä½æ¸©
    for lt in test_extreme_metrics['low_temp']:
        extreme_csv_data.append({
            'dataset': 'test',
            'temp_type': 'low',
            'threshold': lt['threshold'],
            'sample_count': lt['sample_count'],
            'percentage': lt['percentage'],
            'rmse': lt['rmse'],
            'mae': lt['mae'],
            'bias': lt['bias'],
            'underestimate_rate': lt['underestimate_rate'],
            'overestimate_rate': lt['overestimate_rate'],
            'hit_rate': lt['hit_rate'],
            'false_alarm_rate': lt['false_alarm_rate'],
            'miss_rate': lt['miss_rate']
        })

    extreme_df = pd.DataFrame(extreme_csv_data)
    extreme_df.to_csv(save_dir / 'extreme_metrics.csv', index=False)

    # 7.2 ä¿å­˜æŒ‰æ­¥é•¿åˆ†è§£çš„æç«¯å€¼æŒ‡æ ‡ä¸ºCSV
    extreme_per_step_csv = []

    for step_data in train_extreme_per_step:
        step = step_data['step']
        # é«˜æ¸©
        for ht in step_data['high_temp']:
            extreme_per_step_csv.append({
                'dataset': 'train',
                'step': step,
                'temp_type': 'high',
                'threshold': ht['threshold'],
                'sample_count': ht['sample_count'],
                'rmse': ht['rmse'],
                'mae': ht['mae'],
                'bias': ht['bias'],
                'hit_rate': ht['hit_rate']
            })
        # ä½æ¸©
        for lt in step_data['low_temp']:
            extreme_per_step_csv.append({
                'dataset': 'train',
                'step': step,
                'temp_type': 'low',
                'threshold': lt['threshold'],
                'sample_count': lt['sample_count'],
                'rmse': lt['rmse'],
                'mae': lt['mae'],
                'bias': lt['bias'],
                'hit_rate': lt['hit_rate']
            })

    for step_data in val_extreme_per_step:
        step = step_data['step']
        # é«˜æ¸©
        for ht in step_data['high_temp']:
            extreme_per_step_csv.append({
                'dataset': 'val',
                'step': step,
                'temp_type': 'high',
                'threshold': ht['threshold'],
                'sample_count': ht['sample_count'],
                'rmse': ht['rmse'],
                'mae': ht['mae'],
                'bias': ht['bias'],
                'hit_rate': ht['hit_rate']
            })
        # ä½æ¸©
        for lt in step_data['low_temp']:
            extreme_per_step_csv.append({
                'dataset': 'val',
                'step': step,
                'temp_type': 'low',
                'threshold': lt['threshold'],
                'sample_count': lt['sample_count'],
                'rmse': lt['rmse'],
                'mae': lt['mae'],
                'bias': lt['bias'],
                'hit_rate': lt['hit_rate']
            })

    for step_data in test_extreme_per_step:
        step = step_data['step']
        # é«˜æ¸©
        for ht in step_data['high_temp']:
            extreme_per_step_csv.append({
                'dataset': 'test',
                'step': step,
                'temp_type': 'high',
                'threshold': ht['threshold'],
                'sample_count': ht['sample_count'],
                'rmse': ht['rmse'],
                'mae': ht['mae'],
                'bias': ht['bias'],
                'hit_rate': ht['hit_rate']
            })
        # ä½æ¸©
        for lt in step_data['low_temp']:
            extreme_per_step_csv.append({
                'dataset': 'test',
                'step': step,
                'temp_type': 'low',
                'threshold': lt['threshold'],
                'sample_count': lt['sample_count'],
                'rmse': lt['rmse'],
                'mae': lt['mae'],
                'bias': lt['bias'],
                'hit_rate': lt['hit_rate']
            })

    extreme_per_step_df = pd.DataFrame(extreme_per_step_csv)
    extreme_per_step_df.to_csv(save_dir / 'extreme_metrics_per_step.csv', index=False)

    print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    print(f"âœ“ æŒ‰æ­¥é•¿åˆ†è§£çš„æŒ‡æ ‡å·²ä¿å­˜:")
    print(f"  - {save_dir / 'val_metrics_per_step.csv'}")
    print(f"  - {save_dir / 'test_metrics_per_step.csv'}")
    print(f"  - {save_dir / 'metrics_per_step.txt'}")
    print(f"âœ“ æç«¯å€¼ç›‘æ§æŒ‡æ ‡å·²ä¿å­˜:")
    print(f"  - {save_dir / 'extreme_metrics.csv'}")
    print(f"  - {save_dir / 'extreme_metrics_per_step.csv'}")
    print(f"  - metrics.txt (å·²åŒ…å«æç«¯å€¼éƒ¨åˆ†)")


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 80)
    print("myGNN æ°”æ¸©é¢„æµ‹æ¨¡å‹è®­ç»ƒ")
    print("=" * 80)

    # ==================== 1. åŠ è½½é…ç½® ====================
    print("\n[1/7] åŠ è½½é…ç½®...")

    # ğŸ”¥ åŠ è½½é…ç½®
    # é…ç½®æ–¹å¼ï¼šåœ¨config.pyçš„LossConfigç±»ä¸­ï¼Œä¿®æ”¹ self.loss_type = 'MSE' æˆ– 'WeightedTrend'
    config, arch_config = create_config()

    # æ‰“å°ä½¿ç”¨çš„æŸå¤±å‡½æ•°ç±»å‹
    print(f"âœ“ æŸå¤±å‡½æ•°ç±»å‹: {config.loss_config.loss_type}")
    if config.use_enhanced_training:
        print(f"âœ“ ä½¿ç”¨å¢å¼ºè®­ç»ƒæµç¨‹")
    else:
        print(f"âœ“ ä½¿ç”¨æ ‡å‡†è®­ç»ƒæµç¨‹ï¼ˆMSEï¼‰")

    # è®¾ç½®éšæœºç§å­
    setup_seed(config.seed)

    # æ‰“å°é…ç½®ä¿¡æ¯
    print_config(config, arch_config)

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = create_save_dir(config)
    print(f"\nâœ“ ä¿å­˜ç›®å½•: {save_dir}")

    # ==================== 2. æ„å»ºå›¾ç»“æ„ ====================
    print("\n[2/7] æ„å»ºå›¾ç»“æ„...")

    # å¦‚æœä½¿ç”¨ç©ºé—´ç›¸ä¼¼æ€§å›¾ï¼Œéœ€è¦å…ˆåŠ è½½æ•°æ®å‡†å¤‡ç‰¹å¾
    feature_data = None
    if config.graph_type == 'spatial_similarity':
        print("  ç©ºé—´ç›¸ä¼¼æ€§å›¾éœ€è¦ç‰¹å¾æ•°æ®ï¼Œå…ˆåŠ è½½è®­ç»ƒæ•°æ®...")
        MetData_temp = np.load(config.MetData_fp)
        # ä½¿ç”¨é…ç½®ä¸­çš„ç‰¹å¾ç»´åº¦ï¼Œè€Œéç¡¬ç¼–ç çš„19
        num_features = 24  # åŸºç¡€ç‰¹å¾ç»´åº¦ï¼ˆ0-23ï¼Œç§»é™¤äº†doyå’Œmonthï¼‰
        train_data_temp = MetData_temp[config.train_start:config.train_end, :, :num_features]
        feature_data = train_data_temp.mean(axis=0)
        print(f"  âœ“ ç‰¹å¾æ•°æ®å½¢çŠ¶: {feature_data.shape}")

    graph = create_graph_from_config(config, feature_data=feature_data)
    print(f"âœ“ å›¾ç±»å‹: {graph.edge_form}")
    print(f"  èŠ‚ç‚¹æ•°: {graph.node_num}")
    print(f"  è¾¹æ•°: {graph.edge_index.shape[1]}")
    print(f"  ä½¿ç”¨è¾¹å±æ€§: {graph.use_edge_attr}")

    # ==================== 3. åŠ è½½æ•°æ® ====================
    print("\n[3/7] åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader, stats = create_dataloaders(config, graph)

    # æ›´æ–°é…ç½®ä¸­çš„æ ‡å‡†åŒ–å‚æ•°
    config.ta_mean = stats['ta_mean']
    config.ta_std = stats['ta_std']

    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
    print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    print(f"  æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")

    # ==================== 4. åˆ›å»ºæ¨¡å‹ ====================
    print("\n[4/7] åˆ›å»ºæ¨¡å‹...")
    model = get_model(config, arch_config)

    model = model.to(config.device)

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"âœ“ æ¨¡å‹: {config.exp_model}")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  è®¾å¤‡: {config.device}")

    # ==================== 5. è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ====================
    print("\n[5/7] è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨...")
    optimizer = get_optimizer(model, config)
    scheduler = get_scheduler(optimizer, config)

    print(f"âœ“ ä¼˜åŒ–å™¨: {config.optimizer}")
    if scheduler is not None:
        print(f"âœ“ è°ƒåº¦å™¨: {config.scheduler}")
    else:
        print(f"âœ“ è°ƒåº¦å™¨: ä¸ä½¿ç”¨")

    # ==================== 5.5. è®¾ç½®æŸå¤±å‡½æ•° ====================
    # æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©è®­ç»ƒæµç¨‹
    if config.use_enhanced_training:
        print("\n[5.5/7] è®¾ç½®å¢å¼ºæŸå¤±å‡½æ•°...")
        from train_enhanced import get_loss_function, train_epoch as train_enhanced, validate_epoch
        criterion = get_loss_function(config)
        use_enhanced = True
    else:
        print("\n[5.5/7] ä½¿ç”¨æ ‡å‡†MSEæŸå¤±å‡½æ•°...")
        criterion = None  # network_GNN.py ä¸­æœ‰å…¨å±€çš„ criterion
        use_enhanced = False

    # ==================== 6. è®­ç»ƒæ¨¡å‹ ====================
    print("\n[6/7] å¼€å§‹è®­ç»ƒ...")
    print(get_exp_info(config))

    best_val_loss = float('inf')
    best_epoch = 0
    patience = 0
    best_val_results = None

    train_losses = []
    val_losses = []

    for epoch in range(1, config.epochs + 1):
        epoch_start_time = time.time()

        # è®­ç»ƒ - æ ¹æ®é…ç½®é€‰æ‹©è®­ç»ƒæ–¹æ³•
        if use_enhanced:
            # ä½¿ç”¨å¢å¼ºè®­ç»ƒæµç¨‹ï¼ˆå¸¦åŠ æƒè¶‹åŠ¿æŸå¤±ï¼‰
            # å‚æ•°é¡ºåºï¼šmodel, dataloader, optimizer, scheduler, criterion, config, device
            train_loss = train_enhanced(model, train_loader, optimizer, scheduler, criterion, config, config.device)
        else:
            # ä½¿ç”¨æ ‡å‡†è®­ç»ƒæµç¨‹ï¼ˆMSEæŸå¤±ï¼‰
            train_loss = train(train_loader, model, optimizer, scheduler, config)
        train_losses.append(train_loss)

        # éªŒè¯ - æ ¹æ®é…ç½®é€‰æ‹©éªŒè¯æ–¹æ³•
        if use_enhanced:
            # ä½¿ç”¨å¢å¼ºéªŒè¯æµç¨‹ï¼ˆvalidate_epoch ç°åœ¨è¿”å›å®Œæ•´çš„4ä¸ªå€¼ï¼‰
            # å‚æ•°é¡ºåºï¼šmodel, dataloader, criterion, config, device
            val_loss, val_pred, val_label, val_time = validate_epoch(
                model, val_loader, criterion, config, config.device
            )
        else:
            # ä½¿ç”¨æ ‡å‡†éªŒè¯æµç¨‹
            val_loss, val_pred, val_label, val_time = val(val_loader, model, config)
        val_losses.append(val_loss)

        # ReduceLROnPlateauè°ƒåº¦å™¨éœ€è¦åœ¨éªŒè¯åè°ƒç”¨
        if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(val_loss)

        epoch_time = time.time() - epoch_start_time

        # æ‰“å°è¿›åº¦
        print(f"Epoch {epoch:3d}/{config.epochs} | "
              f"Train Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"Time: {epoch_time:.2f}s | "
              f"LR: {optimizer.param_groups[0]['lr']:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch
            patience = 0

            # ä¿å­˜æ¨¡å‹ï¼ˆåŒ…å«configå’Œgraphï¼Œä¿è¯å¯è§£é‡Šæ€§åˆ†æä¸€è‡´æ€§ï¼‰
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'config': config,
                'arch_config': arch_config,
                'graph': graph,  # ğŸ”‘ æ–°å¢: ä¿å­˜è®­ç»ƒæ—¶çš„å›¾ç»“æ„
                'epoch': epoch,
                'val_loss': val_loss,
            }
            torch.save(checkpoint, save_dir / 'best_model.pth')

            # ä¿å­˜æœ€ä½³éªŒè¯ç»“æœ
            best_val_results = {
                'predict': val_pred,
                'label': val_label,
                'time': val_time,
                'loss': val_loss
            }

            print(f"  âœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Val Loss: {val_loss:.4f})")
        else:
            patience += 1

        # æ—©åœ
        if patience >= config.early_stop:
            print(f"\næ—©åœè§¦å‘ï¼å·²è¿ç»­ {patience} ä¸ªepochæ— æ”¹å–„")
            break

    print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³Epoch: {best_epoch}, æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")

    # ä¿å­˜è®­ç»ƒæ›²çº¿æ•°æ®ï¼ˆnumpyæ ¼å¼ï¼‰
    np.save(save_dir / 'train_losses.npy', np.array(train_losses))
    np.save(save_dir / 'val_losses.npy', np.array(val_losses))

    # ä¿å­˜è¯¦ç»†çš„losså†å²è®°å½•ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
    save_loss_history(train_losses, val_losses, best_epoch, save_dir)

    # ç»˜åˆ¶å¹¶ä¿å­˜lossæ›²çº¿å›¾
    plot_loss_curves(train_losses, val_losses, best_epoch, save_dir)

    # ==================== 7. æµ‹è¯•æœ€ä½³æ¨¡å‹ ====================
    print("\n[6/7] æµ‹è¯•æœ€ä½³æ¨¡å‹...")

    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ä¿å­˜äº†æœ€ä½³æ¨¡å‹
    if best_val_results is None:
        print("âš  è­¦å‘Š: è®­ç»ƒè¿‡ç¨‹æœªä¿å­˜ä»»ä½•ç»“æœï¼Œå¯èƒ½è®­ç»ƒè½®æ•°ä¸º0æˆ–æ‰€æœ‰epochéƒ½å¤±è´¥")
        print("å°†ä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
        val_loss, val_pred, val_label, val_time = val(val_loader, model, config)
        best_val_results = {
            'predict': val_pred,
            'label': val_label,
            'time': val_time,
            'loss': val_loss
        }
        best_epoch = 0
    else:
        # åŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
        checkpoint = torch.load(save_dir / 'best_model.pth',weights_only=False)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)

    # æµ‹è¯•
    test_loss, test_pred, test_label, test_time = test(test_loader, model, config)

    # ==================== 6.5. è¯„ä¼°è®­ç»ƒé›† ====================
    print("\næ­£åœ¨è¯„ä¼°è®­ç»ƒé›†...")

    # é€‰æ‹©åˆé€‚çš„è¯„ä¼°å‡½æ•°
    if use_enhanced:
        # ä½¿ç”¨å¢å¼ºéªŒè¯æµç¨‹
        train_eval_loss, train_pred, train_label, train_time = validate_epoch(
            model, train_loader, criterion, config, config.device
        )
    else:
        # ä½¿ç”¨æ ‡å‡†éªŒè¯æµç¨‹
        # æ³¨æ„ï¼šval()å‡½æ•°å®é™…æ˜¯é€šç”¨çš„è¯„ä¼°å‡½æ•°ï¼Œå¯ç”¨äºä»»ä½•æ•°æ®é›†
        train_eval_loss, train_pred, train_label, train_time = val(train_loader, model, config)

    # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
    train_rmse, train_mae, train_r2, train_bias = get_metric(train_pred, train_label)
    val_rmse, val_mae, val_r2, val_bias = get_metric(best_val_results['predict'], best_val_results['label'])
    test_rmse, test_mae, test_r2, test_bias = get_metric(test_pred, test_label)

    # è®¡ç®—æŒ‰é¢„æµ‹æ­¥é•¿åˆ†è§£çš„æŒ‡æ ‡
    train_metrics_per_step = get_metrics_per_step(train_pred, train_label)
    val_metrics_per_step = get_metrics_per_step(best_val_results['predict'], best_val_results['label'])
    test_metrics_per_step = get_metrics_per_step(test_pred, test_label)

    print(f"âœ“ è®­ç»ƒé›†è¯„ä¼°å®Œæˆ:")
    print(f"  RMSE: {train_rmse:.4f} Â°C, MAE: {train_mae:.4f} Â°C, "
          f"RÂ²: {train_r2:.4f}, Bias: {train_bias:+.4f} Â°C")

    # è¾“å‡ºè®­ç»ƒé›†æŒ‡æ ‡
    print(f"\nè®­ç»ƒé›† (æœ€ä½³æ¨¡å‹ Epoch {best_epoch}):")
    print(f"  æ•´ä½“ï¼ˆæ‰€æœ‰é¢„æµ‹æ­¥é•¿å¹³å‡ï¼‰:")
    print(f"    RMSE: {train_rmse:.4f} Â°C, MAE: {train_mae:.4f} Â°C, "
          f"RÂ²: {train_r2:.4f}, Bias: {train_bias:+.4f} Â°C")
    print(f"  æŒ‰é¢„æµ‹æ­¥é•¿åˆ†è§£:")
    for metrics in train_metrics_per_step:
        print(f"    ç¬¬{metrics['step']}æ­¥ (+{metrics['step']}å¤©): "
              f"RMSE: {metrics['rmse']:.4f} Â°C, "
              f"MAE: {metrics['mae']:.4f} Â°C, "
              f"RÂ²: {metrics['r2']:.4f}, "
              f"Bias: {metrics['bias']:+.4f} Â°C")

    # è¾“å‡ºéªŒè¯é›†æŒ‡æ ‡
    print(f"\néªŒè¯é›† (æœ€ä½³æ¨¡å‹ Epoch {best_epoch}):")
    print(f"  æ•´ä½“ï¼ˆæ‰€æœ‰é¢„æµ‹æ­¥é•¿å¹³å‡ï¼‰:")
    print(f"    RMSE: {val_rmse:.4f} Â°C, MAE: {val_mae:.4f} Â°C, "
          f"RÂ²: {val_r2:.4f}, Bias: {val_bias:+.4f} Â°C")
    print(f"  æŒ‰é¢„æµ‹æ­¥é•¿åˆ†è§£:")
    for metrics in val_metrics_per_step:
        print(f"    ç¬¬{metrics['step']}æ­¥ (+{metrics['step']}å¤©): "
              f"RMSE: {metrics['rmse']:.4f} Â°C, "
              f"MAE: {metrics['mae']:.4f} Â°C, "
              f"RÂ²: {metrics['r2']:.4f}, "
              f"Bias: {metrics['bias']:+.4f} Â°C")

    # è¾“å‡ºæµ‹è¯•é›†æŒ‡æ ‡
    print(f"\næµ‹è¯•é›†:")
    print(f"  æ•´ä½“ï¼ˆæ‰€æœ‰é¢„æµ‹æ­¥é•¿å¹³å‡ï¼‰:")
    print(f"    RMSE: {test_rmse:.4f} Â°C, MAE: {test_mae:.4f} Â°C, "
          f"RÂ²: {test_r2:.4f}, Bias: {test_bias:+.4f} Â°C")
    print(f"  æŒ‰é¢„æµ‹æ­¥é•¿åˆ†è§£:")
    for metrics in test_metrics_per_step:
        print(f"    ç¬¬{metrics['step']}æ­¥ (+{metrics['step']}å¤©): "
              f"RMSE: {metrics['rmse']:.4f} Â°C, "
              f"MAE: {metrics['mae']:.4f} Â°C, "
              f"RÂ²: {metrics['r2']:.4f}, "
              f"Bias: {metrics['bias']:+.4f} Â°C")

    # è®¡ç®—å¹¶è¾“å‡ºæç«¯å€¼ç›‘æ§æŒ‡æ ‡
    print("\n" + "=" * 80)
    print("æç«¯å€¼ç›‘æ§æŒ‡æ ‡")
    print("=" * 80)

    # å®šä¹‰æ¸©åº¦é˜ˆå€¼
    high_thresholds = [28, 30, 35]
    low_thresholds = [0, -5, -10]

    # è®¡ç®—æç«¯å€¼æŒ‡æ ‡
    train_extreme = get_extreme_metrics(
        train_pred, train_label,
        high_thresholds=high_thresholds, low_thresholds=low_thresholds
    )
    val_extreme = get_extreme_metrics(
        best_val_results['predict'], best_val_results['label'],
        high_thresholds=high_thresholds, low_thresholds=low_thresholds
    )
    test_extreme = get_extreme_metrics(
        test_pred, test_label,
        high_thresholds=high_thresholds, low_thresholds=low_thresholds
    )

    # è¾“å‡ºæ ·æœ¬åˆ†å¸ƒç»Ÿè®¡
    print("\nã€æ ·æœ¬åˆ†å¸ƒç»Ÿè®¡ã€‘")
    print("è®­ç»ƒé›†:")
    if train_extreme['normal_temp']:
        print(f"  æ­£å¸¸æ¸©åº¦ ({train_extreme['normal_temp']['range']}): "
              f"{train_extreme['normal_temp']['sample_count']}æ ·æœ¬ "
              f"({train_extreme['normal_temp']['percentage']:.1f}%)")
    for ht in train_extreme['high_temp']:
        print(f"  é«˜æ¸© (â‰¥{ht['threshold']}Â°C): {ht['sample_count']}æ ·æœ¬ ({ht['percentage']:.1f}%)")
    for lt in train_extreme['low_temp']:
        print(f"  ä½æ¸© (â‰¤{lt['threshold']}Â°C): {lt['sample_count']}æ ·æœ¬ ({lt['percentage']:.1f}%)")

    print("\néªŒè¯é›†:")
    if val_extreme['normal_temp']:
        print(f"  æ­£å¸¸æ¸©åº¦ ({val_extreme['normal_temp']['range']}): "
              f"{val_extreme['normal_temp']['sample_count']}æ ·æœ¬ "
              f"({val_extreme['normal_temp']['percentage']:.1f}%)")
    for ht in val_extreme['high_temp']:
        print(f"  é«˜æ¸© (â‰¥{ht['threshold']}Â°C): {ht['sample_count']}æ ·æœ¬ ({ht['percentage']:.1f}%)")
    for lt in val_extreme['low_temp']:
        print(f"  ä½æ¸© (â‰¤{lt['threshold']}Â°C): {lt['sample_count']}æ ·æœ¬ ({lt['percentage']:.1f}%)")

    print("\næµ‹è¯•é›†:")
    if test_extreme['normal_temp']:
        print(f"  æ­£å¸¸æ¸©åº¦ ({test_extreme['normal_temp']['range']}): "
              f"{test_extreme['normal_temp']['sample_count']}æ ·æœ¬ "
              f"({test_extreme['normal_temp']['percentage']:.1f}%)")
    for ht in test_extreme['high_temp']:
        print(f"  é«˜æ¸© (â‰¥{ht['threshold']}Â°C): {ht['sample_count']}æ ·æœ¬ ({ht['percentage']:.1f}%)")
    for lt in test_extreme['low_temp']:
        print(f"  ä½æ¸© (â‰¤{lt['threshold']}Â°C): {lt['sample_count']}æ ·æœ¬ ({lt['percentage']:.1f}%)")

    # è¾“å‡ºé«˜æ¸©äº‹ä»¶æ€§èƒ½
    print("\nã€é«˜æ¸©äº‹ä»¶æ€§èƒ½ã€‘")
    print("è®­ç»ƒé›†:")
    for ht in train_extreme['high_temp']:
        print(f"  æç«¯é«˜æ¸© (â‰¥{ht['threshold']}Â°C):")
        print(f"    RMSE: {ht['rmse']:.4f} Â°C, MAE: {ht['mae']:.4f} Â°C, Bias: {ht['bias']:+.4f} Â°C")
        print(f"    ä½ä¼°ç‡: {ht['underestimate_rate']:.1f}%, é«˜ä¼°ç‡: {ht['overestimate_rate']:.1f}%")
        print(f"    å‘½ä¸­ç‡: {ht['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {ht['false_alarm_rate']:.1f}%, "
              f"æ¼æŠ¥ç‡: {ht['miss_rate']:.1f}%")

    print("\néªŒè¯é›†:")
    for ht in val_extreme['high_temp']:
        print(f"  æç«¯é«˜æ¸© (â‰¥{ht['threshold']}Â°C):")
        print(f"    RMSE: {ht['rmse']:.4f} Â°C, MAE: {ht['mae']:.4f} Â°C, Bias: {ht['bias']:+.4f} Â°C")
        print(f"    ä½ä¼°ç‡: {ht['underestimate_rate']:.1f}%, é«˜ä¼°ç‡: {ht['overestimate_rate']:.1f}%")
        print(f"    å‘½ä¸­ç‡: {ht['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {ht['false_alarm_rate']:.1f}%, "
              f"æ¼æŠ¥ç‡: {ht['miss_rate']:.1f}%")

    print("\næµ‹è¯•é›†:")
    for ht in test_extreme['high_temp']:
        print(f"  æç«¯é«˜æ¸© (â‰¥{ht['threshold']}Â°C):")
        print(f"    RMSE: {ht['rmse']:.4f} Â°C, MAE: {ht['mae']:.4f} Â°C, Bias: {ht['bias']:+.4f} Â°C")
        print(f"    ä½ä¼°ç‡: {ht['underestimate_rate']:.1f}%, é«˜ä¼°ç‡: {ht['overestimate_rate']:.1f}%")
        print(f"    å‘½ä¸­ç‡: {ht['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {ht['false_alarm_rate']:.1f}%, "
              f"æ¼æŠ¥ç‡: {ht['miss_rate']:.1f}%")

    # è¾“å‡ºä½æ¸©äº‹ä»¶æ€§èƒ½
    print("\nã€ä½æ¸©äº‹ä»¶æ€§èƒ½ã€‘")
    print("è®­ç»ƒé›†:")
    for lt in train_extreme['low_temp']:
        print(f"  æç«¯ä½æ¸© (â‰¤{lt['threshold']}Â°C):")
        print(f"    RMSE: {lt['rmse']:.4f} Â°C, MAE: {lt['mae']:.4f} Â°C, Bias: {lt['bias']:+.4f} Â°C")
        print(f"    é«˜ä¼°ç‡: {lt['overestimate_rate']:.1f}%, ä½ä¼°ç‡: {lt['underestimate_rate']:.1f}%")
        print(f"    å‘½ä¸­ç‡: {lt['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {lt['false_alarm_rate']:.1f}%, "
              f"æ¼æŠ¥ç‡: {lt['miss_rate']:.1f}%")

    print("\néªŒè¯é›†:")
    for lt in val_extreme['low_temp']:
        print(f"  æç«¯ä½æ¸© (â‰¤{lt['threshold']}Â°C):")
        print(f"    RMSE: {lt['rmse']:.4f} Â°C, MAE: {lt['mae']:.4f} Â°C, Bias: {lt['bias']:+.4f} Â°C")
        print(f"    é«˜ä¼°ç‡: {lt['overestimate_rate']:.1f}%, ä½ä¼°ç‡: {lt['underestimate_rate']:.1f}%")
        print(f"    å‘½ä¸­ç‡: {lt['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {lt['false_alarm_rate']:.1f}%, "
              f"æ¼æŠ¥ç‡: {lt['miss_rate']:.1f}%")

    print("\næµ‹è¯•é›†:")
    for lt in test_extreme['low_temp']:
        print(f"  æç«¯ä½æ¸© (â‰¤{lt['threshold']}Â°C):")
        print(f"    RMSE: {lt['rmse']:.4f} Â°C, MAE: {lt['mae']:.4f} Â°C, Bias: {lt['bias']:+.4f} Â°C")
        print(f"    é«˜ä¼°ç‡: {lt['overestimate_rate']:.1f}%, ä½ä¼°ç‡: {lt['underestimate_rate']:.1f}%")
        print(f"    å‘½ä¸­ç‡: {lt['hit_rate']:.1f}%, è¯¯æŠ¥ç‡: {lt['false_alarm_rate']:.1f}%, "
              f"æ¼æŠ¥ç‡: {lt['miss_rate']:.1f}%")

    print("=" * 80)

    # ==================== 8. ä¿å­˜ç»“æœ ====================
    print("\n[7/7] ä¿å­˜ç»“æœ...")

    # æ„å»ºè®­ç»ƒé›†ç»“æœå­—å…¸
    best_train_results = {
        'predict': train_pred,
        'label': train_label,
        'time': train_time,
        'loss': train_eval_loss,
        'rmse': train_rmse,
        'mae': train_mae,
        'r2': train_r2,
        'bias': train_bias
    }

    best_val_results['rmse'] = val_rmse
    best_val_results['mae'] = val_mae
    best_val_results['r2'] = val_r2
    best_val_results['bias'] = val_bias

    test_results = {
        'predict': test_pred,
        'label': test_label,
        'time': test_time,
        'loss': test_loss,
        'rmse': test_rmse,
        'mae': test_mae,
        'r2': test_r2,
        'bias': test_bias
    }

    save_results(save_dir, config, arch_config, best_epoch, best_train_results, best_val_results, test_results)


    print("\n" + "=" * 80)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)

    # ==================== 8. è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰ ====================
    if config.auto_visualize:
        print("\n[8/7] è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–...")
        print("=" * 80)

        try:
            # å¯¼å…¥å¯è§†åŒ–å‡½æ•°
            from visualize_results import visualize_checkpoint

            # è°ƒç”¨å¯è§†åŒ–
            success = visualize_checkpoint(
                checkpoint_dir=str(save_dir),
                output_dir='auto',
                pred_steps=config.viz_pred_steps,
                plot_all_stations=config.viz_plot_all_stations,
                dpi=config.viz_dpi,
                use_basemap=config.viz_use_basemap,
                silent=False
            )

            if success:
                print(f"\nâœ… å¯è§†åŒ–å·²è‡ªåŠ¨ç”Ÿæˆ: {save_dir / 'visualizations'}")
            else:
                print(f"\nâš  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹é”™è¯¯ä¿¡æ¯")

        except ImportError as e:
            print(f"\nâš  æ— æ³•å¯¼å…¥å¯è§†åŒ–æ¨¡å—: {e}")
            print("  å¦‚éœ€è‡ªåŠ¨å¯è§†åŒ–ï¼Œè¯·ç¡®ä¿visualize_results.pyåœ¨åŒä¸€ç›®å½•")
        except Exception as e:
            print(f"\nâš  å¯è§†åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
            print("  è®­ç»ƒç»“æœå·²ä¿å­˜ï¼Œæ‚¨å¯ä»¥ç¨åæ‰‹åŠ¨è¿è¡Œå¯è§†åŒ–")

        print("=" * 80)

    # æç¤ºç”¨æˆ·
    if not config.auto_visualize:
        print("\nğŸ’¡ æç¤º:")
        print(f"  è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
        print(f"  å¦‚éœ€ç”Ÿæˆå¯è§†åŒ–,å¯ä»¥:")
        print(f"  1. åœ¨config.pyä¸­è®¾ç½® config.auto_visualize = True")
        print(f"  2. æˆ–æ‰‹åŠ¨è¿è¡Œ:")
        print(f"     python myGNN/visualize_results.py")
        print(f"     å¹¶ä¿®æ”¹é…ç½®ä¸­çš„ CHECKPOINT_DIR = '{save_dir.name}'")



if __name__ == '__main__':
    main()
